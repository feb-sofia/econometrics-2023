---
title: "Simple ANOVA"
author: "Boyko Amarov"
date: "4/19/2021"
output:
  bookdown::html_document2: default
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
## Warning: to compile the notes you need the "bookdown" and the "broom" packages. Install them by
## running install.packages, see the commented lines below

# install.packages(c("patchwork", "bookdown", "broom"))
library(tidyverse)
library(broom)
library(patchwork)

kids <- read_csv(
  "https://raw.githubusercontent.com/feb-uni-sofia/econometrics2020-solutions/master/data/childiq.csv") %>%
  select(kid_score, mom_hs)
```

# Hypothesis testing

In the previous class we fitted a linear regression model for the IQ
score of the children in the `kids` data set depending on the educational
status of their mother `mom_hs`.

$$
\text{kid_score}_i \sim N(\mu_i, \sigma^2) \\
\mu_i = \beta_0 + \beta_1 \text{mom_hs}_i
$$ We used OLS to find guesses for the unknown coefficients.

```{r}
fit <- lm(kid_score ~ 1 + mom_hs, data = kids)
summary(fit)
```

Which gave us the estimated regression equation:

$$
\hat{y}_i = \hat{\mu}_i = \hat{\beta}_0 + \hat{\beta}_1 \text{mom_hs}_i \\
\hat{y}_i = \hat{\mu}_i = 77.55 + 11.77 \text{mom_hs}_i \\
$$

The OLS estimates of the coefficients are the average IQ score in the
`mom_hs = 0` group for $\beta_0$, and the difference between the group
averages for $\beta_1$.

Now we would like to test whether there is a difference between the two
groups of children in the *population* (we already know that there is a
difference in the sample!).

## Simulation

The model:

$$
y_i \sim N(\mu_i, \sigma^2 = 19.85^2) \\
\mu_i = 77.548 + 11.771 \text{mom_hs}_i
$$

```{r}
## Fix the random numbers generator so that you can reproduce your results
set.seed(123)

sim_coeffs <- expand_grid(
  R = 1:200,
  mom_hs = rep(c(0, 1), c(93, 341))
) %>%
  mutate(
    mu = 77.548 + 11.771 * mom_hs,
    kid_score = rnorm(n = n(), mean = mu, sd = 19.85)
  ) %>%
  group_by(R) %>%
  ## The tidy function reformats the output of lm so that it can fit in a data frame
  do(tidy(lm(kid_score ~ 1 + mom_hs, data = .))) %>%
  select(R, term, estimate, std.error, statistic)

slopes <- sim_coeffs %>%
        filter(term == "mom_hs")
```

Plot the distribution of the slope estimates

```{r}
slopes %>%
  ggplot(aes(x = estimate)) +
  geom_point(
    aes(y = 0),
    position = "jitter",
    size = 1 / 2,
    alpha = 0.5
  ) +
  geom_boxplot(alpha = 0.5) +
  labs(
    x = "Slope estimate",
    title = "Distribution of slope estimates",
    y = ""
  ) +
  geom_density(color = "steelblue") +
  geom_vline(xintercept = 11.77, color = "red")
```

## Testing a true null hypothesis

$$
H_0: \beta_1 = 11.77\\
H_1: \beta_1 < 11.77
$$

$$
\text{t} = \frac{\hat{\beta}_1 - \hat{\beta}_1^{H_0}}{se(\hat{\beta}_1)}
$$ 

$$
\text{t} = \frac{\hat{\beta}_1 - 11.77}{se(\hat{\beta}_1)}
$$

1.  Compute the t-statistic for each simulated sample sample

```{r}
slopes <- slopes %>%
  mutate(
    t_statistic = (estimate - 11.77) / std.error,
    reject_H0 = t_statistic < -1
)

sum(slopes$reject_H0)
mean(slopes$reject_H0)
```

2. Plot the distribution of the t-statistic

```{r}
slopes %>%
  ggplot(aes(x = t_statistic)) +
  geom_point(
    aes(y = 0),
    position = "jitter",
    size = 1 / 2,
    alpha = 0.5
  ) +
  geom_boxplot(alpha = 0.5) +
  labs(
    x = "Slope estimate",
    title = "Distribution of slope estimates",
    y = ""
  ) +
  geom_density(color = "steelblue") +
  geom_vline(xintercept = 0, color = "red")
```

5.  Make a decision to reject $H_0$ if the t-statistic is less than -2.
    In how many cases you make a wrong decision?
    
```{r}
slopes <- slopes %>%
  mutate(
    reject_H0_less_2 = t_statistic < -2
  )

sum(slopes$reject_H0_less_2)
mean(slopes$reject_H0_less_2)
```

## Testing a wrong hypothesis

$$
H_0: \beta_1 = 0\\
H_1: \beta_1 > 0
$$


$$
\text{t} = \frac{\hat{\beta}_1 - 0}{se(\hat{\beta}_1)}
$$

1.  Compute the value of the t-statistic for all samples in the
    simulation
    
```{r}
slopes <- slopes %>%
  mutate(
    t_statistic0 = estimate / std.error
  )
```
    
2.  Plot the distribution of the t-statistic

```{r}

slopes %>%
  ggplot(aes(x = t_statistic0)) +
  geom_point(
    aes(y = 0),
    position = "jitter",
    size = 1 / 2,
    alpha = 0.5
  ) +
  geom_boxplot(alpha = 0.5) +
  labs(
    x = "Slope estimate",
    title = "Distribution of slope estimates",
    y = ""
  ) +
  geom_density(color = "steelblue") +
  geom_vline(xintercept = 0, color = "red")
```


3.  In how many cases would you make a wrong decision if you reject
    $H_0$ for values of the t-statistic greater than 2?

```{r}
slopes <- slopes %>%
  mutate(
    reject_H0_gt_2 = t_statistic0 > 2
  )

sum(slopes$reject_H0_gt_2)
```


## t-distribution

It can be shown that the t-statistic follows t-distribution with $n - p$
degrees of freedom _if the null hypothesis is true_. We call this
the distribution of the statsitic _under_ the null hypothesis.


$$
\text{t-statistic} = \frac{\hat{\beta_1} -  \beta_1^{H_0}}{se(\hat{\beta}_1)}
$$

$$
\text{t-statistic} \underbrace{\sim}_{H_0 \text{ is true}} t(\text{df} = n - p)
$$


```{r}
dt <- expand_grid(
  ## Creates a sequence of 100 numbers between -3 and 3
  x = seq(-4, 4, length.out = 200),
  df = c(1, 5, 50, 500)
) %>%
  mutate(
    ## Computes the standard normal density at each of the 100 points in x
    t_dens = dt(x, df = df),
    df = factor(df)
  )

ggplot() +
  ## Draws the normal density line
  geom_line(data = dt, aes(x = x, y = t_dens, colour = df))
```

$$
y_i = \beta_0 + \beta_1 x_i + u_i
$$

Under some assumptions it can be shown that under the null hypothesis
(this simply means that we assume the null hypothesis is true)

```{r, echo = FALSE}
dt <- data.frame(
  ## Creates a sequence of 100 numbers between -3 and 3
  x = seq(-4, 4, length.out = 1000)
) %>%
  mutate(
    ## Computes the standard normal density at each of the 100 points in x
    t_dens = dt(x, df = 434 - 2)
  )
ggplot() +
  ## Draws the normal density line
  geom_line(data = dt, aes(x = x, y = t_dens)) +
  ## Draws the shaded area under the curve between
  ## -1 and 1
  geom_ribbon(
    data = filter(dt, x > -1.96, x < 1.96),
    aes(x = x, y = t_dens, ymin = 0, ymax = t_dens),
    ## Controls the transparency of the area
    alpha = 0.5
  ) +
  annotate(
    "text",
    x = 0,
    y = dnorm(0) / 3,
    label = paste("Pr(-1.96 < X < 1.96) = ", round(pt(-1.96, df = 434 - 2) - pt(1.96, df = 434 - 2), 2), sep = " ")
  ) +
  geom_vline(xintercept = c(-1.96, 1.96), lty = 2, colour = "steelblue") +
  # geom_density(data = slopes, aes(x = t_statistic), color = "steelblue4") +
  scale_x_continuous(breaks = c(-1.96, 0, 1.96)) +
  labs(
    y = "Density"
  )
```

```{r}
# p: probability, t: t-distribution
pt(-2, df = 434 - 2)
```

```{r}
## q: quantile, t: t-distribution
qt(p = 0.02306292, df = 434 - 2)
```

## Critical values in t-tests

For a test with a probability of wrong rejection of a true null hypothesis $\alpha$
the critical values are:


$$
H_0: \beta_1 = \beta_1^{H_0} \\
H_1: \beta_1 < \beta_1^{H_0}
$$

We reject $H_0$ for values of the t-statistic greater than $t_{\alpha}(n - p)$


$$
H_0: \beta_1 = \beta_1^{H_0} \\
H_1: \beta_1 > \beta_1^{H_0}
$$

We reject $H_0$ for values of the t-statistic greater than $t_{1 - \alpha}(n - p)$

$$
H_0: \beta_1 = \beta_1^{H_0} \\
H_1: \beta_1 \neq \beta_1^{H_0}
$$

We reject $H_0$ if the t-statistic is less than $t_{\alpha / 2}(n - p)$ or greater
than $t_{1 - \alpha / 2}(n - p)$.



## p-values

The p-value is the probability to observe (by chance) a t-statistic more
extreme (i.e. against the null hypothesis) than the one observed in the
sample assuming that $H_0$ is true. For the one-sided hypothesis
considered here it is given by

$$
\text{p-value} = P(t < t^{obs} | H_0)
$$

$$
H_0: \beta = 11.77 \\
H_1: \beta < 11.77
$$

$$
H_0: \beta = 0\\
H_1: \beta < 0
$$

$$
H_0: \beta = 0\\
H_1: \beta \neq 0
$$

## Confidence intervals

From the distribution of the t-statistic we can see that

$$
\text{t-statistic} = \frac{\hat{\beta}_1 - \beta_1}{se(\hat{\beta}_1)} \sim t(n - p)
$$


Therefore the probability to observed a value of the t-statistic in the interval
[$t_{\alpha / 2}(n - p)$, $t_{1 - \alpha / 2}$] is 95%.

$$
P\left(t_{\alpha / 2}(n - p) < \frac{\hat{\beta}_1 - \beta_1}{se(\hat{\beta}_1)} < t_{1 - \alpha / 2}\right) \implies \\
P\left(\hat{\beta_1} + se(\hat{\beta}_1) t_{\alpha/2} < \beta_1 < \hat{\beta_1} + se(\hat{\beta}_1) t_{1 - \alpha/2} \right)
$$
Compute the upper and the lower bounds of the confidence intervals for each sample.
In how many samples did the confidence interval contain the real coefficient $\beta_1$?

```{r}

```




