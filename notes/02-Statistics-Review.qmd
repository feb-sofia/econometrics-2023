# Statistics Review

```{r}
## First, load the tidyverse package
## If the package is not installed, uncomment the next line and run the code chunk again
## install.packages("tidyverse")
library(tidyverse)
source("lib.R")
```

## Probability {#sec-probability}

Imagine a game where you flip a single coin once. The possible outcomes are then head (H) and tail (T). The set of all possible outcomes of the game is called the *sample* space of the experiment. We will denote this set with $\Omega$. For a single coin toss game the sample space is $\Omega = \{\text{heads}, \text{tails}\}$.

::: {#def-probability}
## Probability

Let $\Omega$ denote the sample space of a random experiment. Let $A \subseteq \Omega$ and $B \subset \Omega$ be two disjoint events (i.e. $A \cap B = \varnothing$). Disjoint events are events that cannot occur simultaneously.

$$
\begin{align}
  & P(A) \geq 0 \qquad \text{non-negativity}\\
  & P(\Omega) = 1 \qquad  \text{unit measure} \\
  & P(A \cup B) = P(A) + P(B) \qquad \text{additivity}
\end{align}
$$
:::

::: {#thm-probability-complement}
## Probability of complements

Let $\Omega$ be a sample space, let $A \subseteq \Omega$ be a subset of the sample space and let $\bar{A} = \Omega \setminus A$ be the complement of $A$ in $\Omega$. Then the probability of the complement is given by:

$$
P(A) = 1 - P(\bar{A})
$$
:::

::: {proof}
For the proof note that $A$ and $\bar{A}$ are disjoint by definition ($A \cap \bar{A} = \varnothing$). Using the additivity property of probability together with the unit probability of the sample space $\Omega$ from @def-probability it follows that:

```{=tex}
\begin{align}
    P(A \cup \bar{A}) = P(A) + P(\bar{A})
  \end{align}
```
```{=tex}
\begin{align}
    P(A \cup \bar{A}) & =  P(\Omega) \\
    P(A) + P(\bar{A}) & = 1 \implies \\
    P(A) & = 1 - P(\bar{A}).
  \end{align}
```
:::

::: {#exm-probability-weather}
## Weather forecast

Looking at the weather forecast for the next day you see that it will be raining ($A$) with probability $P(A) = 0.3$. The sample space is $\Omega = \{A, \bar{A}\}$ and the probability of not raining ($\bar{A}$) is then $P(\bar{A}) = 1 - 0.3 = 0.7$.
:::

::: {#exm-probability-dice}
## Dice

In a game where you roll a (6-sided) dice once the sample space is $\Omega = \{1, 2, 3, 4, 5, 6\}$. Denote the outcome of a roll with $X$ and assume that the probability of each outcome is equal: $P(X = i) = 1 / 6, i = 1,2,\ldots,6$. The probability of the event $X = 1$ is then $P(X = 1) = 1/6$. The probability of the event "outcome is not one" is $P(X \neq 1) = P(X > 1) = 5 / 6.$
:::

See @BERTSEKAS2008IntroductionProbability (Chapter 1) for a more thorough treatment of the subject.

## Discrete distributions {#sec-discrete-distributions}

```{r, echo = FALSE}
x <- c(0, 1, 2)
y <- c(0, 1, 2)
probs <- c(0.5, 0.3, 0.2)
probs1 <- c(0.5, 0.1, 0.3)
probsY <- probs1

Ex <- sum(x * probs)
Ex1 <- sum(y * probs1)
Ey <- Ex1
devx <- x - Ex
varX <- sum(devx ^ 2 * probs)
varY <- sum((y - Ey) ^ 2 * probsY)

gameDat <- data.frame(x = paste(x, 'EUR', sep = ' '), p = probs, y = paste(y, 'EUR', sep = ' '), p1 = probs1)
gameDat1 <- gameDat
colnames(gameDat1) <- c("Winnings(x)", "P(x)", "Winnings(y)", "P(y)")
```

```{=tex}
\newcommand{\xa}{`r x[1]`}
\newcommand{\xb}{`r x[2]`}
\newcommand{\xc}{`r x[3]`}
```
```{=tex}
\newcommand{\pa}{`r probs[1]`}
\newcommand{\pb}{`r probs[2]`}
\newcommand{\pc}{`r probs[3]`}
```
```{=tex}
\newcommand{\qa}{`r probs1[1]`}
\newcommand{\qb}{`r probs1[2]`}
\newcommand{\qc}{`r probs1[3]`}
```
```{=tex}
\newcommand{\devxa}{`r fmt(devx[1])`}
\newcommand{\devxb}{`r fmt(devx[2])`}
\newcommand{\devxc}{`r fmt(devx[3])`}
```
```{=tex}
\newcommand{\expa}{`r Ex`}
\newcommand{\expb}{`r Ex1`}
```
```{=tex}
\newcommand{\varx}{`r fmt(varX)`}
\newcommand{\vary}{`r fmt(varY)`}
```
When we presented the axioms of probability we mentioned the concept of a sample space and probability. For the sake of brevity we will skip almost all of the set theoretic foundation of probability measures and will directly introduce the concept of a random variable.

Let us go back to the roots of probability theory that date back at least to the 17th century and the study of games of chance (gambling) [@FREEDMAN2007Statistics, pp. 248]. Almost all introductory text on probability theory start with an example of some simple game of chance. We will follow this example, because it is easy to understand a simple game and the mathematical concepts involved. Later we will see how we can apply the concepts developed here to an extremely broad range of problems.

## Discrete Random Variables

Imagine that you are about to buy a lottery ticket that costs 1 EUR. Until you buy and scratch the ticket you don't actually know how much you will win, but before you commit to the purchase you may wonder what winnings you should *expect*. Without knowing the *rules* of the lottery you would be completely in the dark about your prospects, so let us assume that you actually know how the lottery works.

From our point of view the rules of the game are completely determined by two things. The first one is the set of possible outcomes (the sample space) of the lottery and let's assume that each ticket can win 0 EUR, 1 EUR or 2 EUR. Notice that the set of possible values is *finite* as there are only three possible outcomes. Let us write $X$ for the (yet unknown) winning from our lottery ticket. In lottery games the value of $X$ depends on some random mechanism (for example drawing numbered balls, spinning a wheel, etc.), therefore it is a *function* of the outcome of this random mechanism. We will call functions like $X$ *random variables*. For the most part we will not refer the underlying random mechanism and will simply focus on the distribution of the possible values of $X$. The second part of the rules is how often winnings of 0 EUR, 1 EUR and 2 EUR occur when you repeatedly play the game. Obviously, a game where half of the tickets win 2 EUR is quite different from a game where only one out of 100 tickets wins 2 EUR.

```{r discrete-game-probabilities, echo = FALSE}
#| label: tbl-discrete-game-probabilities
#| tbl-cap: "Distribution of outcomes for two games of chance. Possible outcomes and probabilities for each outcome."

fmtTable(as.data.frame(gameDat1))
```

Let us focus on the first game with probabilities (0.5, 0.3 and 0.2) and develop some intuitive understanding of these quantities. You can think about the probabilities in Table \@ref(tab:discrete-game-probabilities) as theoretical proportions in a sense that if you play the lottery 100 times you would expect to win nothing (0 EUR) in *about* 50 games, 1 EUR in *about* 30 games and 2 EUR in about *20* games. Notice that to *expect* 20 2-EUR wins out of 100 games is absolutely *not* the same as the statement that you *will* win 2 EUR in exactly 20 out of 100 games! To convince yourself look at \@ref(tab:discrete-games-simulation) which presents the results of five simulated games with 100 tickets each. You can play with this and similar games by changing the number of tickets and the number of games in this [simulation](https://feb-uni-sofia.shinyapps.io/econometrics2020/). In the first game the player had 49 tickets that won nothing, but in the second game she had only 38 0-win tickets. When we say to expect 50 0-wins out of 100 tickets we mean that the number of observed (actually played) 0-wins will *vary* around 50. In neither of the five simulated games was the number of 0-wins exactly equal to 50 (this is also possible, though).

```{r discrete-games-simulation, anchor="table", tab.align='center', echo = FALSE, label="discrete-games-simulation"}
set.seed(43243)
Ex <- sum(x * probs)
nGames <- 5
cnts <- rmultinom(n = nGames, size = 100, prob = probs)
avgWinnings <-apply(cnts, 2, function(cnt) sum(cnt * x) / 100)
gameResult <- data.frame(t(cnts), avgWinnings)

colnames(gameResult) <- c(paste('x =', 0:(length(probs) - 1), sep = ' '), 'average ticket win')
rownames(gameResult) <- paste('Game', 1:nGames, sep = ' ')

fmtTable(
  gameResult, 
  caption="Simulation of five games with 100 tickets each. Number of tickets by outcome (0, 1, or 2 EUR) and average ticket win."
  )
```

The probabilities in @tbl-discrete-game-probabilities completely describe the two games. The functions that assigns a probability to each possible outcome $p(x)$ and $p(y)$ are called *probability mass functions*. These functions incorporate everything there is to know about our hypothetical games. While this knowledge will not guarantee you a profit from gambling, it enables you to compute the expected value of each ticket, the probability that none of your tickets will win, etc. An important property of the probability mass function is that it is always non-negative (no negative probabilities) and that the sum of the probabilities over all possible values is exactly $1$.

::: {#def-probability-mass-function}
## Probability mass function

For a discrete random variable $X$ with possible values $x_1,\ldots,x_K$ a function $p(x_i)$ that assigns a probability to the possible values of $X$ is called a probability mass function.

$$
\begin{align}
  P(X = x_k) = p_k.
\end{align}
$$ The probabilities are real numbers in the interval $[0, 1]$ and need to sum to 1 over all possible values.

```{=tex}
\begin{align}
  p_k \geq 0\\
  \sum_{k = 1} ^ {K} p_k = 1.
\end{align}
```
:::

Note that the set of $K$ possible values $x_1,\ldots,x_K$ is *finite*. The same definition can be used for infinite sets of possible values as long as these are *countably infinite* but we will skip this discussion.

::: {#exm-probability-mass-function-games}
## Probability mass function

The first game in @tbl-discrete-game-probabilities has three possible outcomes: $x_1 = 0, x_2 = 1, x_3 = 2$. The probability mass function is then

$$
p(x) = \begin{cases}
0.5 & \text{if} \quad x = 0 \\
0.3 & \text{if} \quad x = 1 \\
0.2 & \text{if} \quad x = 2 \\
0 & \text{otherwise}
\end{cases}
$$
:::

A common visualization of the probability mass function is a barchart with the heights of the bars corresponding to the probability of each outcome.

```{r}
tibble(
  x = c(0:2),
  p = c(0.5, 0.3, 0.2)
) %>%
  ggplot(aes(x = x, y = p)) + 
    geom_bar(stat = "identity")
```


<!-- ::: {#exr-discrete-game} -->

<!-- ## Discrete probability -->

<!-- Consider a game of chance with the following probability mass function. -->

<!-- $$ -->
<!-- p(x) = \begin{cases} -->
<!-- 0.05 & \text{if} \quad x = -2 \\ -->
<!-- 0.20 & \text{if} \quad x = -1 \\ -->
<!-- 0.25 & \text{if} \quad x = 0 \\ -->
<!-- 0.5 & \text{if} \quad x = 1 \\ -->
<!-- 0 & \text{otherwise} -->
<!-- \end{cases} -->
<!-- $$ -->

<!-- - Plot ths PMF using a barchart -->
<!-- - Write down the corresponding CDF -->

<!-- ```{r} -->
<!-- ## ggplot -->
<!-- ``` -->

<!-- ```{r} -->
<!-- ## Select size = 20 values at random from the numbers 0, 1, 2, -->
<!-- ## with selection probabilities: 0.5, 0.3, 0.2 -->
<!-- sample(0:2, size = 20, prob = c(0.5, 0.3, 0.2), replace = TRUE) -->
<!-- ``` -->

<!-- ::: -->



<!-- ::: {def-cdf} -->
<!-- # Cumulative Distribution Function -->

<!-- Closely related to the probability mass function is the cumulative distribution function (CDF) which computes the probability for events of the type $X \leq x$. -->


<!-- ```{=tex} -->
<!-- \begin{align} -->
<!--     F(x) = \sum_{x_i \leq x} p(x_i). -->
<!--   \end{align} -->
<!-- ``` -->
<!-- ::: -->

<!-- ::: {exm-cdf-games} -->
<!-- # CDF for the lottery game -->

<!-- ```{=tex} -->
<!-- \begin{align} -->
<!--   P(X < 0) = 0 \\ -->
<!--   P(X < 1) & = \sum_{x_k < 1} p(x_k) = p(x_1) = `r probs[1]`. \\ -->
<!--   P(X \leq 1) & = \sum_{x_k \leq 1} p(x_k) = p(x_1) + p(x_2) = `r probs[1]`+ `r probs[2]`= 0.8\\ -->
<!--   P(X < 2) & = \sum_{x_k < 2} p(x_k) = p(x_1) + p(x_2) = `r probs[1]`+ `r probs[2]`= 0.8\\ -->
<!--   P(X \leq 2) & = \sum_{x_k \leq 2} p(x_k) = p(x_1) + p(x_2) + p(x_3) = `r probs[1]`+ `r probs[2]`+ `r probs[3]`= 1\\ -->
<!-- \end{align} -->
<!-- ``` -->
<!-- $$ -->
<!-- F(x) =  -->
<!-- \begin{cases} -->
<!--   0 & x < 0 \\ -->
<!--   0.5 & 0 \leq x < 1 \\ -->
<!--   0.8 & 1 \leq x < 2 \\ -->
<!--   1 & x \leq 2 \\ -->
<!--   1 & x > 2 -->
<!-- \end{cases} -->
<!-- $$ -->
<!-- ::: -->

## Expected value

Just as the descriptive statistics (average, empirical median, empirical quantiles, etc.) are useful for summarising a set of numbers we would like to be able to summarise distribution *functions*.

Imagine that you plan to buy 100 tickets from the first game in Table \@ref(tab:discrete-game-probabilities). Based on the interpretation of probabilities as theoretical proportions you would *expect* that 50 of the tickets will win nothing, 30 of the tickets will bring you 1 EUR and 20 of the tickets will win 2 EUR. Thus you can write the expected winnings per ticket by summing the contributions of each ticket type:

\begin{align}
  \text{expected winnings} & = 
    \underbrace{\frac{50}{100} \times `r x[1]`\text{ EUR}}_{`r x[1]`\text{ EUR tickets}} + 
    \underbrace{\frac{30}{100} \times `r x[2]`\text{ EUR}}_{`r x[2]`\text{ EUR tickets}} + 
    \underbrace{\frac{20}{100} \times `r x[3]`\text{ EUR}}_{`r x[3]`\text{ EUR tickets}} \\
    & = 0.5 \times 0 \text{EUR} + 0.3 \times 1 \text{EUR} + 0.2 \times 2 \text{EUR} = `r Ex`\text{EUR}.
\end{align}{eq-expected-value-game-1}

Note that the coefficients before the possible outcomes are simply their probabilities. Therefore in a game with 100 tickets you expect that each ticket will bring you $`r Ex`$ EUR (on average). Just as with the probabilities, the expected values does not tell you that your average win per ticket *will* be `r Ex` EUR. If you take a look at the five simulated games in Table \@ref(tab:discrete-games-simulation) you will notice that the *realised* average ticket wins are *not* equal to `r Ex` but they *vary* around it. You can think about the expected value as the *centre* of the distribution (see Figure \@ref(fig:game-probabilities-plot) and @FREEDMAN2007Statistics, pp. 288). It is important to see that the expected value only depends on the probabilities and the possible values and it *does not* depend on the outcome of any particular game. Therefore it is a *constant* and not a random variable itself.

```{r game-probabilities-plot, fig.align="center", fig.cap="Probabilities plot. The black vertical line depicts the expected value.", echo = FALSE}

gameDat$x <- x

ggplot(gameDat, aes(x, y = p)) + geom_bar(stat="identity") + 
  labs(y = 'Probability') + 
  geom_vline(xintercept = Ex)
```

<!-- TODO: bridge -->

Let us write the expected value in a more general way:

::: {#def-expectation-discrete}

## Expected Value

For discrite random variable $X$ with possible values $x_1, x_2,\ldots,x_n$ the weighted average of the possible outcomes:
  \begin{align}
    E(X) = \sum_{i = 1} ^ {n} x_i p(x_i) (\#eq:expectation-discrete)
  \end{align}
is called the expected value of $X$. Sometimes we will refer to the expected value as the mean of the distribution or the mean of the random variable following the distribution.
:::

We introduced the expected value with an example of a game with 100 tickets in order to illustrate it. You should notice from @def-expectation-discrete that the expected value is independent of the *number* of games played as it is a property of the probability mass function of the game.

::: {#exr-expected-value-game-2}

## Expected Value

Let $Y$ be a game with possible winnings of -1, 0, and 2 EUR. The probabilities of these outcomes are $P(x = -1) = 0.2, P(x = 0) = 0.7, P(x = 2) = 0.1$. 

- Plot the PMF using a barchart
- Calculate the expected winnings of this game using @def-expectation-discrete

Play the game using the function `sample` (check its documentation either by typing `?sample` on the command line in R or by searching for it online). It selects `size` of the values speficied in `x` according to the probabilities given in `prob`.

```{r}
sample(
  x = c(-1, 0, 2), 
  size = 10, 
  prob = c(0.2, 0.7, 0.1),
  replace = TRUE
)
```

Play the game a couple of times and look at the outcomes. Compute the average winnings using `mean` and compare the result with the expected value that you calculated in the previous step.

:::


### Properties of the expected value

In the following we list a few important properties of the expected value that we will use throughout the course. In the following let $X$ and $Y$ be random variables with expected values $E(X)$ and $E(Y)$.

::: {#thm-expectation-linearity}

## Linearity of the Expected Value

Let $X$ and $Y$ be two random variables. Then the exected value of their sum equals the sum of their expected values.

\begin{align}
  E(X + Y) = E(X) + E(Y)
\end{align}
:::

:::{#thm-expectation-constant}
## The Expected Value of a Constant

The expected value of a constant equals the constant itself. Let $a$ be any fixed (not random) real number. Then its expected value is:

\begin{align}
  E(a) = a.
\end{align}
:::

:::{#thm-expectation-scaled}
## Expected value of a scaled random variable

Let $X$ be a random variable and let $a$ be any fixed (not random) real number. Then the expected value of $aX$ is:


\begin{align}
  E(aX) = a E(X)
\end{align}
:::

:::{.proof}
\begin{align}
  E(aX) = \sum_{i = 1} ^ {n} a x_i p(x_i) = a \sum_{i = 1} ^ {n} x_i p(x_i) = aE(X).
\end{align}
:::

## Variance

Let us compare the two games in @tbl-discrete-game-probabilities. Both have the same sample space (set of possible outcomes) and both have the same expected winnings, see @eq-expected-value-game-1 and @eq-expected-value-game-2. The games are not identical, though, because their probability distributions are different. If given the choice to play only one game, which one would you prefer?

The second game offers a higher probability to win the highest prize (`r x[3]` EUR) at the cost of a lower probability for the middle prize (`r x[2]` EUR). In other words it places a higher probability on *extreme* outcomes (outcomes that are far from the centre of the distribution, i.e. the expected value). A summary of a distribution that measures its spread (i.e. how likely are extreme values) is the variance:

:::{#def-variance}

## Variance

For a discrete random variable $X$ with possible outcomes $x_1, x_2,\ldots,x_n$ and probability function p(x) the variance is the expected quadratic deviation from the expected value of the distribution $E(X)$.

\begin{align}
  Var(X) & = E(X - E(X)) ^ 2 \\
         & = \sum_{i = 1}  ^ n \left(x_i - E(X)\right) ^ 2 p(x_i)
\end{align}
:::

:::{#exm-variance-discrete}
## Variance

The variance of the first game (X) is:

  \begin{align}
    Var(X) & = \sum_{i = 1} ^ {3} (x_i - E(X)) ^ 2 p(x_i) \\
           & = (x_1 - E(X)) ^ 2 p(x_i) + (x_2 - E(X)) ^ 2 p(x_2) + (x_3 - E(X)) ^ 2 p(x_3) \\
           & = (\devxa) ^ 2 \times \pa + (\devxb) ^ 2 \times \pb + (\devxc) ^ 2 \times \pc \\
           & = \varx.
  \end{align}
:::

:::{#exr-variance-discrete}
Compute the variance of $Y$, the second game described in Table @tbl-discrete-game-probabilities.
:::

:::{.solution}
$Var(Y) = \vary$.
:::

:::{#independence-random-variables}

## Independence of Random Variables

Two discrete random variables $X$ and $Y$ with possible values $x_1,\ldots,x_K$ and $y_1,\ldots,y_L$ are independent if for every $k$ and $l$:

\begin{align}
  P(X = x_k, Y = y_l) = P(X = x_k)P(Y = y_l)
\end{align}
:::

:::{#thm-variance-sum-independent}
For two independent random variables $X$ and $Y$.
\begin{align}
  Var(X + Y) = Var(X) + Var(Y)
\end{align}

More generally, if $X_i, i = 1,\ldots,n$ are independent random variables, the variance of their sum is equal to the sum of their variances:

\begin{align}
Var\left(\sum_{i = 1}^{n} X_i\right) = \sum_{i = 1}^{n} Var(X_i).
\end{align}
:::

<!-- TODO: rename to variance-sum-independent -->


@thm-variance-sum-independent actually holds even for only uncorrelated variables. We will discuss the concept correlation later.

:::{#thm-variance-scaled}

## Variance of a Scaled Random Variable
Let $a \in \mathbf{R}$ be a fixed (not random) real number and let $X$ be a random variable with variance $Var(X)$.

The variance of $aX$ is given by:

\begin{align}
Var(aX) = a^2 Var(X).
\end{align}
:::
:::{.proof}
\begin{align}
  Var(aX) & = \sum_{k = 1} ^ K (ax_k - E(aX))^2 p(x_k) \\
          & = \sum_{k = 1} ^ K a^2(x_k - E(X))^2 p(x_k) \\
          & = a^2 \sum_{k = 1} ^ K a^2(x_k - E(X))^2 p(x_k) \\
          & = a^2 Var(X).
\end{align}
:::

Another useful formula for working with the variance is:

Often it will be easier to compute the variance using the following decomposition: 
\begin{align}
  Var(X) & = E(X - E(X))^2 \\
         & = E\left(X^2 - 2XE(X) + E(X) ^ 2\right) \\
         & = E(X^2) - E(2XE(X)) + E(E(X) ^ 2) \\
         & = E(X^2) - 2 E(X)E(X) + E(X) ^ 2 \\
         & = E(X^2) - E(X)^2 (\#eq:variance-expansion).
\end{align}

The proof above uses the fact that $E(X)$ is a constant and applies @thm-expectation-constant and @thm-expectation-scaled.


## Continuous Distributions

### Uniform distributions

\begin{align}
X \sim \text{Uniform}(a, b), \quad a,b \in \mathbb{R} \\
f(x) = \begin{cases}
\frac{1}{b - a} & a \leq x \leq b\\
0 & \text{otherwise}
\end{cases} \\
E(X) = \int_{a}^{b} f(x) dx = \frac{1}{2}(a + b) \\
Var(X) = \int_{a}^{b} \left(x - E(X)\right)^2 = \frac{1}{12}(b - a)^2 \\
SD(X) = \sqrt{Var(X)} = \frac{1}{\sqrt{12}}(b - a) \\
F(x) = \int_{-\infty}^{x} = \begin{cases}
0 & x < a \\
\frac{x - a}{b - a} & a \leq x \leq b\\
1 & x > b
\end{cases}
\end{align}

For $a = -1$ and $b = 1$ the density of the uniform distribution is depicted in the following figure:

```{r unif-dens}
#| label: fig-unif-dens
#| fig-cap: "Density function of the uniform distribution over [-1, 1]."

unif_dens_plt <- ggplot() +
  # Sets the range of the x-axis from -2 to 2
  xlim(c(-2, 2)) +
  stat_function(
    fun = dunif, 
    args = list(
      min = -1,
      max = 1
      ),
    n = 1000
  ) +
  labs(
    x = "x",
    y = "Density"
  )
unif_dens_plt
```

In order to select values at random from the uniform distribution with limits $a = -1$ and $b = 1$ we can use the `runif` function. The number of values selected is controlled by the `n` argument, while the parameters of the distribution are specified in `min` ($a$) and `max` ($b$).

```{r}
## Select n = 5 values at random from the uniform distribution over [-1, 1]
runif(n = 5, min = -1, max = 1)

```

Now that you've seen what `runif` does, lets create a larger simulation which we will
use in the examples. We will select $n = 200$ values at random from the same distribution
and we will store these values as a column in a data table using the function `tibble`.

```{r}
unif_sim <- tibble(
  x = runif(n = 200, min = -1, max = 1)
)
```


The following code plots the values in `x` as small lines just over the x-axis (rug plot).

```{r}
## Code for illustration only
unif_sim <- tibble(
  x = x
)

unif_dens_plt +
  geom_rug(aes(x = x), data = unif_sim)
```

The area under the density function for an interval is the probability of occurrence of outcomes in this interval. It is easy to calculate the area under @fig-unif-dens for the interval [-1, -0.2]. The hight of the density function is constant at $1/2$. The length of the interval is 0.8, therefore the area of the rectangle is $0.8 \cdot 1/2 = 0.4$.


$$
P(X < -0.2) = 0.4
$$
You can also compute this probability using the `punif` function. By default it computes probabilities of the type `P(X < x)`, where `x` is its first argument. Note that you also need to speficy the limits of the distribution ([-1, 1] in this case).


```{r}
## Compute the probability
punif(-0.2, min = -1, max = 1)
```

Earlier we used `runif` to select a couple of values at random from this distribution and stored the result in an object called `x`. Let us compare the _observed_ proportion of values less than -0.2 to the theoretical probability.

```{r}
## Use < to see which values in x are less than -0.2
x
x < -0.2
```


```{r}
## Compute the sample proportion in the simulated data
## This relies on the following conversion of the logical values
## TRUE -> 1, FALSE -> 0. This conversion happens under the hood 
## in the mean function

mean(unif_sim$x < -0.2)
```

As an exercise, increase the number of games plated (change the `size` argument in `runif`) and compare the _observed_ proportion to the probability.

$$
P(X > -0.2) = 1 - F(-0.2)
$$

```{r}
## Compute the probability
1 - punif(-0.2, min = -1, max = 1)
punif(-0.2, min = -1, max = 1, lower.tail = FALSE)

## Compute the sample proportion in the simulated data
mean(unif_sim$x > -0.2)
```

$$
P(-0.2 < X < 0.3) = F(0.3) - F(-0.2) = 0.25
$$

```{r}
## Probability
punif(0.3, min = -1, max = 1) - punif(-0.2, min = -1, max = 1)

## Sample proportion in the simulation
## The & operator is a logical AND

mean(unif_sim$x < 0.3 & unif_sim$x > -0.2)
```

```{r}
unif_sim %>%
  ggplot(aes(x = x)) +
  geom_histogram(breaks = c(-1, -0.6, -0.2, 0.2, 0.6, 1)) +
  xlim(c(-1, 1))
```

<!-- # Normal distributions -->

<!-- $$ -->
<!-- X \sim N(\mu, \sigma^2) \\ -->
<!-- f(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right)^2}\\ -->
<!-- E(X) = \mu \\ -->
<!-- Var(X) = \sigma^2 \\ -->
<!-- SD(X) = \sqrt{\sigma^2} = \sigma -->
<!-- $$ -->

<!-- ```{r} -->
<!-- players_n <- 300 -->
<!-- games_n <- 16 -->

<!-- unif_games <- expand_grid( -->
<!--   game = 1:games_n, -->
<!--   player = 1:players_n -->
<!-- ) %>% -->
<!--   mutate( -->
<!--     ## When used in mutate, n() returns the number of rows in a group of obs -->
<!--     ## When the data is not grouped as here, it retuns the number of obs in the whole table -->
<!--     result = runif(n(), min = -1, max = 1) -->
<!--   ) %>% -->
<!--   bind_rows( -->
<!--     ## Add a initial values so that all players start with 0 -->
<!--     tibble( -->
<!--       player = 1:players_n, -->
<!--       game = as.integer(0), -->
<!--       result = 0, -->
<!--     ) -->
<!--   ) -->

<!-- unif_games <- unif_games %>% -->
<!--   ## Sort the data by player id and game id -->
<!--   arrange(player, game) %>% -->
<!--   ## Groups the data by player, because we want the running totals to be calculated for each -->
<!--   ## player separately -->
<!--   group_by(player) %>% -->
<!--   mutate( -->
<!--     running_total = cumsum(result) -->
<!--   ) -->

<!-- ## Illustration only -->
<!-- unif_games %>% -->
<!--   ggplot(aes(x = game, y = running_total, group = player)) + -->
<!--   geom_vline(xintercept = c(4, 8, 16), linetype = 2) + -->
<!--   geom_hline(yintercept = 0) + -->
<!--   geom_line(aes(color = player < 2, alpha = player < 2)) + -->
<!--   scale_color_manual(values = c("skyblue4", "firebrick4")) + -->
<!--   scale_alpha_manual(values = c(1 / 5, 1)) + -->
<!--   scale_x_continuous("Game number", breaks = c(0, 4, 8, 12, 16)) + -->
<!--   theme(legend.position = "none") + -->
<!--   labs(y = "Running Total") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- unif_games %>% -->
<!--   filter(game == 4) %>% -->
<!--   ggplot(aes(x = running_total)) + -->
<!--   geom_density() + -->
<!--   labs(title = "Running total distribution at the 4-th game") + -->
<!--   labs( -->
<!--     x = "Running total" -->
<!--   ) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- unif_games %>% -->
<!--   filter(game == 16) %>% -->
<!--   ggplot(aes(x = running_total)) + -->
<!--   geom_density() + -->
<!--   labs(title = "Running total distribution at the 16-th game") + -->
<!--   labs( -->
<!--     x = "Running total" -->
<!--   ) -->
<!-- ``` -->

<!-- # Statistical tests -->

<!-- | Истина/Решение | Осъден | Неосъден | -->
<!-- |----------------|--------|----------| -->
<!-- | Виновен        |        | X        | -->
<!-- | Невинен        | X      |          | -->

<!-- $$ -->
<!-- H_0: \mu = a \\ -->
<!-- H_1: \mu \neq a -->
<!-- $$ -->

<!-- $$ -->
<!-- t = \frac{\overline{x} - a}{S_x / \sqrt{n}} -->
<!-- $$ -->

<!-- The t-statistic follows a t-distribution with $n - 1$ degrees of freedom. -->

<!-- ```{r} -->
<!-- mu_0 <- 0 -->
<!-- B <- 2000 -->
<!-- n <- 300 -->

<!-- sim_test <- expand.grid( -->
<!--   experiment_id = 1:B, -->
<!--   obs_id = 1:n -->
<!-- ) %>% -->
<!--   mutate( -->
<!--     x = rnorm(n = n(), mean = mu_0, sd = 1) -->
<!--   ) %>% -->
<!--   group_by(experiment_id) %>% -->
<!--   summarise( -->
<!--     mean_x = mean(x), -->
<!--     sd_x = sd(x), -->
<!--     t_stat = (mean_x - mu_0) / (sd_x / sqrt(n)) -->
<!--   ) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- mean(sim_test$t_stat < -2) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- sim_test %>% -->
<!--   ggplot(aes(x = t_stat)) + -->
<!--   geom_density() + -->
<!--   geom_vline(xintercept = -2, lty = 2) + -->
<!--   geom_vline(xintercept = 2, lty = 2) -->
<!-- ``` -->
