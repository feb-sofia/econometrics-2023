```{r setup, include=FALSE}
## Warning: to compile the notes you need the "bookdown" and the "broom" packages. Install them by
## running install.packages, see the commented lines below

# install.packages(c("patchwork", "bookdown", "broom"))
library(tidyverse)
library(broom)
library(patchwork)

kids <- read_csv(
  "https://raw.githubusercontent.com/feb-uni-sofia/econometrics2020-solutions/master/data/childiq.csv") %>%
  select(kid_score, mom_hs)
```

# Hypothesis testing

The previous [notebook](https://github.com/feb-sofia/econometrics-2023/blob/main/04-Simple-ANOVA.rmd) discussed the linear regression model with a single (0/1) explanatory variable. The example used the `kids` dataset, see the notes from the previous class for a description.

In the previous class we studied the difference between the average IQ test scores of two groups of children: the ones with a mother with a high school degree (`mom_hs = 1`), and the ones whose mothers had no high school degree (`mom_hs = 0`).

We saw that the linear regression model

$$
\text{kid_score}_i \sim N(\mu_i, \sigma^2) \\
\mu_i = \beta_0 + \beta_1 \text{mom_hs}_i \\
$$

implied the following interpretation of the regression coefficients:

$$
\text{mom_hs} = 1 \implies \mu_{\text{mom_hs} = 1} = \beta_0 + \beta_1 \\
\text{mom_hs} = 0 \implies \mu_{\text{mom_hs} = 0} = \beta_0\\
\beta_1 = \mu_{\text{mom_hs} = 1} - \mu_{\text{mom_hs} = 0}
$$

The intercept in the regression equation corresponds to the expected IQ score (population IQ score) of children with `mom_hs = 0`. The slope coefficient $\beta_1$ is simply the difference between expected (population) scores of the two groups.

We used `lm` to estimate the two coefficients

```{r}
fit <- lm(kid_score ~ 1 + mom_hs, data = kids)
summary(fit)
```

and therefore obtained the estimated regression equation:

$$
\hat{\mu}_i = \hat{\beta}_0 + \hat{\beta}_1 \text{mom_hs}_i \\
\hat{\mu}_i = 77.55 + 11.77 \text{mom_hs}_i \\
$$

The OLS estimates of the coefficients are the average IQ score in the `mom_hs = 0` group for $\beta_0$, and the difference between the group averages for $\beta_1$.

It is easy to show that the OLS estimators for $\beta_0$ and $\beta_1$ are simply the.

## Simulation

In order to study the distribution of the OLS estimates $\hat{\beta}_0$ and $\hat{\beta}_1$ we will generate a large number of samples from the following model.

```{=tex}
\begin{align}
y_i \sim N(\mu_i, \sigma^2 = 19.85^2), i = 1\ldots, n = 434 \\
\mu_i = 77.548 + 11.771 \text{mom_hs}_i \\
\end{align}
```
The model mimicks the sample of children that we studied in the previous class.

First, we constructed a column `mom_hs` with values 0 and 1. We set the number of zeros and ones to the observed counts of these values in the `kids` dataset (93 zeros and 341 ones). Then `expand_grid` repeated this column as many times as the number of unique values of the column `R`. This will be the number of simulated samples that we'll generate.

```{r}
## Fix the random numbers generator so that you can reproduce your results
set.seed(123)

sim_coeffs <- expand_grid(
  R = 1:200,
  mom_hs = rep(c(0, 1), c(93, 341))
) %>%
  mutate(
    ## This is the model for the mean of the IQ scores
    mu = 77.548 + 11.771 * mom_hs,
    ## Select a value at random from a normal distribution with 
    ## mean mu and standard deviation 19.85. Note that
    ## rnorm will take the value of mu in each row of the 
    ## data when generating random values
    kid_score = rnorm(n = n(), mean = mu, sd = 19.85)
  ) %>%
  group_by(R) %>%
  ## The tidy function reformats the output of lm so that it can fit in a data frame
  do(tidy(lm(kid_score ~ 1 + mom_hs, data = .))) %>%
  select(R, term, estimate, std.error, statistic)

## Creates a separate table with the coefficients for mom_hs
slopes <- sim_coeffs %>%
        filter(term == "mom_hs")
```

First we plot the distribution of the slope estimates for each sample. In `geom_point` we add a small random value to each estimate so that we can see all the points (this is waht \`position = "jitter" does). Otherwise all estimates would lie on the x-axis.

```{r}
#| label: fig-sim-distr-slopes
#| fig-cap: "Sampling distribution of $\hat{\beta}_1$. Each dot represents the slope estimate in one sample. The y-axis does not present any meaningful information and it only serves to disentagly the points so that they don't overplot."

slopes %>%
  ggplot(aes(x = estimate)) +
  geom_point(
    aes(y = 0),
    position = "jitter",
    size = 1 / 2,
    alpha = 0.5
  ) +
  geom_boxplot(alpha = 0.5) +
  ## Draws a density estimate
  geom_density(color = "steelblue") +
  ## Draws a vertical line a 11.77 (the value of beta_1 used in the simulation)
  geom_vline(xintercept = 11.77, color = "firebrick") + 
  ## Sets the labels of the x and y axis and the title of the plot
  labs(
    x = "Estimate",
    title = "Distribution of the slope estimates (R samples)",
    y = "Density"
  )
```

The plot reveals two insights:

1.  In most of the samples the estimate was close to the the real value of $\beta_1$ (11.77)
2.  There is a small number of samples that resulted in extreme values of the estimate, e.g. in sample 97 $\hat{\beta}_1 = 4.7$ and in sample 44 the estimated coefficient was 18.6.

We can estimate the center (expected value) of this distribution by computing the mean estimate (i.e. the average estimate over all generated samples).

```{r}
mean(slopes$estimate)
```

We see that this value is very close to the real value of 11.77. This is a consequence of a property of the OLS estimator which we call *unbiasedness*.

The standard deviation of this distribution is called the *standard error* of $\hat{\beta}_1$. It describes the spread of the distribution of estimates.

## Summary of the lm output

```{r}
summary(fit)
```

Standard error: this is an estimate of the standard deviation of the estimator for the coefficient based on assumption of the normal distribution in the model.

The standard deviation of $\hat{\beta}_1$, computed using the simulated samples:

```{r}
## sd: standard deviation
sd(slopes$estimate)
```

## Hypothesis testing

We begin with a simple test of hypotheses about the population value of one of the regression coefficients. Let us start with $\beta_1$ and let us suppose that we want to test the theory that the difference between the average IQ score of the two groups *in the population* equals exactly 11.77. Notice that this is the value of $\beta_1$ that we used in the simulation, so this theory is right. We also want to test this theory against the alternative that the difference between the average IQ scores is less than 11.77.

$$
H_0: \beta_1 = 11.77\\
H_1: \beta_1 < 11.77
$$ The mathematical statistics informs us how to summarize the data so that we can make a decision whether to reject the null hypothesis. The t-statistic is the summary that we need for this test. In general it is the difference between the estimate for $\beta_1$ and the value under the null hypothesis divided by the standard error of the estimate.

$$
t = \frac{\hat{\beta}_1 - \beta_1^{H_o}}{se(\hat{\beta}_1)}
$$

In our case the value of $\beta_1$ under the null hypothesis is $11.77$ so the test statistic becomes:

$$
t = \frac{\hat{\beta}_1 - 11.77}{se(\hat{\beta}_1)}.
$$

From the distribution of $\beta_1$ in

When is the value of the t-statistic close to zero? 1. The estimate is close to the value under the null hypothesis. 2. For high values of the standard error

$$
t = \frac{11.77 - 11.77}{2.322} = 0
$$

```{r}
slopes <- slopes %>%
  mutate(
    t_statistic = (estimate - 11.77) / std.error
)
```

```{r}
slopes %>%
  ggplot(aes(x = t_statistic)) +
  geom_point(
    aes(y = 0),
    position = "jitter",
    size = 1 / 2,
    alpha = 0.5
  ) +
  geom_boxplot(alpha = 0.5) +
  labs(
    x = "Value of the t-statistic",
    title = "Distribution of t-statistic under a true null hypothesis beta_1 = 11.77 (2000 samples)",
    y = ""
  ) +
  geom_density(color = "steelblue") +
  geom_vline(xintercept = 0, color = "red") +
  geom_vline(xintercept = c(-2, 2), color = "steelblue", lty = 2) +
  geom_vline(xintercept = c(-3, 3), color = "firebrick", lty = 2) +
  xlim(c(-4, 8)) +
  scale_x_continuous(breaks = c(-3, -2, 0, 2, 3))
#  xlim(c(0, 21))
```

The real coefficient equals 11.77 (it is known, because we choose it for the simulation).

Let's assume a rule that we reject the null hypothesis $H_0: \beta_1 = 0$ vs. $H_1: \beta_1 \neq 0$ if the value of the t-statistic is less than -2 or greater than +2.

In how many samples will we wrongly reject the null hypothesis using this rule?

```{r}
testing_1 <- slopes %>%
  mutate(
    ## Logical OR: |
    wrong_decision_blue = t_statistic < -2 | t_statistic > 2,
    wrong_decision_red = t_statistic < -3 | t_statistic > 3
  )

## Share of TRUE values (blue)
sum(testing_1$wrong_decision_blue)
mean(testing_1$wrong_decision_blue)

## Share of TRUE values (red)
sum(testing_1$wrong_decision_red)
mean(testing_1$wrong_decision_red)
```

## Hypothesis testing

$$
H_0: \beta_1 = 0\\
H_1: \beta_1 \neq 0
$$

$$
y_i = \beta_0 + \beta_1 x_i + u_i
$$

If $H_0$ is true, the model is simply

$$
y_i = \beta_0 + u_i
$$

t-test

$$
\text{t-statistic} = \frac{\hat{\beta}_1 - 0}{SE(\hat{\beta}_1)}
$$ The value of the t-statistic is small when the estimate for the coefficient is close to the value under the null hypothesis. The value of the t-statistic will be small, if the standard error of the estimator is high.

$$
t = \frac{11.77 - 0}{2.322} = 5.069
$$

Compute the value of the t-statistic for all samples in the simulation (and compare it to the value of the `statistic` column in the `sim_coef` dataset)

```{r}
slopes <- slopes %>%
  mutate(
    t_statistic0 = (estimate - 0) / std.error
  )
```

```{r}
slopes %>%
  ggplot(aes(x = t_statistic0)) +
  geom_point(
    aes(y = 0),
    position = "jitter",
    size = 1 / 2,
    alpha = 0.5
  ) +
  geom_boxplot(alpha = 0.5) +
  labs(
    x = "Value of the t-statistic",
    title = "Distribution of t-statistic, beta_1 = 0 (false)",
    y = ""
  ) +
  geom_density(color = "steelblue") +
  geom_vline(xintercept = 0, color = "red") +
  geom_vline(xintercept = c(-2, 2), color = "steelblue", lty = 2) +
  geom_vline(xintercept = c(-3, 3), color = "firebrick", lty = 2) +
  xlim(c(-4, 8)) +
  scale_x_continuous(breaks = c(-3, -2, 0, 2, 3))
```

```{r}
testing_2 <- slopes %>%
  mutate(
    ## Logical AND: &
    wrong_decision_blue = t_statistic0 < 2 & t_statistic0 > -2,
    wrong_decision_red = t_statistic0 < 3 & t_statistic0 > -3
  )
## Share of TRUE values
mean(testing_2$wrong_decision_blue)
mean(testing_2$wrong_decision_red)
```

## How to choose critical values?

Convention: choose the critical values so that the probability of rejecting a true null hypothesis is 5%.

# t-distribution

$$
y_i = \beta_0 + \beta_1 x_i + u_i
$$

Under some assumptions it can be shown that under the null hypothesis (this simply means that we assume the null hypothesis is true)

$$
H_0: \beta_1 = \beta_{H_0}\\
H_1: \beta_1 \neq \beta_{H_0}\\
$$

$$
\text{t-statistic} = \frac{\hat{\beta_1} -  \beta_{H_0}}{SE(\hat{\beta}_1)}
$$

$$
\text{t-statistic} \underbrace{\sim}_{H_0} t(\text{df} = n - p)
$$

The t-statistic follow a t-distribution with $n - p$ degrees of freedom (parameter of the distribution), where $n$ is the number of observations in the linear model (in our example $n = 434$ kids) and $p$ is the number of coefficients in the linear equation. In our linear regression model the number of coefficients in $p = 2$: the intercept $\beta_0$, and the slope coefficient $\beta_1$.

## Probabilities and quantiles of the t-distribution

```{r, echo = FALSE}
dt <- data.frame(
  ## Creates a sequence of 100 numbers between -3 and 3
  x = seq(-4, 4, length.out = 100)
) %>%
  mutate(
    ## Computes the standard normal density at each of the 100 points in x
    t_dens = dt(x, df = 434 - 2)
  )
ggplot() +
  ## Draws the normal density line
  geom_line(data = dt, aes(x = x, y = t_dens)) +
  ## Draws the shaded area under the curve between
  ## -1 and 1
  geom_ribbon(
    data = filter(dt, x > -1.96, x < 1.96),
    aes(x = x, y = t_dens, ymin = 0, ymax = t_dens),
    ## Controls the transparency of the area
    alpha = 0.5
  ) +
  annotate(
    "text",
    x = 0,
    y = dnorm(0) / 3,
    label = paste("Pr(-1.96 < X < 1.96) = ", round(pt(-1.96, df = 434 - 2) - pt(1.96, df = 434 - 2), 2), sep = " ")
  ) +
  geom_vline(xintercept = c(-1.96, 1.96), lty = 2, colour = "steelblue") +
  # geom_density(data = slopes, aes(x = t_statistic), color = "steelblue4") +
  scale_x_continuous(breaks = c(-1.96, 0, 1.96))
```

### Probability

```{r}
# p: probability, t: t-distribution
pt(-2, df = 434 - 2)
```

```{r}
rt(1, df = 434 - 2)
```

```{r}
# r: random, t: t-distribution
mean(rt(1000000, df = 434 - 2) < -2)
# mean(rt(1000000, df = 2) < -2)
```

### Quantiles

```{r}
## q: quantile, t: t-distribution
qt(p = 0.02306292, df = 434 - 2)
```

## Critical values in t-tests

```{r}
# 0.025 quantile of the t-distribution with 2 degrees of freedom
qt(0.025, df = 434 - 2)
```

```{r}
# r: random, t: t-distribution
mean(rt(10000, df = 434 - 2) < -1.965471)
```

A convention is to use a 5% error probability of rejecting a true null hypothesis, so we use the quantiles of the t-distribution to derive critical values as follows:

```{r}
## Lower critical value: the 0.025 quantile of the t-distribution
qt(0.025, df = 434 - 2)
## Upper critical value: the 0.975 quantile of the t-distribution
## lower.tail = FALSE instructs qt to calculate
qt(0.025, df = 434 - 2, lower.tail = FALSE)
```

Both critical values are equal in absolute value.
