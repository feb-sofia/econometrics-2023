[
  {
    "objectID": "00-Homework-Instructions.html",
    "href": "00-Homework-Instructions.html",
    "title": "Homework (graded)",
    "section": "",
    "text": "Please find the homework assignment here:\nhttps://classroom.github.com/a/TLpH3lT8\nIn order to access the homework you will need a working GitHub account. If you don’t have one, please create a profile with GitHub. Please use a working email address when opening your account, because you will receive notifications from GitHub related to the grading of the final homework.\nAfter clicking the link and logging into GitHub you can select your student id from the roster. Please email me (amarov@feb.uni-sofia.bg) if you can’t find you student id there!\nThe homework is meant as a group assignment. The team size is limited to five members. The first member of the group can open the assignment and create the team. The rest of the members can select that team when joining.\nAfter joining, GitHub will create a repository for your team. This repository will be empty and you will receive your assignment once all the teams are formed. Copy its contents in your local R Studio session to start working on it. You will find submission instructions at the top of the file."
  },
  {
    "objectID": "00-Recommended-Literature.html",
    "href": "00-Recommended-Literature.html",
    "title": "Recommended reading",
    "section": "",
    "text": "The R for data science (Wickham and Grolemund 2016) book covers data management and exploration using the tools from the tidyverse packages.\nDalgaard (2008): Introductory statistics with R. 2nd edition. New York: Springer. This textbook offers an introductory level to probability and basic linear models with a focus on the software (R) and with only a minimal discussion of the mathematical fundamentals. The code shown is base R and mostly differs from what we are using in the classes (tidyverse).\nSheather (2009): A Modern Approach to Regression with R. New York, NY: Springer New York (Springer Texts in Statistics). Available at: https://doi.org/10.1007/978-0-387-09608-7.\nThe Textbook focuses on linear regression models and develops the mathematical fundametals alongside with examples using R.\nGelman, Hill, and Vehtari (2021): Regression and other stories. Cambridge New York, NY Port Melbourne, VIC New Delhi Singapore: Cambridge University Press (Analytical methods for social research). Available at: https://doi.org/10.1017/9781139161879.\nAnother introduction into basic probability theory and linear models. The references to Bayesian inference in the book are not relevant to our course.\nFaraway (2015): Linear models with R. Second edition. Boca Raton: CRC Press, Taylor & Francis Group.\nAnother introduction into linear models with examples in R.\n\n\n\n\n\nDalgaard, Peter. 2008. Introductory Statistics with R. 2nd edition. Statistics and Computing. New York: Springer.\n\n\nFaraway, Julian James. 2015. Linear Models with R. Second edition. Chapman & Hall/CRC Texts in Statistical Science Series. Boca Raton: CRC Press, Taylor & Francis Group.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and Other Stories. Analytical Methods for Social Research. Cambridge New York, NY Port Melbourne, VIC New Delhi Singapore: Cambridge University Press. https://doi.org/10.1017/9781139161879.\n\n\nSheather, Simon. 2009. A Modern Approach to Regression with R. Edited by George Casella, Stephen Fienberg, and Ingram Olkin. Springer Texts in Statistics. New York, NY: Springer New York. https://doi.org/10.1007/978-0-387-09608-7.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. First edition. Sebastopol, CA: O’Reilly."
  },
  {
    "objectID": "02-Statistics-Review.html#sec-probability",
    "href": "02-Statistics-Review.html#sec-probability",
    "title": "1  Statistics Review (Optional)",
    "section": "1.1 Probability",
    "text": "1.1 Probability\nImagine a game where you flip a single coin once. The possible outcomes are head (H) and tail (T). The set of all possible outcomes of the game is called the sample space of the experiment. We will denote this set with \\Omega. For a single coin toss game, the sample space is \\Omega = \\{\\text{heads}, \\text{tails}\\}.\n\nDefinition 1.1 (Probability) Let \\Omega denote the sample space of a random experiment. Let A \\subseteq \\Omega and B \\subset \\Omega be two disjoint events (i.e. A \\cap B = \\varnothing). Disjoint events are events that cannot co-occur. A probability measure on this space has the following properties:\n\n\\begin{align}\n  & P(A) \\geq 0 \\qquad \\text{non-negativity}\\\\\n  & P(\\Omega) = 1 \\qquad  \\text{unit measure} \\\\\n  & P(A \\cup B) = P(A) + P(B) \\qquad \\text{additivity}\n\\end{align}\n\n\n\nTheorem 1.1 (Probability of complementary sets) Let \\Omega be a sample space, let A \\subseteq \\Omega be a subset of the sample space, and let \\bar{A} = \\Omega \\setminus A be the complement of A in \\Omega. Then the probability of the complementary set is given by:\n\nP(A) = 1 - P(\\bar{A})\n\n\n\nProof. For the proof note that A and \\bar{A} are disjoint by definition (A \\cap \\bar{A} = \\varnothing). Using the additivity of probability together with the unit probability of the sample space \\Omega from Definition 1.1, it follows that:\n\\begin{align}\n    P(A \\cup \\bar{A}) = P(A) + P(\\bar{A})\n  \\end{align}\n\\begin{align}\n    P(A \\cup \\bar{A}) & =  P(\\Omega) \\\\\n    P(A) + P(\\bar{A}) & = 1 \\implies \\\\\n    P(A) & = 1 - P(\\bar{A}).\n  \\end{align}\n\n\nExample 1.1 (Weather forecast) The weather forecast for the next day shows that it will be raining (A) with probability P(A) = 0.3. The sample space is \\Omega = \\{A, \\bar{A}\\} and the probability of not raining (\\bar{A}) is then P(\\bar{A}) = 1 - 0.3 = 0.7.\n\n\nExample 1.2 (Dice) In a game where you roll a (6-sided) dice once the sample space is \\Omega = \\{1, 2, 3, 4, 5, 6\\}. Denote the outcome of a roll with X and assume that the probability of each outcome is equal: P(X = i) = 1 / 6, i = 1,2,\\ldots,6. The probability of the event X = 1 is then P(X = 1) = 1/6. The probability of the event “outcome is not one” is P(X \\neq 1) = P(X > 1) = 5 / 6.\n\nSee Bertsekas and Tsitsiklis (2008) (Chapter 1) for a more thorough treatment of the subject."
  },
  {
    "objectID": "02-Statistics-Review.html#sec-discrete-distributions",
    "href": "02-Statistics-Review.html#sec-discrete-distributions",
    "title": "1  Statistics Review (Optional)",
    "section": "1.2 Discrete distributions",
    "text": "1.2 Discrete distributions\n\n\n\nWhen we presented the axioms of probability we mentioned the concept of a sample space and probability. For the sake of brevity we will skip almost all of the set theoretic foundation of probability measures and will directly introduce the concept of a random variable.\nLet us go back to the roots of probability theory that date back at least to the 17th century and the study of games of chance (gambling) (Freedman, Pisani, and Purves 2007, 248). Almost all introductory text on probability theory start with an example of some simple game of chance. We will follow this example, because it is easy to understand a simple game and the mathematical concepts involved. Later we will see how we can apply the concepts developed here to an extremely broad range of problems."
  },
  {
    "objectID": "02-Statistics-Review.html#discrete-random-variables",
    "href": "02-Statistics-Review.html#discrete-random-variables",
    "title": "1  Statistics Review (Optional)",
    "section": "1.3 Discrete Random Variables",
    "text": "1.3 Discrete Random Variables\nImagine that you are about to buy a lottery ticket that costs 1 EUR. Until you buy and scratch the ticket you don’t actually know how much you will win, but before you commit to the purchase you may wonder what winnings you should expect. Without knowing the rules of the lottery you would be completely in the dark about your prospects, so let us assume that you actually know how the lottery works.\nFrom our point of view the rules of the game are completely determined by two things. The first one is the set of possible outcomes (the sample space) of the lottery and let’s assume that each ticket can win 0 EUR, 1 EUR or 2 EUR. Notice that the set of possible values is finite as there are only three possible outcomes. Let us write X for the (yet unknown) winning from our lottery ticket. In lottery games the value of X depends on some random mechanism (for example drawing numbered balls, spinning a wheel, etc.), therefore it is a function of the outcome of this random mechanism. We will call functions like X random variables. For the most part we will not refer the underlying random mechanism and will simply focus on the distribution of the possible values of X. The second part of the rules is how often winnings of 0 EUR, 1 EUR and 2 EUR occur when you repeatedly play the game. Obviously, a game where half of the tickets win 2 EUR is quite different from a game where only one out of 100 tickets wins 2 EUR.\n\n\n\n\n\nTable 1.1:  Distribution of outcomes for two games of chance. Possible outcomes and probabilities for each outcome. \n \n  \n    Winnings(x) \n    P(x) \n    Winnings(y) \n    P(y) \n  \n \n\n  \n    0 EUR \n    0.5 \n    0 EUR \n    0.5 \n  \n  \n    1 EUR \n    0.3 \n    1 EUR \n    0.1 \n  \n  \n    2 EUR \n    0.2 \n    2 EUR \n    0.3 \n  \n\n\n\n\n\n\nLet us focus on the first game with probabilities (0.5, 0.3 and 0.2) and develop some intuitive understanding of these quantities. You can think about the probabilities in Table 1.1 as theoretical proportions in a sense that if you play the lottery 100 times you would expect to win nothing (0 EUR) in about 50 games, 1 EUR in about 30 games and 2 EUR in about 20 games. Notice that to expect 20 2-EUR wins out of 100 games is absolutely not the same as the statement that you will win 2 EUR in exactly 20 out of 100 games! To convince yourself look at Table 1.2 which presents the results of five simulated games with 100 tickets each. You can play with this and similar games by changing the number of tickets and the number of games in this simulation. In the first game the player had 49 tickets that won nothing, but in the second game she had only 38 0-win tickets. When we say to expect 50 0-wins out of 100 tickets we mean that the number of observed (actually played) 0-wins will vary around 50. In neither of the five simulated games was the number of 0-wins exactly equal to 50 (this is also possible, though).\n\nset.seed(43243)\n\nEx <- sum(x * probs)\nnGames <- 5\ncnts <- rmultinom(n = nGames, size = 100, prob = probs)\navgWinnings <-apply(cnts, 2, function(cnt) sum(cnt * x) / 100)\ngameResult <- data.frame(t(cnts), avgWinnings)\n\ncolnames(gameResult) <- c(paste('x =', 0:(length(probs) - 1), sep = ' '), 'Average winnings per ticket')\nrownames(gameResult) <- paste('Game', 1:nGames, sep = ' ')\n\ngameResult %>%\n  knitr::kable()\n\n\n\nTable 1.2:  Simulation of five games with 100 tickets each. Number of tickets by outcome (0, 1, or 2 EUR) and average ticket win. \n \n  \n      \n    x = 0 \n    x = 1 \n    x = 2 \n    Average winnings per ticket \n  \n \n\n  \n    Game 1 \n    49 \n    34 \n    17 \n    0.68 \n  \n  \n    Game 2 \n    38 \n    36 \n    26 \n    0.88 \n  \n  \n    Game 3 \n    47 \n    31 \n    22 \n    0.75 \n  \n  \n    Game 4 \n    49 \n    26 \n    25 \n    0.76 \n  \n  \n    Game 5 \n    62 \n    23 \n    15 \n    0.53 \n  \n\n\n\n\n\n\nThe probabilities in Table 1.1 completely describe the two games. The functions that assigns a probability to each possible outcome p(x) and p(y) are called probability mass functions. These functions incorporate everything there is to know about our hypothetical games. While this knowledge will not guarantee you a profit from gambling, it enables you to compute the expected value of each ticket, the probability that none of your tickets will win, etc. An important property of the probability mass function is that it is always non-negative (no negative probabilities) and that the sum of the probabilities over all possible values is exactly 1.\n\nDefinition 1.2 (Probability mass function) For a discrete random variable X with possible values x_1,\\ldots,x_K a function p(x_i) that assigns a probability to the possible values of X is called a probability mass function.\n\n\\begin{align}\n  P(X = x_k) = p_k.\n\\end{align}\n The probabilities are real numbers in the interval [0, 1] and need to sum to 1 over all possible values.\n\\begin{align}\n  p_k \\geq 0\\\\\n  \\sum_{k = 1} ^ {K} p_k = 1.\n\\end{align}\n\nNote that the set of K possible values x_1,\\ldots,x_K is finite. The same definition can be used for infinite sets of possible values as long as these are countably infinite but we will skip this discussion.\n\nExample 1.3 (Probability mass function) The first game in Table 1.1 has three possible outcomes: x_1 = 0, x_2 = 1, x_3 = 2. The probability mass function is then\n\np(x) = \\begin{cases}\n0.5 & \\text{if} \\quad x = 0 \\\\\n0.3 & \\text{if} \\quad x = 1 \\\\\n0.2 & \\text{if} \\quad x = 2 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\n\nA common visualization of the probability mass function is a barchart with the heights of the bars corresponding to the probability of each outcome.\n\ntibble(\n  x = c(0:2),\n  p = c(0.5, 0.3, 0.2)\n) %>%\n  ggplot(aes(x = x, y = p)) + \n    geom_bar(stat = \"identity\")"
  },
  {
    "objectID": "02-Statistics-Review.html#expected-value",
    "href": "02-Statistics-Review.html#expected-value",
    "title": "1  Statistics Review (Optional)",
    "section": "1.4 Expected value",
    "text": "1.4 Expected value\nJust as the descriptive statistics (average, empirical median, empirical quantiles, etc.) are useful for summarizing a set of numbers we would like to be able to summarize distribution functions.\nImagine that you plan to buy 100 tickets from the first game in Table 1.1. Based on the interpretation of probabilities as theoretical proportions you would expect that 50 of the tickets will win nothing, 30 of the tickets will bring you 1 EUR and 20 of the tickets will win 2 EUR. Thus you can write the expected winnings per ticket by summing the contributions of each ticket type:\n\n\\begin{align}\n  \\text{expected winnings} & =\n    \\underbrace{\\frac{50}{100} \\times 0\\text{ EUR}}_{0\\text{ EUR tickets}} +\n    \\underbrace{\\frac{30}{100} \\times 1\\text{ EUR}}_{1\\text{ EUR tickets}} +\n    \\underbrace{\\frac{20}{100} \\times 2\\text{ EUR}}_{2\\text{ EUR tickets}} \\\\\n    & = 0.5 \\times 0 \\text{EUR} + 0.3 \\times 1 \\text{EUR} + 0.2 \\times 2 \\text{EUR} = 0.7\\text{EUR}.\n\\end{align}\n\\tag{1.1}\nNote that the coefficients before the possible outcomes are simply their probabilities. Therefore in a game with 100 tickets you expect that each ticket will bring you 0.7 EUR (on average). Just as with the probabilities, the expected values does not tell you that your average win per ticket will be 0.7 EUR. If you take a look at the five simulated games in Table 1.2 you will notice that the realized average ticket wins are not equal to 0.7 but they vary around it. You can think about the expected value as the center of the distribution (see Table 1.2 and Freedman, Pisani, and Purves (2007), pp. 288). It is important to see that the expected value only depends on the probabilities and the possible values and it does not depend on the outcome of any particular game. Therefore it is a constant and not a random variable itself.\n\n\n\n\n\nFigure 1.1: Probabilities plot. The black vertical line depicts the expected value.\n\n\n\n\n\nLet us write the expected value in a more general way:\n\nDefinition 1.3 (Expected Value) For discrete random variable X with possible values x_1, x_2,\\ldots,x_n the weighted average of the possible outcomes:\n\nE(X) = \\sum_{i = 1} ^ {n} x_i p(x_i)\n\nis called the expected value of X. Sometimes we will refer to the expected value as the mean of the distribution or the mean of the random variable following the distribution.\n\nWe introduced the expected value with an example of a game with 100 tickets in order to illustrate it. You should notice from Definition 1.3 that the expected value is independent of the number of games played as it is a property of the probability mass function of the game.\n\nExercise 1.1 (Expected Value) Let Y be a game with possible winnings of -1, 0, and 2 EUR. The probabilities of these outcomes are P(x = -1) = 0.2, P(x = 0) = 0.7, P(x = 2) = 0.1.\n\nPlot the PMF using a bar chart\nCalculate the expected winnings of this game using Definition 1.3\n\nPlay the game using the function sample (check its documentation either by typing ?sample on the command line in R or by searching for it online). It selects size of the values specified in x according to the probabilities given in prob.\n\nsample(\n  x = c(-1, 0, 2), \n  size = 10, \n  prob = c(0.2, 0.7, 0.1),\n  replace = TRUE\n)\n\n [1]  0  0  0 -1  0  0  0  0 -1  0\n\n\nPlay the game a couple of times and look at the outcomes. Compute the average winnings using mean and compare the result with the expected value that you calculated in the previous step.\n\n\n1.4.1 Properties of the expected value\nIn the following we list a few important properties of the expected value that we will use throughout the course. In the following let X and Y be random variables with expected values E(X) and E(Y).\n\nTheorem 1.2 (Linearity of the Expected Value) Let X and Y be two random variables. Then the expected value of their sum equals the sum of their expected values.\n\\begin{align}\n  E(X + Y) = E(X) + E(Y)\n\\end{align}\n\n\nTheorem 1.3 (The Expected Value of a Constant) The expected value of a constant equals the constant itself. Let a be any fixed (not random) real number. Then its expected value is:\n\\begin{align}\n  E(a) = a.\n\\end{align}\n\n\nTheorem 1.4 (Expected value of a scaled random variable) Let X be a random variable and let a be any fixed (not random) real number. Then the expected value of aX is:\n\\begin{align}\n  E(aX) = a E(X)\n\\end{align}\n\n\nProof. \n\\begin{align}\n  E(aX) = \\sum_{i = 1} ^ {n} a x_i p(x_i) = a \\sum_{i = 1} ^ {n} x_i p(x_i) = aE(X).\n\\end{align}"
  },
  {
    "objectID": "02-Statistics-Review.html#variance",
    "href": "02-Statistics-Review.html#variance",
    "title": "1  Statistics Review (Optional)",
    "section": "1.5 Variance",
    "text": "1.5 Variance\nLet us compare the two games in Table 1.1. Both have the same sample space (set of possible outcomes) and the same expected winnings; see Equation 1.1 and ?eq-expected-value-game-2. However, the games are not identical because their probability distributions are different. If given the choice to play only one game, which would you prefer?\nThe second game offers a higher probability of winning the highest prize (2 EUR) at the cost of a lower probability for the middle prize (1 EUR). In other words, it places a higher probability on extreme outcomes (far from the distribution’s center, i.e., the expected value). A summary of a distribution that measures its spread (i.e., how likely are extreme values) is the variance:\n\nDefinition 1.4 (Variance) For a discrete random variable X with possible outcomes x_1, x_2,\\ldots,x_n and probability function p(x) the variance is the expected quadratic deviation from the expected value of the distribution E(X).\n\n\\begin{align}\n  Var(X) & = E(X - E(X)) ^ 2 \\\\\n         & = \\sum_{i = 1}  ^ n \\left(x_i - E(X)\\right) ^ 2 p(x_i)\n\\end{align}\n\n\n\nExample 1.4 (Variance) The variance of the first game (X) is:\n\n\\begin{align}\n    Var(X) & = \\sum_{i = 1} ^ {3} (x_i - E(X)) ^ 2 p(x_i) \\\\\n           & = (x_1 - E(X)) ^ 2 p(x_i) + (x_2 - E(X)) ^ 2 p(x_2) + (x_3 - E(X)) ^ 2 p(x_3) \\\\\n           & = (0 - 0.7) ^ 2 \\times 0.5 + (1 - 0.7) ^ 2 \\times 0.3 + (2 - 0.7) ^ 2 \\times 0.2 \\\\\n           & = 0.61.\n  \\end{align}\n\n\n\nExercise 1.2 Compute the variance of Y, the second game described in Table 1.1.\n\n\nSolution. Var(Y) = 0.76.\n\n\nTheorem 1.5 (Independence of Random Variables) Two discrete random variables X and Y with possible values x_1,\\ldots,x_K and y_1,\\ldots,y_L are independent if for every k and l:\n\n\\begin{align}\n  P(X = x_k, Y = y_l) = P(X = x_k)P(Y = y_l)\n\\end{align}\n\n\n\nTheorem 1.6 For two independent random variables X and Y.\n\\begin{align}\n  Var(X + Y) = Var(X) + Var(Y)\n\\end{align}\nMore generally, if X_i, i = 1,\\ldots,n are independent random variables, the variance of their sum is equal to the sum of their variances:\n\n\\begin{align}\nVar\\left(\\sum_{i = 1}^{n} X_i\\right) = \\sum_{i = 1}^{n} Var(X_i).\n\\end{align}\n\n\n\nTheorem 1.6 actually holds even for only uncorrelated variables. We will discuss the concept correlation later.\n\nTheorem 1.7 (Variance of a Scaled Random Variable) Let a \\in \\mathbf{R} be a fixed (not random) real number and let X be a random variable with variance Var(X).\nThe variance of aX is given by:\n\\begin{align}\nVar(aX) = a^2 Var(X).\n\\end{align}\n\n\nProof. \n\\begin{align}\n  Var(aX) & = \\sum_{k = 1} ^ K (ax_k - E(aX))^2 p(x_k) \\\\\n          & = \\sum_{k = 1} ^ K a^2(x_k - E(X))^2 p(x_k) \\\\\n          & = a^2 \\sum_{k = 1} ^ K a^2(x_k - E(X))^2 p(x_k) \\\\\n          & = a^2 Var(X).\n\\end{align}\n\nAnother useful formula for working with the variance is:\nOften it will be easier to compute the variance using the following decomposition: \\begin{align}\n  Var(X) & = E(X - E(X))^2 \\\\\n         & = E\\left(X^2 - 2XE(X) + E(X) ^ 2\\right) \\\\\n         & = E(X^2) - E(2XE(X)) + E(E(X) ^ 2) \\\\\n         & = E(X^2) - 2 E(X)E(X) + E(X) ^ 2 \\\\\n         & = E(X^2) - E(X)^2.\n\\end{align}\nThe proof above uses the fact that E(X) is a constant and applies Theorem 1.3 and Theorem 1.4.\n\n\n\n\nBertsekas, Dimitri P., and John N. Tsitsiklis. 2008. Introduction to Probability. 2nd ed. Optimization and Computation Series. Belmont: Athena scientific.\n\n\nFreedman, David, Robert Pisani, and Roger Purves. 2007. Statistics. 4th ed. New York: W.W. Norton & Co."
  },
  {
    "objectID": "01-Introduction-to-R.html#arithmetic-operations",
    "href": "01-Introduction-to-R.html#arithmetic-operations",
    "title": "2  Introduction to R",
    "section": "2.1 Arithmetic Operations",
    "text": "2.1 Arithmetic Operations\n\n1 + 4\n\n[1] 5\n\n3 - 2\n\n[1] 1\n\n2 * 8\n\n[1] 16\n\n2 / 8\n\n[1] 0.25\n\n2^4\n\n[1] 16"
  },
  {
    "objectID": "01-Introduction-to-R.html#assignment",
    "href": "01-Introduction-to-R.html#assignment",
    "title": "2  Introduction to R",
    "section": "2.2 Assignment",
    "text": "2.2 Assignment\nVery often we want to store a value in the memory of the computer so that we can reuse it later. In R we store values under names (variables) by using the assignment operator `<-` Shortcut for the assignment operator: Alt - (minus)\n\ny <- 34\ny - 40\n\n[1] -6\n\n\nRun this chunks and look at the global environment (right side of R Studio) to see it appear\nin the list of objects."
  },
  {
    "objectID": "01-Introduction-to-R.html#vectors",
    "href": "01-Introduction-to-R.html#vectors",
    "title": "2  Introduction to R",
    "section": "2.3 Vectors",
    "text": "2.3 Vectors\nIt is very common to group values that belong together in a single structure.\n\nx <- c(1, 4)\n\n\n## Length, average, sum of a numeric vector\nmean(x)\n\n[1] 2.5\n\nsum(x)\n\n[1] 5\n\nlength(x)\n\n[1] 2\n\n\n\n## Documentation\n?mean"
  },
  {
    "objectID": "01-Introduction-to-R.html#character-values",
    "href": "01-Introduction-to-R.html#character-values",
    "title": "2  Introduction to R",
    "section": "2.4 Character values",
    "text": "2.4 Character values\n\nz <- \"Hello, world!\""
  },
  {
    "objectID": "01-Introduction-to-R.html#logical-values-and-logical-operators",
    "href": "01-Introduction-to-R.html#logical-values-and-logical-operators",
    "title": "2  Introduction to R",
    "section": "2.5 Logical values and logical operators",
    "text": "2.5 Logical values and logical operators\nThere are two logical values: TRUE and FALSE. These emerge from logical operations and indicate whether some condition is fulfilled (TRUE) or not FALSE. You will find similar constructs in all other languages, where this type of data is commonly known as boolean or binary (i.e., only two values).\nThe basic logical operators in R are\n\n## Less than\n2 < 5\n\n[1] TRUE\n\n## Less than or equal\n2 <= 5\n\n[1] TRUE\n\n## Greater than\n2 > 5\n\n[1] FALSE\n\n## Greater or equal\n2 >= 5\n\n[1] FALSE\n\n## Exactly equal\n2 == 5\n\n[1] FALSE\n\n\"Text 2\" == \"Text 2\"\n\n[1] TRUE\n\n\n\nz == \"Text 2\"\n\n[1] FALSE"
  },
  {
    "objectID": "01-Introduction-to-R.html#data",
    "href": "01-Introduction-to-R.html#data",
    "title": "2  Introduction to R",
    "section": "2.6 Data",
    "text": "2.6 Data\nIn most of our work we will use data tables containing variables (columns) that describe characteristics of observations (rows). Most of the time we will use tibble objects to hold the data. tibble objects are a modern rewrite of the data.frame (an older object type for storing data).\nTo use it we need to load the tidyvsere packages\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.4.2     ✔ purrr   1.0.1\n✔ tibble  3.2.1     ✔ dplyr   1.1.2\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nIn a limited number of cases we need to construct tables by hand. You can find out more about tibble here.\n\ndt <- tibble(\n  ## Shorthand syntax for creating a sequence of integers from one to five\n  id = 1:5,\n  y = c(2, 2.5, 3, 8, 12)\n)\ndt\n\n# A tibble: 5 × 2\n     id     y\n  <int> <dbl>\n1     1   2  \n2     2   2.5\n3     3   3  \n4     4   8  \n5     5  12  \n\n\nMost of our data will come from external sources such as text files in a csv format. For the purpose of this course you don’t need to worry about reading these files, you will always have a starter code chunk that imports the data.\nThe earnings data set contains data on 1816 customers of a shopping mall. The customers have answered a short interview and gave information about their sex, age, ethnicity, annual income, weight and height.\nWe will use this data set to demonstrate some common operations and basic data summaries.\n\nearnings <- read_csv(\"https://raw.githubusercontent.com/feb-uni-sofia/econometrics2021/main/data/earnings.csv\")\n\nRows: 1816 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): ethnicity\ndbl (14): height, weight, male, earn, earnk, education, mother_education, fa...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nheight (numeric): Height in inches (1 inch = 2.54 cm)\nweight (numeric): Weight in pounds (1 pound \\approx 0.45 kilograms)\nmale (numeric): 1: Male, 0: Female\nearn (numeric): Annual income in USD\nearnk (numeric): Annual income in 1,000 USD\nethnicity (character): Ethnicity\nage (numeric): Age\n\nFirst we will convert the height and weight measurements from their original scales (inch, pound) to cm and kg. We will create two new columns with informative names using the mutate function.\n\nearnings <- mutate(\n  earnings,\n  height_cm = 2.54 * height,\n  weight_kg = 0.45 * weight\n)\nearnings1 <- select(earnings, height_cm, weight_kg)\n\nThe same code can be rewritten in a more convenient way using pipes.\n\nearnings1 <- earnings %>%\n  mutate(\n    height_cm = 2.54 * height,\n    weight_kg = 0.45 * weight\n  ) %>%\n  select(height_cm, weight_kg)\n\nNote that the object holding the original data is unaffected by mutate and select. The reason for this is that functions in R generally do not change their arguments. If you want to add the two new columns to the original data set earnings, you need to overwrite it with an assignment."
  },
  {
    "objectID": "01-Introduction-to-R.html#basic-data-summaries",
    "href": "01-Introduction-to-R.html#basic-data-summaries",
    "title": "2  Introduction to R",
    "section": "2.7 Basic data summaries",
    "text": "2.7 Basic data summaries\nThe first step in any data analysis is to gain an initial understanding of the context of the data and the distributions of the variables of interest. In this course our main focus will be on two features of the variables: their location and their variability (how different are the observations between each other).\n\n2.7.1 Location\nThe most important measure of location for us will be the empirical mean of a variable (arithmetic average). Let i index the observation in our data set from the first (i = 1) to the last i = n. In our case n = 1816: the number of all interviewed customers. We can represent the values of some (numeric) characteristic (e.g., the persons’ weight) as a vector of values x = (x_1, \\ldots, x_n). In this notation x_1 is the weight of the first customer in the data set (x_1 = 210 pounds). The arithmetic average is defined as the sum of all values divided by the number of observations:\n\n\\bar{x} = \\frac{1}{n}(x_1 + x_2 + \\ldots + x_n) = \\frac{1}{n}\\sum_{i = 1}^{n} x_i\n\nLet us now compute the arithmetic average of weight and height. One way to access the columns of the data set earnings is to write the name of the data set and then after a $ sign the name of the column.\n\nmean(earnings$height)\n\n[1] 66.56883\n\nmean(earnings$weight, na.rm = TRUE)\n\n[1] 156.3052\n\n\nAnother measure of location is the (empirical) median. You can compute it using the median function.\n\nmedian(earnings$height)\n\n[1] 66\n\n\nThe result is a median height of 66 inches. This means that about half of the customers were taller than 66 inches.\n\n\n2.7.2 Variability\nThe next important feature of the data is its variability. It answers the following question: how different are the customers between each other with respect to body height (for example). There are numerous ways to measure variability.\nOne intuitive measure would be the range of the data, defined as the difference by the maximal observed height and the minimal observed height\n\nmin(earnings$height)\n\n[1] 57\n\nmax(earnings$height)\n\n[1] 82\n\nrange(earnings$height)\n\n[1] 57 82\n\n\n\nmax(earnings$height) - min(earnings$height)\n\n[1] 25\n\n\nAnother measure is the inter-quartile range. The quartiles are defined similar to the median. To see this lets use the example of body height. The first quartile of height (25-th percentile and 0.25 quantile are different names for the same thing) is the height for which about one quarter of the customers are shorter than it. You can compute it with the function quantile.\n\nquantile(earnings$height, 0.25)\n\n25% \n 64 \n\n\nAbout 25 percent of our customers were shorter than 64 inches.\nThe second quartile is the same as the median (two quarters).\n\nquantile(earnings$height, 0.5)\n\n50% \n 66 \n\nmedian(earnings$height)\n\n[1] 66\n\n\nThe third quartile is the height for which three quarter of the customers are shorter than it.\n\nquantile(earnings$height, 0.75)\n\n  75% \n69.25 \n\n\nIn our case about three quarter of the customers were shorter than 69.25 inches.\nThe inter-quartile range is simply the difference between the third quartile and the second quartile.\n\nquantile(earnings$height, 0.75) - quantile(earnings$height, 0.25)\n\n 75% \n5.25 \n\n\nAbout half of our customers had a hight between the first quartile (64 inches) and the third quartile (69.25 inches). The inter-quartile range shows you the difference between the height of the talles person and the shortest person for the central 50 percent of the customers.\nAs the range, the inter-quartile range is a measure of variability.\nThe most important measure of variability and the one that will be central to our analysis is the (empirical) variance.\n\nDefinition 2.1 (Empirical variance) For a vector of values x = (x_1, \\ldots, x_n) it is defined as the average (apart from a small correction in the denominator) squared deviation of the values from their mean.\n\nS^2_x = \\frac{(x_1 - \\bar{x})^2 + \\ldots + (x_n - \\bar{x}^2)}{n - 1} = \\frac{1}{n - 1} \\sum_{i = 1}^{n}(x_i - \\bar{x})^2: \\quad \\text{variance}\\\\\nS_x = \\sqrt{S^2_x} \\quad \\text{standard deviation}\n\n\n\nExample 2.1 (Computing the empirical variance) Lets apply the formula from Definition 2.1 to a very simple example with just three values.\n\nx_1 = -1, x_2 = 0, x_3 = 4\n\nFirst, the empirical mean of these values is\n\n\\bar{x} = \\frac{-1 + 0 + 4}{3} = 1\n\nNow lets substitute these values in the definition of the empirical variance:\n\n\\begin{aligned}\nS^2_{x} & = \\frac{(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + (x_3 - \\bar{x})^2 }{n - 1} \\\\\n        & = \\frac{(-1 - 1)^2 + (0 - 1)^2 + (4 - 1)^2 }{3 - 1} \\\\\n        & = \\frac{(-2)^2 + (- 1)^2 + (3 )^2 }{2} \\\\\n        & = \\frac{4 + 1 + 9 }{2} \\\\\n        & = \\frac{14}{2} \\\\\n        & = 7\n\\end{aligned}\n Using R to compute the same thing:\n\nx <- c(-1, 0, 4)\nx_avg <- mean(x)\n\n((-1 - x_avg)^2 + (0 - x_avg)^2 + (4 - x_avg)^2) / (length(x) - 1)\n\n[1] 7\n\n\nThere is also a special function var that can compute it from a vector\n\nvar(x)\n\n[1] 7\n\n\nThe (empirical) standard deviation is simply the square root of the (empirical) variance.\n\nS_x = \\sqrt{S^2_x} = \\sqrt{7} \\approx 2.64\n\nIn R you have two options: take the square root of the result of var using the sqrt function or use sd (standard deviation) to compute the standard deviation directly.\n\nsqrt(var(x))\n\n[1] 2.645751\n\nsd(x)\n\n[1] 2.645751\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe following contains code discussed during the classes without additional explanations\n\n\n\n## Basic summaries for the whole tibble\nearnings %>% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n1816\n\n\nNumber of columns\n17\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n16\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nethnicity\n0\n1\n5\n8\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nheight\n0\n1.00\n66.57\n3.83\n57.00\n64.00\n66.00\n69.25\n82.00\n▂▇▅▁▁\n\n\nweight\n27\n0.99\n156.31\n34.62\n80.00\n130.00\n150.00\n180.00\n342.00\n▅▇▃▁▁\n\n\nmale\n0\n1.00\n0.37\n0.48\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▅\n\n\nearn\n0\n1.00\n21147.30\n22531.77\n0.00\n6000.00\n16000.00\n27000.00\n400000.00\n▇▁▁▁▁\n\n\nearnk\n0\n1.00\n21.15\n22.53\n0.00\n6.00\n16.00\n27.00\n400.00\n▇▁▁▁▁\n\n\neducation\n2\n1.00\n13.24\n2.56\n2.00\n12.00\n12.00\n15.00\n18.00\n▁▁▁▇▃\n\n\nmother_education\n244\n0.87\n13.61\n3.22\n3.00\n12.00\n13.00\n16.00\n99.00\n▇▁▁▁▁\n\n\nfather_education\n295\n0.84\n13.65\n3.25\n3.00\n12.00\n13.00\n16.00\n99.00\n▇▁▁▁▁\n\n\nwalk\n0\n1.00\n5.30\n2.60\n1.00\n3.00\n6.00\n8.00\n8.00\n▃▁▃▁▇\n\n\nexercise\n0\n1.00\n3.05\n2.32\n1.00\n1.00\n2.00\n5.00\n7.00\n▇▁▁▁▃\n\n\nsmokenow\n1\n1.00\n1.75\n0.44\n1.00\n1.00\n2.00\n2.00\n2.00\n▃▁▁▁▇\n\n\ntense\n1\n1.00\n1.42\n2.16\n0.00\n0.00\n0.00\n2.00\n7.00\n▇▁▁▁▁\n\n\nangry\n1\n1.00\n1.42\n2.16\n0.00\n0.00\n0.00\n2.00\n7.00\n▇▁▁▁▁\n\n\nage\n0\n1.00\n42.93\n17.16\n18.00\n29.00\n39.00\n56.00\n91.00\n▇▇▃▃▁\n\n\nheight_cm\n0\n1.00\n169.08\n9.73\n144.78\n162.56\n167.64\n175.89\n208.28\n▂▇▅▁▁\n\n\nweight_kg\n27\n0.99\n70.34\n15.58\n36.00\n58.50\n67.50\n81.00\n153.90\n▅▇▃▁▁\n\n\n\n\n\n\ntable(earnings$ethnicity)\n\n\n   Black Hispanic    Other    White \n     180      104       38     1494"
  },
  {
    "objectID": "01-Introduction-to-R.html#visualizations",
    "href": "01-Introduction-to-R.html#visualizations",
    "title": "2  Introduction to R",
    "section": "2.8 Visualizations",
    "text": "2.8 Visualizations\nHistogram\n\nearnings %>%\n  ggplot(aes(x = height)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nA smooth density plot is an alternative way to visualize the distribution of a variable.\n\nearnings %>%\n  ggplot(aes(x = height)) +\n  geom_density()\n\n\n\n\nThe boxplot shows the median and the 25-th and 75-th percentiles (the box). The whiskers in the plot stretch to the minimum or the maximum observed value, unless there are extreme observations that are shown as single dots.\n\nearnings %>%\n  ggplot(aes(x = height)) +\n  geom_boxplot()\n\n\n\n\nGroup comparisons\n\nearnings %>%\n  ggplot(aes(x = height, y = ethnicity)) +\n  geom_boxplot()\n\n\n\n\nThe scatterplot will be our primary tool in studying associations between variables. It represents each observation as a point in a coordinate system defined by the variables that we would like to study.\n\nearnings1 %>%\n  ggplot(aes(x = weight_kg, y = height_cm)) +\n  geom_point(position = \"jitter\", alpha = 0.5) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"Weight (kg)\",\n    y = \"Height (cm)\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 27 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 27 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\nsummary(lm(height_cm ~ weight_kg, data = earnings1))\n\n\nCall:\nlm(formula = height_cm ~ weight_kg, data = earnings1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-27.639  -5.611  -0.084   5.645  36.248 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 145.00916    0.89138  162.68   <2e-16 ***\nweight_kg     0.34314    0.01237   27.73   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.15 on 1787 degrees of freedom\n  (27 observations deleted due to missingness)\nMultiple R-squared:  0.3009,    Adjusted R-squared:  0.3005 \nF-statistic: 769.1 on 1 and 1787 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "02a-Continuous-Distributions.html#uniform-distributions",
    "href": "02a-Continuous-Distributions.html#uniform-distributions",
    "title": "3  Continuous Distributions",
    "section": "3.1 Uniform distributions",
    "text": "3.1 Uniform distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe uniform density is constant over the interval [a, b] (the parameters of the distribution). Figure 3.1 shows the density of the uniform distribution over [-1, 1].\n\nunif_dens_plt <- ggplot() +\n  # Sets the range of the x-axis from -2 to 2\n  xlim(c(-2, 2)) +\n  stat_function(\n    fun = dunif, \n    args = list(\n      min = -1,\n      max = 1\n      ),\n    n = 1000\n  ) +\n  labs(\n    x = \"x\",\n    y = \"Density\"\n  )\nunif_dens_plt\n\n\n\n\nFigure 3.1: Density function of the uniform distribution over [-1, 1].\n\n\n\n\n\n3.1.1 Probabilities\nThe area under the density function for an interval is the probability of occurrence of outcomes in this interval. It is easy to calculate the area under Figure 3.1 for the interval [-1, -0.2. The height of the density function is constant at 1/2. The length of the interval is 0.8, therefore the area of the rectangle is 0.8 \\cdot 1/2 = 0.4 Figure 3.2.\n\nunif_dens_plt + \n  geom_ribbon(\n    data = tibble(\n      x = c(-1, -0.2),\n      ymin = 0,\n      y = 0,\n      ymax = 0.5\n    ),\n    aes(x = x, y = y, ymin = ymin, ymax = ymax),\n    fill = \"steelblue\",\n    alpha = 0.2\n  ) + \n  annotate(\"text\", x = -0.6, y = 0.3, label = \"P(X < -0.2) = 0.4\") + \n  scale_x_continuous(breaks = c(-1, -0.2, 1), limits = c(-2, 2))\n\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale.\n\n\n\n\n\nFigure 3.2: Probability of occurence in the interval [-1, -0.2].\n\n\n\n\n\nP(X < -0.2) = 0.4\n\nYou can also compute this probability using the punif function. By default it computes probabilities of the type P(X < x), where x is its first argument. Note that you also need to specify the limits of the distribution ([-1, 1] in this case).\n\npunif(-0.2, min = -1, max = 1)\n\n[1] 0.4\n\n\nLet us compute the probabilities of another event: X > 5. Again, the probability of an interval is given by the area under the density over that interval. The length of the interval is 0.5, the height of the density is the same as before (1/2). Therefore the area of the rectangle is 0.5 \\cdot 0.5 = 0.25 Figure 3.3.\n\nunif_dens_plt + \n  geom_ribbon(\n    data = tibble(\n      x = c(0.5, 1),\n      ymin = 0,\n      y = 0,\n      ymax = 0.5\n    ),\n    aes(x = x, y = y, ymin = ymin, ymax = ymax),\n    fill = \"steelblue\",\n    alpha = 0.2\n  ) + \n  annotate(\"text\", x = 0.7, y = 0.3, label = \"P(X > 0.5) = 0.25\") + \n  scale_x_continuous(breaks = c(-1, 0.5, 1), limits = c(-2, 2))\n\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale.\n\n\n\n\n\nFigure 3.3: Probability of occurence in the interval [0.5, 1].\n\n\n\n\nTo compute this probability using R you have two options. By default punif computes probabilities for intervals of the type (-\\infty, x]. Running the following code will give you the probability of P(X < 0.5).\n\npunif(0.5, min = -1, max = 1)\n\n[1] 0.75\n\n\nYou can use the fact that the total area under the density is one.\n\nP(X > 0.5) = 1 - P(X < 0.5)\n\n\n1 - punif(0.5, min = -1, max = 1)\n\n[1] 0.25\n\n\nOr you can instruct punif to compute probabilities of the type P(X > x) using its lower.tail argument.\n\npunif(0.5, min = -1, max = 1, lower.tail = FALSE)\n\n[1] 0.25\n\n\nFinally, lets compute the following probability:\n\nP(-0.2 < X < 0.4)\n The area of the rectangle is (0.4 - (-0.2)) \\cdot 0.5 = 0.3 Figure 3.4.\n\nunif_dens_plt + \n  geom_ribbon(\n    data = tibble(\n      x = c(-0.2, 0.4),\n      ymin = 0,\n      y = 0,\n      ymax = 0.5\n    ),\n    aes(x = x, y = y, ymin = ymin, ymax = ymax),\n    fill = \"steelblue\",\n    alpha = 0.2\n  ) + \n  annotate(\"text\", x = 0.2, y = 0.3, label = \"P(-0.2 < X < 0.4) = 0.3\") + \n  scale_x_continuous(breaks = c(-1, -0.2, 0.4, 1), limits = c(-2, 2))\n\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale.\n\n\n\n\n\nFigure 3.4: Probability of occurence in the interval [-0.2, 0.4].\n\n\n\n\nTo use punif we will avail ourselfs of the fact that\n\nP(x_1 < X < x_2) = P(X < x_2) - P(X < x_1)\n\n\npunif(0.4, min = -1, max = 1) - punif(-0.2, min = -1, max = 1)\n\n[1] 0.3\n\n\n\n\n3.1.2 Sampling\nIn order to illustrate the concept of probability it is useful to select values at random from a distribution and to observe their behavior. To select values ot random from the uniform distribution with limits a = -1 and b = 1 we can use the runif function. The number of values selected is controlled by the n argument, while the parameters of the distribution are specified in min (a) and max (b).\n\n## Select n = 5 values at random from the uniform distribution over [-1, 1]\nrunif(n = 5, min = -1, max = 1)\n\n[1] -0.6005076  0.7321707 -0.2437995 -0.4477999  0.1429172\n\n\nNow that you’ve seen what runif does, lets create a larger simulation which we will use in the examples. We will select n = 200 values at random from the same distribution and we will store these values as a column in a data table using the function tibble.\n\nunif_sim <- tibble(\n  x = runif(n = 200, min = -1, max = 1)\n)\n\nThe following code plots the values in x as small lines just over the x-axis (a rug plot).\n\n## Code for illustration only\n\nunif_dens_plt +\n  geom_rug(aes(x = x), data = unif_sim)\n\n\n\n\nThe area under the density function for an interval is the probability of occurrence of outcomes in this interval. It is easy to calculate the area under Figure 3.1 for the interval [-1, -0.2]. The height of the density function is constant at 1/2. The length of the interval is 0.8, therefore the area of the rectangle is 0.8 \\cdot 1/2 = 0.4.\n\nP(X < -0.2) = 0.4\n You can also compute this probability using the punif function. By default it computes probabilities of the type P(X < x), where x is its first argument. Note that you also need to specify the limits of the distribution ([-1, 1] in this case).\n\n## Compute the probability\npunif(-0.2, min = -1, max = 1)\n\n[1] 0.4\n\n\nEarlier we used runif to select a couple of values at random from this distribution and stored the result in an object called x. Let us compare the observed proportion of values less than -0.2 to the theoretical probability.\n\n## Use < to see which values in x are less than -0.2\nhead(unif_sim$x < -0.2)\n\n[1]  TRUE  TRUE FALSE  TRUE FALSE FALSE\n\n\n\n## Compute the sample proportion in the simulated data\n## This relies on the following conversion of the logical values\n## TRUE -> 1, FALSE -> 0. This conversion happens under the hood \n## in the mean function\n\nmean(unif_sim$x < -0.2)\n\n[1] 0.385\n\n\nAs an exercise, increase the number of games plated (change the size argument in runif) and compare the observed proportion to the probability.\n\nunif_sim %>%\n  ggplot(aes(x = x)) +\n  geom_histogram(breaks = c(-1, -0.6, -0.2, 0.2, 0.6, 1)) +\n  xlim(c(-1, 1))"
  },
  {
    "objectID": "02a-Continuous-Distributions.html#normal-distributions",
    "href": "02a-Continuous-Distributions.html#normal-distributions",
    "title": "3  Continuous Distributions",
    "section": "3.2 Normal distributions",
    "text": "3.2 Normal distributions\nBy far the most important distribution in this course is the normal distribution. When we say a normal distribution we usually mean the family of normal distributions which described by two parameters \\mu and \\sigma^2.\n\nX \\sim N(\\mu, \\sigma^2) \\\\\nf(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}\\right)^2}\\\\\nE(X) = \\mu \\\\\nVar(X) = \\sigma^2 \\\\\nSD(X) = \\sqrt{\\sigma^2} = \\sigma"
  },
  {
    "objectID": "02a-Continuous-Distributions.html#parameters-of-the-normal-distribution",
    "href": "02a-Continuous-Distributions.html#parameters-of-the-normal-distribution",
    "title": "3  Continuous Distributions",
    "section": "3.3 Parameters of the normal distribution",
    "text": "3.3 Parameters of the normal distribution\nThe first parameter of the normal distribution (\\mu) controls the location (center) of the distribution. The second parameter (\\sigma) controls the spread (variance) of the distribution.\n\ndt <- expand_grid(\n  ## Creates a sequence of 100 numbers between -3 and 3\n  x = seq(-5, 5, length.out = 500),\n  mu = c(-2, 2),\n  sigma = c(0.5, 1)\n) %>%\n  mutate(\n    ## Computes the standard normal density at each of the 100 points in x\n    norm_dens = dnorm(x, mean = mu, sd = sigma),\n    mu = factor(mu),\n    sigma = factor(paste(\"sigma =\", sigma))\n  )\n\nggplot() +\n  ## Draws the normal density line\n  geom_line(data = dt, aes(x = x, y = norm_dens, colour = mu)) + \n  facet_wrap(~ sigma) + \n  geom_vline(xintercept = c(-2, 2), lty = 2, alpha = 0.3) + \n  scale_x_continuous(breaks = c(-5, -2, 2, 5), limits = c(-5, 5)) +\n  labs(\n    y = \"Density\"\n  )\n\n\n\n\nFigure 3.5: Densities of normal distributions for different values of \\mu and \\sigma.\n\n\n\n\nThe normal distribution emerges as the limiting distribution of sums independent random variables (with finite variance). To illustrate this, consider a collection of players who play one and the same game repeatedly. Each game is independent from the other and a player can win any amount between 0 and 1 EUR or lose anything between 0 and 1 EUR with equal probability. The outcome (win or loss) of each game is thus uniformly distributed over [-1, 1]. The running total of each player is simply the cumulative sum of the outcomes of their games. Let X_{ik} denote the outcome of game k for player i. The running total is defined as:\n\n\\text{Total}_{ik} = \\sum_{t = 1}^{k} X_{it} = X_{i1} + \\ldots + X_{ik}\n Figure 3.6 shows the trajectories of 300 players playing a total of 16 games. While it is hard to predict where each individual player will end up, it turns out that we can be quite confident about how the collection of players will look like (in terms of total winnings).\n\n\n\n\n\nFigure 3.6: A simulation of 300 players playing 16 games with outcomes uniformly distributed on [-1, 1].\n\n\n\n\nFigures 3.7 and 3.8 show the distribution of winnings after the fourth and after the sixteenth game. These distributions appear to be roughly symmetric around zero with high probability close to the center and low probability for extreme winnings. It can be shown that for a large number of games this distribution is close to a normal distribution with \\mu = 0 and variance proportional to the number of games.\n\n\n\n\n\nFigure 3.7: Distribution of the running total of the players after the fourth game.\n\n\n\n\n\n\n\n\n\nFigure 3.8: Distribution of the running total of the players after the sixteenth game.\n\n\n\n\n\n3.3.1 Probabilities\nTODO\n\n\n3.3.2 Quantiles\nTODO"
  },
  {
    "objectID": "03-Simple-Linear-Model.html#interpretation-of-the-linear-equation",
    "href": "03-Simple-Linear-Model.html#interpretation-of-the-linear-equation",
    "title": "4  Simple Linear Model",
    "section": "4.1 Interpretation of the linear equation",
    "text": "4.1 Interpretation of the linear equation\nThe first thing that you should consider are the units of measurement of the various variables in your model. y, the processing time is measured in hours (you seed that in the data description), x is the number of invoices. Both sides of any equation need to have the same unit of measurement, otherwise the equation would not make any sense. Therefore the constant in the equation is measured in the same units as y (hours) and the coefficient of x must be in hours per invoice.\n\n\\begin{align*}\n\\mu(x) = E(y | x) \\left[\\text{hours}\\right] = 0.5 [\\text{hours}] + 0.01 \\left[\\frac{\\text{hours}}{\\text{invoice}}\\right] x [\\#\\text{invoices}]\n\\end{align*}\n\nIf you have trouble understanding the unit of measurement of \\beta_1, think about how you would calculate your rent for three months. Let’s say that you apartment rent is given in EUR per month and that you pay 500 EUR per month. Your total rent for three months would amount to:\n\n3 [\\text{months}] \\times 500 \\left[\\frac{\\text{EUR}}{\\text{month}}\\right] = 1500 [\\text{EUR}]\n\n(XXX, fix) In general, how you would interpret the interpretation of the model coefficients vary depending on the context of the model (the data). In this example you can think about the processing time as the costs of the firm for processing a given number of invoices.\nThe the \\beta_1 coefficient would correspond to the marginal cost as it describes how much the expected processing time would change for one additional invoice. The constant (\\beta_0) is the expected processing time for x = 0 invoices, thus it corresponds to (expected) fixed costs of the firm (production costs that do not depend on the actual number of invoices)."
  },
  {
    "objectID": "03-Simple-Linear-Model.html#sec-ols-intro",
    "href": "03-Simple-Linear-Model.html#sec-ols-intro",
    "title": "4  Simple Linear Model",
    "section": "4.2 Ordinary Least Squares",
    "text": "4.2 Ordinary Least Squares\nUp to now we have considered a model with known coefficients \\beta_0 and \\beta_1. In general, these coefficients are not known and we need to make reasonable guesses about them using the observed data. Lets compare two guesses about the coefficients. We will write a “hat” over the expected value of y to make clear that this is an estimated value and not the true (and unknown) \\mu_i.\n\n\\hat{\\mu}^{(1)}_i = 0.1 + 0.015 x_i \\quad \\text{(blue line)} \\\\\n\\hat{\\mu}^{(2)}_i =  0.2 + 0.005 x_i \\quad \\text{(red line)}\n\n\ninvoices %>%\nggplot(\n  aes(x = Invoices, y = Time)\n  ) +\n  geom_point() +\n  ## Draws the blue line\n  geom_abline(intercept = 0.1, slope = 0.015, color = \"steelblue4\") +\n  ## Draws the red line\n  geom_abline(intercept = 0.2, slope = 0.005, color = \"firebrick\") + \n  ## Sets the range of the y-axis. We use this here in order to\n  ## clearly see both lines\n  ylim(c(0, 5)) + \n  ## (Optional) write the equations as annotation in the plot\n  geom_text(\n    data = tribble(\n      ~x,  ~y,  ~label,\n      250, 4.5, \"mu_hat_1 = 0.1 + 0.015 x\",\n      200, 1.5, \"mu_hat_2 = 0.2 + 0.005 x\"\n    ),\n    aes(x = x, y = y, label = label)\n  )\n\n\n\n\nFigure 4.3: The lines corresponding to the two guesses about the model coefficients.\n\n\n\n\nClearly, the red line would provide poor predictions, because it would underestimate the processing time for each of the observed days (because all dots line above the line).\nThe blue line appears to fit the data much better, as it lies close to the points and does not systematically under or overestimate the observed processing time.\nLet us formalize the idea about closeness to the data. A model that produces “good” predictions should have “small” prediction errors for the observed values. Let the predictionerror be the difference between the observed processing time and the predicted processing time. We call this difference the residual for observation i.\n\nr_i = y_i - \\hat{\\mu}_i\n Let us calculate the residuals for the two models: red and blue. We will do this by adding four new columns to the invoices data set.\n\ninvoices <- invoices %>%\n  mutate(\n    ## Calculate the predictions using the model equation (blue)\n    mu_hat_1 = 0.1 + 0.015 * Invoices,\n    ## The residuals are the differences between observed value (Time) and predicted value\n    residuals_1 = Time - mu_hat_1,\n    ## We do the same for the second model (red)\n    mu_hat_2 = 0.2 + 0.005 * Invoices,\n    residuals_2 = Time - mu_hat_2\n  )\n\n\n## Select the first few observations (head)\nhead(invoices) %>%\n  ## (Optional) used to produce the html table that you see below\n  knitr::kable()\n\n\n\nTable 4.1: Predicted values and residuals for the two guesses. For the sake of brevity the table only shows the first few observations.\n\n\nDay\nInvoices\nTime\nmu_hat_1\nresiduals_1\nmu_hat_2\nresiduals_2\n\n\n\n\n1\n149\n2.1\n2.335\n-0.235\n0.945\n1.155\n\n\n2\n60\n1.8\n1.000\n0.800\n0.500\n1.300\n\n\n3\n188\n2.3\n2.920\n-0.620\n1.140\n1.160\n\n\n4\n23\n0.8\n0.445\n0.355\n0.315\n0.485\n\n\n5\n201\n2.7\n3.115\n-0.415\n1.205\n1.495\n\n\n6\n58\n1.0\n0.970\n0.030\n0.490\n0.510\n\n\n\n\n\n\nYou can see the full table with all predicted values and residuals by running the code above and clicking on the invoices object in the global environment in R Studio. In the scatterplot the residuals correspond to the vertical distance between the dots (observed processing time) and the two lines (predicted processing times). Figures 4.4 and 4.5 visualise these residuals.\n\ninvoices %>%\nggplot(aes(x = Invoices, y = Time)) +\n  # geom_point() +\n  geom_abline(intercept = 0.1, slope = 0.015, color = \"steelblue4\") +\n  ylim(c(0, 5)) +\n  geom_segment(aes(xend = Invoices, yend = mu_hat_1), lty = 2, alpha = 0.5, color = \"steelblue4\") +\n  geom_label(\n    aes(\n      label = round(residuals_1, 2)),\n      alpha = 0.5\n    )\n\n\n\n\nFigure 4.4: Residuals of the two models\n\n\n\n\n\ninvoices %>%\nggplot(aes(x = Invoices, y = Time)) +\n  # geom_point() +\n  geom_abline(intercept = 0.2, slope = 0.005, color = \"firebrick4\") +\n  ylim(c(0, 5)) +\n  geom_segment(aes(xend = Invoices, yend = mu_hat_2), lty = 2, alpha = 0.5, color = \"firebrick4\") +\n  geom_label(\n    aes(\n      label = round(residuals_2, 2)),\n      alpha = 0.5\n    )\n\n\n\n\nFigure 4.5: Residuals of the two models\n\n\n\n\nNow let us imagine that we pay the accounting firm a compensation each time that our prediction (for the observed days) is off-target (i.e. has a non-zero residuals). The accounting firm considers both underestimation and overestimation of the observed processing a bad thing and they know that any reasonable model such as the blue line would yield negative residuals for some of the observations and positive residuals for the rest of the observations.\nIf the fine is proportional to the sum of the residuals, the even terrible models can have a low residual sum (and we will get away with a low fine), because positive and negative residuals will cancel in the sum. If you are not convinced, consider the following (much simpler) model\n\n\\hat{\\mu}_i^{(3)} = 2.11\n\nThis model predicts the processing time with the average processing time (which is about 2.11 hours) and does not consider the number of invoices. The residuals for the first few days are shown in Table 4.2 and Figure 4.6 plots all the residuals.\n\ninvoices <- invoices %>%\n  mutate(\n    mu_hat_3 = 2.11,\n    residuals_3 = Time - mu_hat_3\n  )\n\nhead(invoices) %>%\n  select(Day, Time, Invoices, mu_hat_3, residuals_3) %>%\n  knitr::kable()\n\n\n\nTable 4.2: Residuals for the third model\n\n\nDay\nTime\nInvoices\nmu_hat_3\nresiduals_3\n\n\n\n\n1\n2.1\n149\n2.11\n-0.01\n\n\n2\n1.8\n60\n2.11\n-0.31\n\n\n3\n2.3\n188\n2.11\n0.19\n\n\n4\n0.8\n23\n2.11\n-1.31\n\n\n5\n2.7\n201\n2.11\n0.59\n\n\n6\n1.0\n58\n2.11\n-1.11\n\n\n\n\n\n\n\ninvoices %>%\nggplot(aes(x = Invoices, y = Time)) +\n  # geom_point() +\n  geom_hline(yintercept = 2.11) +\n  ylim(c(0, 5)) +\n  geom_segment(aes(xend = Invoices, yend = 2.11), lty = 2, alpha = 0.5) +\n  geom_label(\n    aes(\n      label = round(residuals_3, 2)),\n      alpha = 0.5\n    )\n\n\n\n\nFigure 4.6: Residuals for the thrid model, using only the average\n\n\n\n\nEven though the third model produces much worse predictions than the blue model, the sum of its residuals is zero.\n\ninvoices %>%\n  summarise(\n    sum_residuals_1 = sum(residuals_1),\n    sum_residuals_2 = sum(residuals_2),\n    sum_residuals_3 = sum(residuals_3),\n  ) %>%\n  knitr::kable()\n\n\n\n\nsum_residuals_1\nsum_residuals_2\nsum_residuals_3\n\n\n\n\n1.785\n37.795\n0\n\n\n\n\n\nThat is why the accounting firm comes up with the idea to base our fine not on the sum of the residuals but on the sum of the squared residuals.\n\nr_i^2 = (y_i - \\hat{\\mu}_i)^2\n\nOur fine would be proportional to the residual sum of squares (RSS).\n\nRSS = \\sum_{i = 1}^n r_i ^ 2 = \\sum_{i = 1}^n (y_i - \\hat{\\mu}_i) ^ 2\n\nLet us calculate the RSS for the three models considered thus far:\n\ninvoices %>%\n  summarise(\n    rss_1 = sum(residuals_1 ^ 2),\n    rss_2 = sum(residuals_2 ^ 2),\n    rss_3 = sum(residuals_3 ^ 2)\n  ) %>%\n  knitr::kable()\n\n\n\nTable 4.3: Sums of squared residuals for the three models.\n\n\nrss_1\nrss_2\nrss_3\n\n\n\n\n5.384075\n57.08767\n23.747\n\n\n\n\n\n\nThe RSS of the third model is now larger than the RSS of the first model which makes intuitive sense, because we agreed that the blue line is much more closer to the points in the scatterplots than the other two lines. From Table 4.3 it should become clear that the RSS depends on our choice of coefficients.\nBecause we would like to avoid paying too much for wrong predictions, let us find the coefficients that make our penalty as small as possible. Let use use \\hat{\\beta}_0 and \\hat{\\beta}_1 to denote our guesses about \\beta_0 and \\beta_1. Our predictions for the observed values are then\n\n\\hat{\\mu}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i, \\quad i = 1,\\ldots,n\n The RSS therefore depends on these guesses. To emphasize this point we will write RRS(\\hat{\\beta}_0, \\hat{\\beta}_1).\n\nRSS(\\hat{\\beta}_0, \\hat{\\beta}_1) = \\sum_{i = 1}^{n} (y_i - \\hat{\\mu}_i)^2 = \\sum_{i = 1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2\n\nThe values of \\hat{\\beta}_0 and \\hat{\\beta}_1 that make the RSS as small as possible are called the Ordinary Least Squares (OLS) estimates for \\beta_0 and \\beta_1.\n\n\\hat{\\beta}^{\\text{OLS}}_0, \\hat{\\beta}^{\\text{OLS}}_1 = \\arg\\min_{\\hat{\\beta}_0, \\hat{\\beta}_1} RSS(\\hat{\\beta}_0, \\hat{\\beta}_1)\n\nBecause we will deal almost exclusively with OLS estimates we will not write the OLS label every time. You can see how the minimization problem is solved in Section 4.6.2."
  },
  {
    "objectID": "03-Simple-Linear-Model.html#finding-the-ols-solution",
    "href": "03-Simple-Linear-Model.html#finding-the-ols-solution",
    "title": "4  Simple Linear Model",
    "section": "4.3 Finding the OLS solution",
    "text": "4.3 Finding the OLS solution\nTo find the OLS solution we can use a R function called lm. This function takes a formula as its first argument that describes the model.\nOur model for the processing time is\n\n\\text{Time}_i = \\beta_0 + \\beta_1 \\text{Invoices}_i + e_i, e_i \\sim N(0, \\sigma^2)\n On the left hand side of the formula you write the name of the variable in the data that you want to model (Time in our case). On the right hand side of the formula you write the names of the predictor variables in the model. Your model has only one predictor variables: Invoices. Finally, the data argument instructs lm where to look for this variables. In our case they reside in the object invoices.\n\nfit <- lm(Time ~ 1 + Invoices, data = invoices)\n\nPrinting the output from lm shows you the OLS solution for \\hat{beta}_0 and \\hat{\\beta}_1. Now you can write the estimated regression equation\n\n\\hat{\\mu} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x \\\\\n\\hat{\\mu} = 0.64 + 0.011 x \\\\\n\\tag{4.2}\nLets plot the OLS regression line.\n\ninvoices %>%\n  ggplot(aes(x = Invoices, y = Time)) + \n  geom_point() + \n  geom_abline(intercept = 0.64, slope = 0.011)\n\n\n\n\nFigure 4.7: OLS regression line\n\n\n\n\nLet us also calculate the residual sum of squares:\n\ninvoices <- invoices %>%\n  mutate(\n    y_hat_ols = 0.64 + 0.011 * Invoices,\n    residuals_ols = Time - y_hat_ols,\n  )\n\ninvoices %>%\n  summarise(\n    rss_1 = sum(residuals_1 ^ 2),\n    rss_2 = sum(residuals_2 ^ 2),\n    rss_3 = sum(residuals_3 ^ 2),\n    rss_ols = sum(residuals_ols ^ 2)\n  ) %>%\n  knitr::kable()\n\n\n\nTable 4.4: Residuals sums of squares\n\n\nrss_1\nrss_2\nrss_3\nrss_ols\n\n\n\n\n5.384075\n57.08767\n23.747\n3.105947\n\n\n\n\n\n\nIn Table 4.4 you can see that the OLS estimates resulted in the lowest RSS among the three models that we compared."
  },
  {
    "objectID": "03-Simple-Linear-Model.html#predictions",
    "href": "03-Simple-Linear-Model.html#predictions",
    "title": "4  Simple Linear Model",
    "section": "4.4 Predictions",
    "text": "4.4 Predictions\nOnce you have a estimates for the unknown coefficients in the model you can use these to make predictions about y. The predicted value of y is simply the estimated expected value of y. Using Equation 4.2 we can compute the predictions for x = 50, x = 150, and x = 250 (our original goal).\n\n\\begin{align}\n& \\hat{\\mu}_{x = 50} = 0.64 + 0.011\\cdot 50 =  1.19 \\\\\n& \\hat{\\mu}_{x = 150} = 0.64 + 0.011\\cdot 50 =  2.29 \\\\\n& \\hat{\\mu}_{x = 250} = 0.64 + 0.011\\cdot 250 =  3.39\n\\end{align}\n\n\n## Where to get the data?\ninvoices %>%\n  ggplot(\n    ## How to map data to elements in the graphic\n    aes(\n      x = Invoices,\n      y = Time\n    )\n  ) +\n  ## How to visualise the data\n  geom_point() +\n  ## Add the regression line (least squares) to the graphic\n  geom_abline(intercept = 0.64, slope = 0.011) +\n  geom_vline(\n    ## Where should the vertical lines intercept with the x-axis\n    xintercept = c(50, 150, 250),\n    ## Alpha channel: controls transparency\n    alpha = 0.5,\n    ## lty: line type\n    lty = 2\n  ) +\n  ## Controls the x-axis\n  scale_x_continuous(breaks = c(50, 150, 250)) +\n  geom_segment(\n    data = tibble(\n      x = c(50, 150, 250),\n      y = c(1.19, 2.29, 3.39),\n      xend = 0\n    ),\n    aes(x = x, y = y, xend = xend, yend = y),\n    alpha = 0.5,\n    lty = 2\n  ) +\n  scale_y_continuous(breaks = c(\n    1.19,\n    2.29,\n    3.39\n    ))\n\n\n\n\nInstead of doing the calculations manually, you can use the predict function. It takes a model object (in our case the result from running lm) and a data object (e.g. a tibble). The data object must have the same variables (columns) as the ones specified in the formula, otherwise predict would fail.\n\npredict(fit, newdata = tibble(Invoices = c(50, 150, 250)))\n\n       1        2        3 \n1.206292 2.335456 3.464621 \n\n\nThe results from predict differ slightly from our own calculations above due to rounding errors in our calculations."
  },
  {
    "objectID": "03-Simple-Linear-Model.html#interpretation-of-the-estimated-coefficients",
    "href": "03-Simple-Linear-Model.html#interpretation-of-the-estimated-coefficients",
    "title": "4  Simple Linear Model",
    "section": "4.5 Interpretation of the estimated coefficients",
    "text": "4.5 Interpretation of the estimated coefficients\n\n\\hat{\\mu} = 0.64 + 0.011 x\n As previously discussed the estimated fixed costs are 0.64 hours. The estimated cost of processing one additional invoice is 0.011 hours."
  },
  {
    "objectID": "03-Simple-Linear-Model.html#mathematical-details-optional",
    "href": "03-Simple-Linear-Model.html#mathematical-details-optional",
    "title": "4  Simple Linear Model",
    "section": "4.6 Mathematical Details (optional)",
    "text": "4.6 Mathematical Details (optional)\n\n4.6.1 The conditional expectation as prediction\nUntil now we have used the expected value of y given x as a prediction for y without a justification. Assume that we offer the accounting firm a prediction quality guarantee. Each time that our prediction (\\hat{y}) fails to hit the real y we pay a penalty that is equal to\n\n(y - \\hat{y})^2\n If our model is adequate, then future values of for let’s say x = 50 invoices will be generated from a normal distribution with mean 1 and standard deviation 0.3. Now the question is, how should we make our prediction so that the penalties that we expect to pay are as small as possible.\nOur expected loss would be:\n\n\\text{Expected Loss}(\\hat{y}) = E\\left((y - \\hat{y})^2 | x = 50\\right)\n\n\n\\begin{align}\n\\text{Expected Loss} & = E\\left((y - \\hat{y})^2 | x = 50\\right)\\\\\n& = E\\left(y^2 - 2 y \\hat{y} + \\hat{y}^2 | x = 50\\right) \\\\\n& = E(y^2 | x = 50) - 2\\hat{y}E(y | x = 50) + \\hat{y}^2 \\\\\n\\end{align}\n\nNow let’s find the minimal expected loss by setting the first derivative of the loss function equal to zero:\n\n\\frac{\\partial \\text{Expected Loss}(\\hat{y})}{\\partial \\hat{y}} = -2 E(y | x = 50) + 2\\hat{y} = 0\n Solving this equation is very easy.\n\n\\hat{y} = E(y | x = 50)\n You should also check the sufficient condition for a local minium (positive second derivative!). If the second derivative is negative, then you have found a local maximum, not a minimum.\n\n\\frac{\\partial^2 \\text{Expected Loss}(\\hat{y})}{\\partial^2 \\hat{y}} = 2 > 0\n The second derivative is positive, so our solution (predicting the random variable with its expected value) yields the best prediction in the sense of minimizing the quadratic loss function.\nTo strenghten your understanding, take a couple of minutes to play the following game. You would like to predict the value of random variable generated from N(1, 0.3^2). You want to compare two predictions: the first using the expected value of the distribution (\\hat{y} = 1), the second one using another value: \\hat{y}^{(1)} = 2.\n\nsim_loss <- tibble(\n  ## Generate value from N(1, 0.3^2)\n  y = rnorm(10, mean = 1, sd = 0.3),\n  ## Compute the error of each prediction (you are always predicting with the expected value, i.e. 1)\n  error = y - 1,\n  ## Compute the loss for each prediction\n  loss = error^2,\n  ## Now compute the error and the loss when using the second prediction\n  error1 = y - 2,\n  loss1 = error1 ^ 2\n)\n\nsim_loss\n\n# A tibble: 10 × 5\n       y   error    loss error1 loss1\n   <dbl>   <dbl>   <dbl>  <dbl> <dbl>\n 1 0.475 -0.525  0.276   -1.53  2.33 \n 2 0.724 -0.276  0.0762  -1.28  1.63 \n 3 0.700 -0.300  0.0897  -1.30  1.69 \n 4 1.11   0.114  0.0130  -0.886 0.785\n 5 1.56   0.562  0.316   -0.438 0.192\n 6 1.20   0.200  0.0399  -0.800 0.640\n 7 0.951 -0.0494 0.00244 -1.05  1.10 \n 8 0.876 -0.124  0.0153  -1.12  1.26 \n 9 0.934 -0.0659 0.00434 -1.07  1.14 \n10 1.24   0.240  0.0574  -0.760 0.578\n\n\nFinally, calculate the total lossese for the two predictions and compare the two.\n\nsim_loss %>%\n  summarise(\n    total_loss = sum(loss),\n    total_loss1 = sum(loss1)\n  )\n\n# A tibble: 1 × 2\n  total_loss total_loss1\n       <dbl>       <dbl>\n1      0.890        11.3\n\n\nNotice that the prediction using \\hat{y} = 2 yields a much worse total loss than the prediction using the conditional expected value \\hat{y} = 1. Change the value of the second prediction (some other value instead of 2) and re-play the game a couple of times.\n\n\n4.6.2 Derivation of the OLS estimator\nAs we have already discussed, the OLS estimator looks for guesses \\hat{\\beta}_0 and \\hat{\\beta}_1 that minimize the residual sum of squares (RSS).\n\n\\begin{align}\n\\text{RSS}(\\hat{\\beta}_0, \\hat{\\beta}_1) = \\sum_{i = 1}^{n}\\left(y_i - \\hat{y}_i \\right)^2 = \\sum_{i = 1}^{n}\\left(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i \\right)^2\n\\end{align}\n\nTo find the minimum we compute the first derivatives of \\text{RSS}(\\hat{\\beta}_0, \\hat{\\beta}_1) with respect to its two arguments and set these derivatives to zero.\n\n\\begin{align}\n\\frac{\\partial \\text{RSS}(\\hat{\\beta}_0, \\hat{\\beta}_1)}{\\partial \\hat{\\beta}_0} = 0 \\\\\n\\frac{\\partial \\text{RSS}(\\hat{\\beta}_0, \\hat{\\beta}_1)}{\\partial \\hat{\\beta}_1} = 0 \\\\\n\\end{align}\n\nThe first partial derivative is:\n\n\\begin{align}\n\\frac{\\partial \\text{RSS}(\\hat{\\beta}_0, \\hat{\\beta}_1)}{\\partial \\hat{\\beta}_0} =  - 2 \\sum_{i = 1}^{n}\\left(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i \\right) = 0\\\\\n\\end{align}\n\nWe can simplify the left hand side of the equation. We devide by -2 and expand the expression in the paranthesis.\n\n\\begin{align}\n- 2 \\sum_{i = 1}^{n}\\left(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i \\right) = 0 \\\\\n\\sum_{i = 1}^{n}\\left(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i \\right) = 0 \\\\\n\\sum_{i = 1}^{n} y_i - \\sum_{i = 1}^{n} \\hat{\\beta}_0 - \\sum_{i = 1}^{n} \\hat{\\beta}_1 x_i = 0\n\\end{align}\n\nNow we can notice that \\hat{\\beta}_0 and \\hat{\\beta}_1 do not depend on the index of the sums, so we can write \\hat{\\beta}_1 in front of the sum.\n\n\\begin{align}\n\\sum_{i = 1}^{n} y_i - n \\hat{\\beta}_0 - \\hat{\\beta}_1 \\sum_{i = 1}^{n} x_i = 0\n\\end{align}\n\nIf we devide the equation by n we get\n\n\\begin{align}\n\\frac{1}{n}\\sum_{i = 1}^{n} y_i - \\frac{n}{n} \\hat{\\beta}_0 - \\hat{\\beta}_1 \\frac{1}{n} \\sum_{i = 1}^{n} x_i = 0 \\\\\n\\bar{y} - \\hat{\\beta}_0 - \\hat{\\beta}_1 \\bar{x} = 0 \\implies \\\\\n\\end{align}\n To simplify the notation we use the following shorthands:\n\n\\begin{align}\n\\bar{x} = \\frac{1}{n}\\sum_{i = 1}^{n} x_i \\quad \\text{ the arithmetic average of } x \\\\\n\\bar{y} = \\frac{1}{n}\\sum_{i = 1}^{n} y_i \\quad \\text{ the arithmetic average of } y \\\\\n\\overline{y^2} = \\frac{1}{n}\\sum_{i = 1}^{n} y_i^2 \\quad \\text{ the arithmetic average of } y^2 \\\\\n\\overline{x^2} = \\frac{1}{n}\\sum_{i = 1}^{n} x_i^2 \\quad \\text{ the arithmetic average of } x^2 \\\\\n\\overline{xy} = \\frac{1}{n}\\sum_{i = 1}^{n} x_i y_i \\quad \\text{ the arithmetic average of } x\\cdot y  \\\\\n\\end{align}\n\nFinally we bring all term except \\bar{y} to the right hand side of the equation to obtain:\n\n\\bar{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x}\n This equation informs us that the OLS regression line passes through the center of the observations: (\\bar{x}, \\bar{y}).\nNow we can calculate the other derivative.\n\n\\begin{align}\n\\frac{\\partial \\text{RSS}(\\hat{\\beta}_0, \\hat{\\beta}_1)}{\\partial \\hat{\\beta}_1} =  - 2 \\sum_{i = 1}^{n} x_i \\left(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i \\right) = 0\\\\\n\\end{align}\n\nAs with the previous derivative we simplify the expression by dividing by -2 and expanding\n\n\\begin{align}\n- 2 \\sum_{i = 1}^{n}\\left(x_i y_i - \\hat{\\beta}_0 x_i - \\hat{\\beta}_1 x_i^2 \\right) = 0\\\\\n\\sum_{i = 1}^{n}\\left(x_i y_i - \\hat{\\beta}_0 x_i - \\hat{\\beta}_1 x_i^2 \\right) = 0 \\\\\n\\sum_{i = 1}^{n} x_i y_i - \\sum_{i = 1}^{n} \\hat{\\beta}_0 x_i - \\sum_{i = 1}^{n} \\hat{\\beta}_1 x_i^2 = 0 \\\\\n\\frac{1}{n}\\sum_{i = 1}^{n} x_i y_i - \\hat{\\beta}_0 \\frac{1}{n} \\sum_{i = 1}^{n} x_i - \\hat{\\beta}_1 \\frac{1}{n} \\sum_{i = 1}^{n} x_i^2 = 0 \\\\\n\\overline{xy} - \\hat{\\beta}_0 \\bar{x} - \\hat{\\beta}_1 \\overline{x^2} = 0 \\\\\n\\overline{xy}  = \\hat{\\beta}_0 \\bar{x} + \\hat{\\beta}_1 \\overline{x^2}\n\\end{align}\n\nNow we have a system of two linear equations for \\hat{\\beta}_0 and \\hat{\\beta}_0.\n\n\\begin{align}\n\\bar{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x} \\\\\n\\overline{xy}  = \\hat{\\beta}_0 \\bar{x} + \\hat{\\beta}_1 \\overline{x^2}\n\\end{align}\n\nAn easy way to solve the system of equations is to multiply the first equation by \\bar{x} and subtract the first equation from the second one. This will eliminate \\hat{\\beta}_0 and allow us to solve for \\hat{\\beta}_1.\n\n\\begin{align}\n\\overline{xy} - \\bar{x}\\bar{y} = \\hat{\\beta}_0 \\bar{x} + \\hat{\\beta}_1 \\overline{x^2} - \\hat{\\beta}_0 \\bar{x} - \\hat{\\beta}_1 \\bar{x}^2 \\\\\n\\overline{xy} - \\bar{x}\\bar{y} = \\hat{\\beta}_1 \\overline{x^2}- \\hat{\\beta}_1 \\bar{x}^2 \\\\\n\\overline{xy} - \\bar{x}\\bar{y} = \\hat{\\beta}_1 (\\overline{x^2} - \\bar{x}^2) \\implies \\\\\n\\hat{\\beta}_1 = \\frac{\\overline{xy} - \\bar{x}\\bar{y}}{\\overline{x^2} - \\bar{x}^2}\n\\end{align}\n\nFinally, we obtain the whole solution:\n\n\\begin{align}\n  \\bar{y} & = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x} \\\\\n  \\hat{\\beta}_1 & = \\frac{\\overline{xy} - \\bar{x}\\bar{y}}{\\overline{x^2} - \\bar{x}^2}\n\\end{align}\n\\tag{4.3}\nThe equation for the intercept in Equation 4.3 simply says that the OLS regression line passes through the center of the observations. The second equation says that the slope estimate equals the covariance of x and y scaled with the variance of x.\nThe (empirical) covariance between two variables is defined as the average cross-product of the deviations of x and y from their respective means (\\bar{x}) and \\bar{y}.\n\nCov(x, y) = \\frac{1}{n - 1} \\sum_{i = 1}^n (x_i - \\bar{x})(y_i - \\bar{y})\n\nWe can prove the following theorem\n\nTheorem 4.1 (Covariance decomposition) The sum of cross-products can be decomposed in the following way\n\n\\sum_{i = 1}^n (x_i - \\bar{x})(y_i - \\bar{y}) = n(\\overline{xy} - \\bar{x} \\bar{y})\n Therefore, the covariance of x and y is equal to\n\nCov(x, y) = \\frac{n}{(n - 1)}(\\overline{xy} - \\bar{x}\\bar{y})\n\n\n::: {.proof} \n\\begin{align*}\n\\sum_{i = 1}^n (x_i - \\bar{x})(y_i - \\bar{y}) & = \\sum_{i = 1}^n (x_i y_i - x_i \\bar{y} - \\bar{x} y_i + \\bar{x} \\bar{y}) \\\\\n& = \\sum_{i = 1}^{n} x_i y_i - \\sum_{i = 1}^{n} x_i \\bar{y} - \\sum_{i = 1}^{n} \\bar{x} y_i + \\sum_{i = 1}^{n} \\bar{x} \\bar{y}\n\\end{align*}\n\n\nTheorem 4.2 (Variance decomposition) The (empirical) variance of a numeric variable x is given by\n\nS^2(x)  = \\frac{1}{n - 1}\\sum_{i = 1}^n (x_i - \\bar{x})^2\n The sum of squares can be decomposed as follows\n\n\\sum_{i = 1}^n (x_i - \\bar{x})^2 = n(\\overline{x^2} - \\bar{x}^2)\n\nThe empirical variance is therefore equal to\n\nS^2_{x} = \\frac{n}{n - 1}(\\overline{x^2} - \\bar{x}^2)\n\n\n\nProof. Take the expression for the sum of cross-products from Theorem 4.1 and set y_i = x_i:\n\n\\begin{align*}\n\\sum_{i = 1}^n (x_i - \\bar{x})(y_i - \\bar{y}) & = n(\\overline{xy} - \\bar{x}\\bar{y}) \\implies \\\\\n\\sum_{i = 1}^n (x_i - \\bar{x})(x_i - \\bar{x}) = \\sum_{i = 1}^n (x_i - \\bar{x})^2 = n(\\overline{x \\cdot x} - \\bar{x}\\bar{x}) = n(\\overline{x^2} - \\bar{x}^2\n)\n\\end{align*}\n\n\n\n\n4.6.3 Properties of the OLS estimator\nThe OLS estimators are unbiased for the coefficients.\n\nTheorem 4.3 (Expected value of the OLS estimator) \nE\\hat{\\beta}_0 = \\beta_0 \\\\\nE\\hat{\\beta}_1 = \\beta_1\n\n\n\nProof. From Equation 4.3 we know that the OLS estimator for \\beta_1 is\n\n\\hat{\\beta}_1 = \\frac{\\overline{xy} - \\bar{x}\\bar{y}}{\\overline{x^2} - \\bar{x}^2}\n\nTo investigate the statistical properties (expected value and variance) of this estimator it is convenient to write is as a linear combination of y.\n\n\\begin{align*}\n\\hat{\\beta}_1 & = \\frac{1}{n} \\frac{1}{\\overline{x^2} - \\bar{x}^2} \\left(\\sum_{i = 1}^{n} x_i y_i - \\bar{x} \\sum_{i = 1}^{n} y_i \\right) \\\\\n        & = \\frac{1}{n} \\frac{1}{\\overline{x^2} - \\bar{x}^2} \\sum_{i = 1}^{n} (x_i - \\bar{x}) y_i\n\\end{align*}\n\nWe can write this more compactly as:\n\n\\begin{align*}\n\\hat{\\beta}_1 & = \\sum_{i = 1}^{n} c_i y_i \\\\\nc_i & = \\frac{x_i - \\bar{x}}{n(\\overline{x^2} - \\bar{x}^2)}\n\\end{align*}\n\\tag{4.4}\nNow it is easy to compute the conditional (given x_i) expectation.\n\n\\begin{align*}\nE(\\hat{\\beta}_1 | x_i) & = E\\left(\\sum_{i = 1}^{n} c_i y_i | x_i\\right) \\\\\n               & = \\sum_{i = 1}^{n} E(c_i y_i | x_i) \\\\\n               & = \\sum_{i = 1}^{n} c_i E(y_i | x_i)\n\\end{align*}\n From the model definition we know that the conditional expectation E(y_i | x_i) = \\beta_0 + \\beta_1 x_i.\n\n\\begin{align*}\nE(\\hat{\\beta}_1 | x_i) & = \\sum_{i = 1}^{n} c_i (\\beta_0 + \\beta_1 x_i) \\\\\n                       & = \\beta_0 \\sum_{i = 1}^{n} c_i + \\beta_1 \\sum_{i = 1}^{n} c_i x_i\n\\end{align*}\n Now notice that the sum of c_i is zero:\n\n\\begin{align*}\n\\sum_{i = 1}^{n} c_i & = \\sum_{i = 1}^{n} \\left( \\frac{x_i - \\bar{x}}{n(\\overline{x^2} - \\bar{x}^2)} \\right) \\\\\n                     & = \\frac{1}{n(\\overline{x^2} - \\bar{x}^2)}\\sum_{i = 1}^{n} (x_i - \\bar{x}) \\\\\n                     & = 0\n\\end{align*}\n The last result is due to the fact that the deviation of x from its average \\bar{x} sum to zero!\n\n\\sum_{i = 1}^{n} (x_i - \\bar{x}) = \\sum_{i = 1}^{n} x_i  - \\sum_{i = 1}^{n} \\bar{x} = n \\bar{x} - n\\bar{x} = 0.\n\nThe second sum in the expression for \\hat{\\beta}_1 equals 1.\n\n\\begin{align*}\n\\sum_{i = 1}^{n} c_i x_i & = \\sum_{i = 1}^{n} \\frac{x_i (x_i - \\bar{x})}{n (\\overline{x^2} - \\bar{x}^2)} \\\\\n                         & = \\frac{1}{n (\\overline{x^2} - \\bar{x}^2)} \\sum_{i = 1}^{n} x_i (x_i - \\bar{x}) \\\\\n                         & = \\frac{1}{n (\\overline{x^2} - \\bar{x}^2)} \\sum_{i = 1}^{n} (x_i^2 - \\bar{x} x_i) \\\\\n                         & = \\frac{1}{n (\\overline{x^2} - \\bar{x}^2)} \\left(\\sum_{i = 1}^{n} x_i^2 - \\bar{x} \\sum_{i = 1}^{n} x_i \\right) \\\\\n                         & = \\frac{1}{n (\\overline{x^2} - \\bar{x}^2)} (n \\overline{x^2} - n \\bar{x}^2) \\\\\n                         & = 1.\n\\end{align*}\n Finally, we obtain for the expected value of \\hat{\\beta}_1:\n\nE(\\hat{\\beta}_1 | x_i) = \\beta_1\n For the intercept estimator we get:\n\n\\begin{align*}\nE(\\hat{\\beta}_0 | x) & = E(\\bar{y} - \\hat{\\beta}_1 \\bar{x} | x) \\\\\n                       & = E(\\bar{y} | x) - \\bar{x} E(\\hat{\\beta}_1 | x) \\\\\n                       & = E\\left(\\frac{1}{n}\\sum_{i = 1}^{n} y_i | x\\right) - \\bar{x} \\beta_1 \\\\\n                       & = \\frac{1}{n} \\sum_{i = 1}^{n} E(y_i | x) - \\bar{x}\\beta_1 \\\\\n                       & = \\frac{1}{n} \\sum_{i = 1}^{n} (\\beta_0 + \\beta_1 x_i) - \\bar{x}\\beta_1 \\\\\n                       & = \\frac{1}{n} \\sum_{i = 1}^{n} \\beta_0 + \\frac{1}{n} \\sum_{i = 1}^{n} \\beta_1 x_i - \\bar{x}\\beta_1 \\\\\n                       & = \\beta_0 + \\beta_1 \\bar{x} - \\bar{x}\\beta_1 \\\\\n                       & = \\beta_0\n\\end{align*}\n\n\n\nTheorem 4.4 (Variance of the OLS estimators) The conditional variance of \\hat{\\beta}_1 is given by:\n\n\\begin{align*}\nVar(\\hat{\\beta}_1 | x) & = Var\\left(\\sum_{i = 1}^{n} c_i y_i  | x \\right) \\\\\n                       & = \\sum_{i = 1} ^ {n} Var(c_i y_i | x) \\\\\n                       & = \\sum_{i = 1} ^ {n} c_i ^ 2 Var(y_i | x) \\\\\n                       & = \\sum_{i = 1} ^ {n} c_i ^ 2 \\sigma^2 \\\\\n                       & = \\sigma^2 \\sum_{i = 1} ^ {n} c_i ^ 2\n\\end{align*}\n \n\\begin{align*}\nVar(\\hat{\\beta}_0 | x) & = Var\\left(\\bar{y} - \\hat{\\beta}_1 \\bar{x} | x\\right) \\\\\n                       & = Var\\left(\\frac{1}{n}\\sum_{i = 1}^{n} y_i - \\bar{x} \\sum_{i = 1}^{n} c_i y_i | x\\right) \\\\\n                       & = Var\\left(\\sum_{i = 1} ^{n} \\left(\\frac{y_i}{n} - \\bar{x}c_i y_i \\right) | x\\right) \\\\\n                       & = Var\\left(\\sum_{i = 1} ^{n} \\left(\\frac{1}{n} - \\bar{x}c_i \\right) y_i | x\\right) \\\\\n                       & = \\sum_{i = 1}^{n}\\left(\\frac{1}{n} - \\bar{x}c_i \\right)^2 Var(y_i | x) \\\\\n                       & = \\sigma^2 \\sum_{i = 1}^{n}\\left(\\frac{1}{n} - \\bar{x}c_i \\right)^2\n\\end{align*}\n\n\n\nTheorem 4.5 (Zero sum of residuals) The residuals in the linear model sum to zero if the model includes a constant.\n\n\\sum_{i = 1}^{n} r_i = 0\n\n\n\nProof. \n\\begin{align*}\n\\sum_{i = 1}^{n} r_i & = \\sum_{i = 1}^{n} (y_i - \\hat{\\mu}_i) \\\\\n                     & = n\\bar{y} - \\sum_{i = 1}^{n} (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) \\\\\n                     & = n\\bar{y} - n \\hat{\\beta}_0 - n \\hat{\\beta}_1 \\bar{x} \\\\\n                     & = n(\\bar{y} - \\hat{\\beta}_0 - \\hat{\\beta}_1 \\bar{x}) \\\\\n                     & = n(\\bar{y} - (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) - \\hat{\\beta}_1 \\bar{x}) \\\\\n                     & = 0\n\\end{align*}\n You can verify this property with a quick example.\n\n## Create some data\ndt <- tibble(\n  x = 1:10,\n  y = rnorm(10, mean = 2 + 0.5 * x, sd = 0.5)\n)\n\n## Fit a model including a constant (this is the 1 in the formula)\nfit_with_constant <- lm(y ~ 1 + x, data = dt)\nsum(residuals(fit_with_constant))\n\n[1] 2.498002e-16\n\n\nSuppressing the constant in the model results in a non-zero sum of the residuals.\n\n## The 0 in the formula removes the constant\nfit_without_constant <- lm(y ~ 0 + x, data = dt)\nsum(residuals(fit_without_constant))\n\n[1] 3.783743\n\n\n\n\nTheorem 4.6 (Orthogonality of residuals and predictors) The product of the residuals and the predictor variables x sums to zero.\n\n\\sum_{i = 1}^{n} r_i x_i = 0\n\n\n\nProof. \n\\begin{align*}\n\\sum_{i = 1}^{n} r_i x_i & = \\sum_{i = 1}^{n} (y_i - \\hat{\\mu}_i)x_i \\\\\n                         & = \\sum_{i = 1}^{n} x_i y_i - \\sum_{i = 1}^{n} x_i \\hat{\\mu}_i \\\\\n                         & = n \\overline{xy} - \\sum_{i = 1}^{n} x_i (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) \\\\\n                         & = n \\overline{xy} - \\left( \\sum_{i = 1}^{n} x_i \\hat{\\beta}_0 + \\sum_{i = 1}^{n} \\hat{\\beta}_1 x_i^2 \\right) \\\\\n                         & = n \\overline{xy} - n \\hat{\\beta}_0 \\bar{x} - n \\hat{\\beta}_1 \\overline{x^2} \\\\\n                         & = n (\\overline{xy} - \\hat{\\beta}_0 \\bar{x} - \\hat{\\beta}_1 \\overline{x^2}) \\\\\n                         & = n (\\overline{xy} - (\\bar{y} - \\hat{\\beta}_1 \\bar{x})\\bar{x} - \\hat{\\beta}_1 \\overline{x^2}) \\\\\n                         & = n(\\overline{xy} - \\bar{x}\\bar{y} + \\hat{\\beta}_1 \\bar{x}^2 - \\hat{\\beta}_1 \\bar{x}^2) \\\\\n                         & = 0.\n\\end{align*}\n\nYou can check this property with test data that we used above.\n\nsum(dt$x * residuals(fit_with_constant))\n\n[1] 2.63678e-16"
  },
  {
    "objectID": "04-Simple-ANOVA.html#the-data",
    "href": "04-Simple-ANOVA.html#the-data",
    "title": "5  Simple ANOVA",
    "section": "5.1 The Data",
    "text": "5.1 The Data\nVariables description:\n\nkid_score: (numeric) Kid’s IQ score.\nmom_hs (numeric): This variable has only two possible values (a binary variable): 1 if the mother of the child has finished high school, and 0 otherwise.\n\nFor the sake of simplicity we will assume that the children in the sample (in the data set kids) were selected at random from all children living in the US at the time of the survey.\nResearch question: Do children whose mother did not finish a high school (mom_hs = 0) tend to achieve lower IQ scores compared to the children of mothers with a high school degree (mom_hs = 1)."
  },
  {
    "objectID": "04-Simple-ANOVA.html#the-linear-regression-model-with-a-single-binary-predictor",
    "href": "04-Simple-ANOVA.html#the-linear-regression-model-with-a-single-binary-predictor",
    "title": "5  Simple ANOVA",
    "section": "5.2 The Linear Regression Model with a single binary predictor",
    "text": "5.2 The Linear Regression Model with a single binary predictor\nLet us first summarize the data.\n\nkids %>%\n  ## Group the data according to the educational status of the mothers\n  group_by(mom_hs) %>%\n  ## Compute the average IQ score in each group\n  summarise(\n    ## The function n() counts the number of rows (children) in each group\n    n = n(),\n    ## Compute the average IQ score for each group\n    Average_IQ_score = mean(kid_score)\n  ) %>%\n  knitr::kable()\n\n\n\nTable 5.1: Average IQ scores and number of observations (n) by the status of the mother.\n\n\nmom_hs\nn\nAverage_IQ_score\n\n\n\n\n0\n93\n77.54839\n\n\n1\n341\n89.31965\n\n\n\n\n\n\n\n## Where to get the data\nkids %>%\n  ggplot(\n    aes(\n      ## Map the status of the mother to the y-axis.\n      ## The factor function converts mom_hs to a factor variable\n      ## so that ggplot would not treat mom_hs as a continuous variable.\n      y = factor(mom_hs),\n      ## Map the kid_score column to the x-axis.\n      x = kid_score\n  )\n  ) +\n  geom_point(\n    ## Add some noise to each observation so that we can \n    ## see the collection of dots. Without this noise\n    ## all dots would lie on two straight lines\n    position = \"jitter\"\n  ) +\n  ## Draws the two boxplots to help us see the centers and the spreads\n  ## of the distributions of the scores in the two groups\n  geom_boxplot(\n    ## Makes the boxplots transparent so that we can see the dots\n    ## behind them\n    alpha = 0.5\n  ) +\n  ## Sets human-readable labels for the two axes\n  labs(\n    x = \"IQ score\",\n    y = \"Status of the kid's mother\"\n  )\n\n\n\n\nFigure 5.1: IQ scores of the children by the status of their mother (high school degree/no high school degree). The plot adds a small random noise to the observations to avoid overplotting.\n\n\n\n\nLet us see how we can model the average IQ scores within a simple linear model.\n\n\\begin{align*}\n& i = 1,\\ldots, n = 434 \\text{ observations}\\\\\n& y_i: \\text{Kid IQ score} \\\\\n& x_i \\in \\{0, 1\\}: \\text{status of the mother}\n\\end{align*}\n\nThe linear model is then\n\ny_i = \\beta_0 + \\beta_1 x_i + e_i, e_i \\sim N(0, \\sigma^2)\n Alternatively (and more generally) we can write the model (see Chapter 4) as:\n\n\\begin{align*}\n& y_i \\sim N(\\mu_i, \\sigma^2), \\quad i = 1,\\ldots,n \\\\\n& \\mu_i = \\beta_0 + \\beta_1 x_i, \\quad x_i \\in \\{0, 1\\}\n\\end{align*}\n\\tag{5.1}\nLet us see what the model coefficients mean. This becomes obvious when we consider the expected value of y (the IQ score) for the two possible values of x. For x = 1 (children with a mother with a HS degree) the expected IQ score is:\n\n\\mu_1 = \\beta_0 + \\beta_1 \\cdot 1\n\\tag{5.2}\nFor x = 0 (children with a mother without a HS degree)\n\n\\mu_0 = \\beta_0 + \\beta_1 \\cdot 0\n\\tag{5.3}\nIf you take the difference between equations 5.2 and 5.3 you will get:\n\n\\begin{align*}\n\\beta_0 & = \\mu_0 \\\\\n\\beta_1 & = \\mu_1 - \\mu_0\n\\end{align*}\n Therefore, the constant in the model (\\beta_0) equals the population average IQ score of children in the x = 0 group. The slope coefficient \\beta_1 equals the difference between the population average IQ scores. Figure 5.2 visualizes the population of children (as the model in Equation 5.1 sees it). There are two groups of children in that population. The first has an average IQ score of \\mu_0, the second group has an average IQ score of \\mu_1. The children in both groups are normally distributed and the two distributions have the same standard deviation (spread).\n\n\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale.\n\n\n\n\n\nFigure 5.2: Distribution of IQ scores in the population of children according to Equation 5.1.\n\n\n\n\nWithin that model our research question boils down to examining \\beta_1 (the difference between the population averages). If \\beta_1 is positive, then children born to a mother with a HS degree would tend to perform better at the IQ test.\nThe first obstacle to answering the research question is that we don’t know the value of \\beta_1. Therefore, we would need to rely on the sample to learn something about the value of \\beta_1.\nIn the previous section we discussed the OLS estimator and now we will use it to find the best (in terms of lowest RSS) estimates for \\beta_0 and \\beta_1. As in the previous section will will find these using the lm function. We will also store the result of lm in an object so that we can use it later without running lm every time.\n\n## Run lm and save the output in an object called \"fit\"\nfit <- lm(kid_score ~ 1 + mom_hs, data = kids)\n## Print the object storing the lm output\nfit\n\n\nCall:\nlm(formula = kid_score ~ 1 + mom_hs, data = kids)\n\nCoefficients:\n(Intercept)       mom_hs  \n      77.55        11.77  \n\n\nLook at the output of lm and write down the estimated regression equation:\n\n\\begin{align*}\n\\hat{\\mu} = 77.55 + 11.77 x\n\\end{align*}\n\nThis equation summarizes the sample. The children that we have selected and actually observed. For these children there was a difference of average IQ scores of 11.77. However, the sample only includes 434 children. A more interesting question is whether there is a difference between the average IQ scores between the two groups (mother with HS degree, mother without HS degree) in the population from which the sample was selected.\nAn interesting research hypothesis that we can test is whether the population coefficient \\beta_1 is equal to zero, for example against a two-sided alternative \\beta_1 \\neq 0.\n\nH_0: \\beta_1 = 0 \\\\\nH_1: \\beta_1 \\neq 0\n A test of this hypothesis against the alternative is a t-test with a test statistic\n\n\\text{t-statistic} = \\frac{\\hat{\\beta}_1 - 0}{se(\\hat{\\beta}_1)}\n\nIf \\beta_1 is really equal to zero, then this statistic follows a t-distribution with n - p degrees of freedom. The critical values of this test at \\alpha = 0.05 probability of wrong rejection of the null hypothesis.\nFor the kids data set you can test this hypothesis using the output of summary.\n\nsummary(fit)\n\n\nCall:\nlm(formula = kid_score ~ 1 + mom_hs, data = kids)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-57.55 -13.32   2.68  14.68  58.45 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   77.548      2.059  37.670  < 2e-16 ***\nmom_hs        11.771      2.322   5.069 5.96e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.85 on 432 degrees of freedom\nMultiple R-squared:  0.05613,   Adjusted R-squared:  0.05394 \nF-statistic: 25.69 on 1 and 432 DF,  p-value: 5.957e-07\n\n\nThe second row in the coefficients table corresponds to our \\hat{\\beta}_1 in our model notation (the coefficient of mom_hs). You can find the estimated standard error of \\hat{\\beta}_1 in the second column (Std. Error).\n\n\\text{t-statistic}^{obs} = \\frac{11.77 - 0}{2.322} = 5.0689\n\nYou can find this value in the column t value in the summary above. This t-statistic is shown in the regression output by almost all statistical software, because the hypothesis\n\nH_0: \\beta_1 = 0 \\\\\nH_1: \\beta_1 \\neq 0\n is particularly interesting. If true it implies that there are no population-level differences between the average IQ scores of the two groups of children.\nIn order to make a decision whether to reject this hypothesis or not, we compare the observed t-statistic to two critical values (because the alternative is two-sided).\nThe critical values at a five percent error probability (wrong rejection of a true null hypothesis) are derived from the t-distribution. You can find a detailed explanation in Chapter 6.\nThe quantiles of the t-distribution are\n\nalpha <- 0.05\nqt(alpha / 2, df = nrow(kids) - 2)\n\n[1] -1.965471\n\nqt(1 - alpha / 2, df = nrow(kids) - 2)\n\n[1] 1.965471\n\n\nThe observed t-statistic equals 5.0689 and is therefore greater than the upper critical value (1.96). Therefore we reject the null hypothesis at a significance level 1 - \\alpha = 0.95 (this is another way to state the probability of wrong rejection of a true null hypothesis).\nApart from the point estimate for the difference of average IQ scores (11.77 points) we usually also want to communicate the uncertainty of this estimate. One description of that uncertainty is the (estimated) standard error of the estimator for the regression coefficient.\nOne way to construct a confidence interval for the coefficients is to invert the two-sided t-test (see Section 6.7 for more details).\nWe will use the critical values of the t-test at the 95 percent significance level to obtain an confidence interval (CI) with a 95 percent coverage probability:\n\nP\\left(\\hat{\\beta}_1 - 1.96 se(\\hat{\\beta}_1)  < \\beta_1 < \\hat{\\beta}_1 + 1.96 se(\\hat{\\beta}_1) \\right) = 0.95\n\\tag{5.4}\nNotice that the boundaries of the interval are random variables (because \\hat{\\beta}_1 and se(\\hat{\\beta}_1)) are random variables (they depend on the data). For the sample of children in kids we obtain estimates for these boundaries: [11.77 - 1.96 \\cdot 2.322; 11.77 + 1.96 \\cdot 2.322] \\approx [7.2; 16.3]. Based on these estimated boundaries we would say that range of plausible values for the difference between the population IQ score averages between 7.2 and 16.3.\nNotice that in interpreting the boundaries of the CI we don’t say that the probability of \\beta_1 being between 7.2 and 16.3 is 0.95. Generally this statement does not make any sense, because we have assumed that \\beta_1 is a fixed number (not random). The probability statement in Equation 5.4 does make sense, because the boundaries are random variables (depend on the data), but the statement P(7.2 < \\beta_1 < 16.3) = 0.95 does not!"
  },
  {
    "objectID": "04-Simple-ANOVA.html#mathematical-details-optional",
    "href": "04-Simple-ANOVA.html#mathematical-details-optional",
    "title": "5  Simple ANOVA",
    "section": "5.3 Mathematical Details (Optional)",
    "text": "5.3 Mathematical Details (Optional)\n\n5.3.1 The OLS Estimator in the Binary Predictor Case\nIn the case when x_i \\in \\{0, 1\\} the average of x, i.e \\overline{x} is simply the proportion of ones, lets say n_1 / n. Notice that the average of the squared x is the same as the average of x, because x only contains zeroes and ones (therefore x_i^2 = x_i). The average of the product of x and y is equal to the sum of y where the x = 1, because the other values in y are multiplied by zero.\n\n\\begin{align*}\n\\overline{x^2} = & \\frac{1}{n}\\sum_{i = 1}^{n} x_i^2 =  \\\\\n               = & \\frac{1}{n} \\sum_{i = 1}^n x_i \\\\\n               = & \\bar{x}\n\\end{align*}\n\n\n\\begin{align*}\n\\overline{xy} = & \\frac{1}{n} \\sum_{i = 1}^{n} x_i y_i = \\\\\n              = & \\frac{1}{n}\\left(\\sum_{i: x_i = 0} y_i \\cdot 0 + \\sum_{ i: x_i = 1} y_i \\cdot 1 \\right) \\\\\n              = & \\frac{1}{n}\\sum_{i: x_i = 1} y_i \\\\\n              = & \\frac{n_1}{n_1}\\frac{1}{n} \\sum_{i: x_i = 1} y_i \\\\\n              = & \\frac{n_1}{n} \\bar{y}_{1}\n\\end{align*}\n\nIn the last expression we use \\bar{y}_1 to denote the sample average of the x = 1 group.\nTherefore the expression for \\hat{\\beta}_1 simplifies to:\n\n\\hat{\\beta}_1 = \\frac{ \\frac{n_1}{n} \\overline{y}_{1} - \\overline{x}\\overline{y}}{\\overline{x} - \\overline{x}^2} \\\\\n\\hat{\\beta}_1 = \\frac{\\overline{y}_{1} - \\overline{y}}{1 - \\frac{n_1}{n}} \\\\\n\\hat{\\beta}_1 = \\frac{\\overline{y}_{1} - \\overline{y}}{\\frac{n - n_1}{n}} \\\\\n\\hat{\\beta}_1 = \\frac{\\overline{y}_{1} - \\overline{y}}{\\frac{n_0}{n}} \\\\\n\nFurthermore, the average of y is simply the weighted average of the two group means. Let there be n_0 observations with x_i = 0 and n_1 observations with x_i = 1.\n\n\\overline{y} = \\frac{1}{n}\\left(\\sum_{x_i = 0} y_i + \\sum_{x_i = 0} y_i\\right) \\\\\n\\overline{y} = \\frac{1}{n}\\left(n_0 \\overline{y}_0 + n_1 \\overline{y}_1 \\right)\n\nSubstituting gives us:\n\n\\hat{\\beta}_1 = \\frac{\\overline{y}_{1} - \\frac{1}{n}\\left(n_0 \\overline{y}_0 + n_1 \\overline{y}_1 \\right)}{\\frac{n_0}{n}} \\\\\n\nWhen you simplify the above expression (this is left as an exercise) you will arrive at:\n\n\\hat{\\beta}_1 = \\bar{y}_1 - \\bar{y}_0\n\nFrom the equation for \\hat{\\beta}_0 we get:\n\n\\hat{\\beta}_0 = \\bar{y} - (\\bar{y}_1 - \\bar{y}_0) \\bar{x}\n\nTaking into account that \\bar{x} = n_1 / n this equation simplifies to\n\n\\hat{\\beta}_0 = \\overline{y}_0\n\n\n\n5.3.2 What about the standard errrors?\nIn the case of the simple ANOVA model the standard errors of both coefficients are pretty easy to derive, because the estimators are simply a group average (\\hat{\\beta}_0) and the difference between two group averages (\\hat{\\beta}_1).\nWe can compute the variance of \\hat{beta}_0. Note that when studying the statistical properties of an estimator like the OLS estimator for \\beta_0 we treat the data y_i as random variables. In the following derivation we assume that the y_i are not correlated.\n\n\\begin{align*}\nVar(\\hat{\\beta}_0) & = Var\\left(\\frac{1}{n_0}\\sum_{i: x_i = 0} y_i \\right) \\\\\n                   & = \\frac{1}{n_0^2} \\sum_{i: x_i = 0} Var(y_i) \\\\\n                   & = \\frac{1}{n_0^2} \\sum_{i: x_i = 0} \\sigma^2 \\\\\n                   & = \\frac{n_0 \\sigma^2}{n_0^2} \\\\\n                   & = \\frac{\\sigma^2}{n_0}\n\\end{align*}\n\\tag{5.5}\nFor the variance of \\hat{beta}_1 we obtain. In the derivation we use the assumption of y_i being uncorrelated and the fact that the variance of a difference of uncorrelated random variables equals the sum of their variances.\n\n\\begin{align*}\nVar(\\hat{\\beta}_1) & = Var\\left(\\frac{1}{n_1}\\sum_{i: x_i = 1} y_i - \\frac{1}{n_0}\\sum_{i: x_i = 0} y_i\\right) \\\\\n                   & = Var\\left(\\frac{1}{n_1}\\sum_{i: x_i = 1} y_i \\right) + Var\\left(\\frac{1}{n_0}\\sum_{i: x_i = 0} y_i\\right) \\\\\n                   & = \\frac{\\sigma^2}{n_1} + \\frac{\\sigma^2}{n_0} \\\\\n                   & = \\sigma^2 \\left(\\frac{1}{n_0} + \\frac{1}{n_1}\\right)\n\\end{align*}\n\\tag{5.6}\nThe variances of the two coefficients involve the unknown parameter \\sigma^2. From the expressions for the variances you will notice that the variances increase in \\sigma^2. This should make sense intuitively. \\sigma^2 describes how much noise the data contains (random deviations from the regression line). Estimating the coefficients from noisy data will result in uncertain estimates (high variances).\nIn order to arrive to estimates the coefficient variances we need a way to estimate \\sigma^2 from the data. Here we will take a theorem from mathematical statistics that says that\n\n\\hat{\\sigma}^2 = \\frac{1}{n - p} \\text{RSS}\n\\tag{5.7}\nis an unbiased estimator for \\sigma. In the above expression n is the number of observations and p is the number of coefficients in the model. RSS is the residual sum of squares Section 4.2.\n\nE\\hat{\\sigma}^2 = \\sigma\n\nPlugging in this estimator for \\sigma^2 into Equation 5.5 and Equation 5.6 yields the following estimators for the variances of the model coefficients.\n\n\\begin{align*}\nVar(\\hat{\\beta}_0) = & \\frac{\\hat{\\sigma}^2}{n_0} \\\\\nVar(\\hat{\\beta}_1) = & \\hat{\\sigma}^2 \\left(\\frac{1}{n_0} + \\frac{1}{n_1}\\right)\n\\end{align*}\n Let us compare the two formulas with the standard errors from the lm output (printed here again for convinience).\n\nsummary(fit)\n\n\nCall:\nlm(formula = kid_score ~ 1 + mom_hs, data = kids)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-57.55 -13.32   2.68  14.68  58.45 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   77.548      2.059  37.670  < 2e-16 ***\nmom_hs        11.771      2.322   5.069 5.96e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.85 on 432 degrees of freedom\nMultiple R-squared:  0.05613,   Adjusted R-squared:  0.05394 \nF-statistic: 25.69 on 1 and 432 DF,  p-value: 5.957e-07\n\n\nThe number of children in the two groups are n_0 = 93 and n_1 = 341. The number of observations is n = n_0 + n_1 = 434. The number of coefficients in the model is p = 2. We can compute the RSS by extracting the residuals from the model fit object (for example by using the residuals function).\n\nres <- residuals(fit)\n\nsum(res ^ 2)\n\n[1] 170261.2\n\n\nThe estimate for \\sigma^2 is therefore:\n\n\\hat{\\sigma}^2 = \\frac{170261.2}{434 - 2} = 394.1231 \\\\\n\\hat{\\sigma} = \\sqrt{\\hat{\\sigma}^2} = 19.85\n\nYou can find last value under residual standard error in the summary of the lm output above.\nNow let us compute the estimated variances and standard deviations of the coefficients:\n\n\\begin{align*}\nVar(\\hat{\\beta}_0) & = \\frac{\\hat{\\sigma}^2}{n_0} = \\frac{394.1}{93} = 4.23 \\\\\nse(\\hat{\\beta}_0) & = \\sqrt{Var(\\hat{\\beta}_0)} = \\sqrt{4.23} = 2.059 \\\\\nVar(\\hat{\\beta}_1) & = \\hat{\\sigma}^2 \\left(\\frac{1}{n_0} + \\frac{1}{n_1}\\right)  = 394.1 \\left(\\frac{1}{93} + \\frac{1}{341}\\right) = 5.39\\\\\nse(\\hat{\\beta}_1) & = \\sqrt{Var(\\hat{\\beta}_1)} = \\sqrt{5.39} = 2.32\n\\end{align*}"
  },
  {
    "objectID": "05-Hypothesis-Testing-and-CI.html#the-population",
    "href": "05-Hypothesis-Testing-and-CI.html#the-population",
    "title": "6  A Simulation Study",
    "section": "6.1 The population",
    "text": "6.1 The population\nIn order to study the statistical properties of the OLS estimators \\hat{\\beta}_0 and \\hat{\\beta}_1 we will generate a large number of samples from the following model.\n\n\\begin{align*}\n& \\text{kid\\_score}_i \\sim N(\\mu_i, \\sigma^2 = 19.85^2), \\quad i = 1,\\ldots, n = 434 \\\\\n& \\mu_i = 77.548 + 11.771 \\text{mom\\_hs}_i, \\quad \\text{mom\\_hs} \\in \\{0, 1\\}\n\\end{align*}\n\\tag{6.1}\nThis model takes the sample of children from Chapter 5 for an inspiration. Basically, we will study a population that looks exactly like the sample of children in the previous example. However, the insights gained in the next sections are more general and are not tied to that specific sample."
  },
  {
    "objectID": "05-Hypothesis-Testing-and-CI.html#selecting-the-sample",
    "href": "05-Hypothesis-Testing-and-CI.html#selecting-the-sample",
    "title": "6  A Simulation Study",
    "section": "6.2 Selecting the sample",
    "text": "6.2 Selecting the sample\nThis section shows presents the technical part of the simulation. In each sample we need two variables: kid_score and mom_hs. In this simulation study we will fix the number of observations with mom_hs = 0 to be exactly 93 and the number of observations with mom_hs = 1 to be exactly 341 (as was the case in the sample of children). The total number of observations in each sample will be 93 + 341 = 434.\nFirst, we construct a column mom_hs variables that has two possible values: 0 and 1. We set the number of zeros and ones to the observed counts of these values in the kids dataset (93 zeros and 341 ones). Then expand_grid repeated this column as many times as the number of unique values of the column R. This will be the number of simulated samples that we’ll generate.\nThe role of expand_grid is to create a large table with 200 \\cdot 434 = 86,800 rows with two variables: R and mom_hs. Our random samples will be identified by the value of the R column. For the first 434 rows its value will be 1, for the next 434 observations its value will be 2 and so forth. For each value of R the content of mom_hs will be identical. The first 93 observations will have mom_hs = 0 and the next 341 observations will have mom_hs = 1.\nOpen the data set sim_grid in the viewer by clicking on it in the global environment. Try to change the arguments (e.g. set R to 1:2) and see how the data set changes.\n\n## Create a table with two columns: R and mom_hs\nsim_grid <- expand_grid(\n  R = 1:200,\n  mom_hs = rep(c(0, 1), c(93, 341))\n)\n\nIn the next step we will generate values for the IQ score of each child by selecting a value at random from a normal distribution. First we create a new column called mu according to Equation 6.1. It will have only two different values: 77.548 or 77.548 + 11.771 = 89.319 depending on the value of mom_hs. Finally, we select a value at random from each child’s IQ scores distributions. According to the model there are two distributions: both are normal distributions but have a different expected value (population average IQ for the group). The result is stored in the kid_score column.\n\n## Fix the random numbers generator so that you can reproduce your results\nset.seed(123)\n\nsim_samples <- sim_grid %>%\n  mutate(\n    ## This is the model for the mean of the IQ scores\n    mu = 77.548 + 11.771 * mom_hs,\n    ## Select a value at random from a normal distribution with \n    ## mean mu and standard deviation 19.85. Note that\n    ## rnorm will take the value of mu in each row of the \n    ## data when generating random values\n    kid_score = rnorm(n = n(), mean = mu, sd = 19.85)\n  )"
  },
  {
    "objectID": "05-Hypothesis-Testing-and-CI.html#ols-estimates-in-each-sample",
    "href": "05-Hypothesis-Testing-and-CI.html#ols-estimates-in-each-sample",
    "title": "6  A Simulation Study",
    "section": "6.3 OLS estimates in each sample",
    "text": "6.3 OLS estimates in each sample\nNow that we have the samples in the data set sim_samples we can compute the OLS estimates for \\beta_0 and \\beta_1 in each sample.\n\nsim_coeffs <- sim_samples %>%\n  group_by(R) %>%\n  ## The tidy function reformats the output of lm so that it can fit in a data frame\n  do(tidy(lm(kid_score ~ 1 + mom_hs, data = .))) %>%\n  select(R, term, estimate, std.error, statistic)\n\n## Creates a separate table with the coefficients for mom_hs\nslopes <- sim_coeffs %>%\n        filter(term == \"mom_hs\")\n\nThe last code chunk may seem a little bit complex, but is simply groups the sim_samples data by the sample number and then runs the lm function with the data in each sample. You can verify the results in sim_coeffs by running the lm function manually with the data from the first sample (of course you can choose another sample). The coefficient estimates in sim_coeffs are stored in a column called estimate. As there are two coefficients in our model, the column term tells you whether a row holds the estimate for the intercept \\hat{\\beta}_0 (term == \"(Intercept)\") or the slope \\hat{\\beta}_1 (term == \"mom_hs\").\nYou can use the filter function to select only the observations in the first sample\n\nsample_1 <- sim_samples %>% filter(R == 1)\n\nNow apply lm on that sample and compare the coefficient estimates with the first two values in sim_coeffs.\n\nlm(kid_score ~ 1 + mom_hs, data = sample_1)\n\n\nCall:\nlm(formula = kid_score ~ 1 + mom_hs, data = sample_1)\n\nCoefficients:\n(Intercept)       mom_hs  \n      78.92        10.42"
  },
  {
    "objectID": "05-Hypothesis-Testing-and-CI.html#distribution-of-the-ols-estimates",
    "href": "05-Hypothesis-Testing-and-CI.html#distribution-of-the-ols-estimates",
    "title": "6  A Simulation Study",
    "section": "6.4 Distribution of the OLS estimates",
    "text": "6.4 Distribution of the OLS estimates\nFirst we plot the distribution of the slope estimates for each sample. In geom_point we add a small random value to each estimate so that we can see all the points (this is what position = \"jitter\" does). Otherwise all estimates would lie on the x-axis and we would not be able to see the individual points.\n\n# The red line is drawn at the population value of $\\beta_1$: 11.77. The  position of the dots on the y-axis does not convey any meaningful information and it only serves to  disentangle  the points so that they don't overplot.\n\nslopes %>%\n  ggplot(aes(x = estimate)) +\n  geom_point(\n    aes(y = 0),\n    position = \"jitter\",\n    size = 1 / 2,\n    alpha = 0.5\n  ) +\n  geom_boxplot(alpha = 0.5) +\n  ## Draws a density estimate\n  geom_density(color = \"steelblue\") +\n  ## Draws a vertical line a 11.77 (the value of beta_1 used in the simulation)\n  geom_vline(xintercept = 11.77, color = \"firebrick\") + \n  ## Sets the labels of the x and y axis and the title of the plot\n  labs(\n    x = \"Estimate\",\n    title = \"Distribution of the slope estimates (R samples)\",\n    y = \"Density\"\n  )\n\n\n\n\nFigure 6.1: Sampling distribution of \\hat{\\beta}_1. Each dot represents the slope estimate in one sample.\n\n\n\n\nThe plot reveals three key insights:\n\nThe estimates for the slope (\\beta_1) vary from sample to sample\nIn most of the samples the estimate was close to the the real value of \\beta_1 (11.77)\nThere is a small number of samples that resulted in extreme values of the estimate, e.g. in sample 97 \\hat{\\beta}_1 = 4.7 and in sample 44 the estimated coefficient was 18.6.\n\nWe can estimate the center (expected value) of this distribution by computing the mean estimate (i.e. the average estimate over all generated samples).\n\nmean(slopes$estimate)\n\n[1] 11.73587\n\n\nWe see that this value is very close to the real value of 11.77. This is a consequence of a property of the OLS estimator which we call unbiasedness Theorem 4.3. The average estimate computed above estimates the expected value of the distribution of \\hat{\\beta}_1: E\\hat{\\beta}_1.\nThe standard deviation of this distribution is called the standard error of \\hat{\\beta}_1. It describes the spread of the sampling distribution of the estimates.\n\nsd(slopes$estimate)\n\n[1] 2.504912"
  },
  {
    "objectID": "05-Hypothesis-Testing-and-CI.html#hypothesis-testing",
    "href": "05-Hypothesis-Testing-and-CI.html#hypothesis-testing",
    "title": "6  A Simulation Study",
    "section": "6.5 Hypothesis testing",
    "text": "6.5 Hypothesis testing\nIn Chapter 5 we tested the hypothesis that \\beta_1 = 0 vs \\beta_1 \\neq 0 and talked about a t-statistic, a t-distribution and about critical values derived from that distribution. In the present section our goal is to demystify all these words.\nWe begin with a simple test of hypotheses about the population value of one of the regression coefficients. Let us start with \\beta_1 and let us suppose that we want to test the theory that the difference between the average IQ score of the two groups in the population equals exactly 11.77. Notice that this is the value of \\beta_1 that we used in the simulation, so this theory is correct. We also want to test this theory against the alternative that the difference between the average IQ scores is less than 11.77.\n\n6.5.1 Testing a true null hypothesis\n\nH_0: \\beta_1 \\geq 11.77\\\\\nH_1: \\beta_1 < 11.77\n\\tag{6.2}\nThe mathematical statistics informs us how to summarize the data so that we can make a decision whether to reject the null hypothesis. The t-statistic is the summary that we need for this test. In general it is the difference between the estimate for \\beta_1 and the value under the null hypothesis divided by the standard error of the estimate.\n\n\\text{t-statistic} = \\frac{\\hat{\\beta}_1 - \\beta_1^{H_o}}{se(\\hat{\\beta}_1)}\n\nIn our case the value of \\beta_1 under the null hypothesis is 11.77 so the test statistic becomes:\n\n\\text{t-statistic} = \\frac{\\hat{\\beta}_1 - 11.77}{se(\\hat{\\beta}_1)}.\n\nNotice that this statistic can be large (far away from zero) in two cases:\n\nThe null hypothesis is not true. As we have seen in Figure 6.1, the estimates \\hat{\\beta}_1 vary around the true value of \\beta_1 and this behavior does not depend on the hypothesis that we are testing.\nLarge values of the t-statistic can happen by chance alone, even if the null hypothesis is true. Such large values occur more often (in more samples) when the distribution of \\hat{\\beta}_1 has a high standard deviation.\n\nLet us compute the t-statistic for the hypothesis pair in 6.2. In the data set slopes the column estimate holds the estimated \\beta_1, the (estimate) standard error is in the column std.error. Figure 6.2 shows the distribution of the t-statistics in our simulated samples.\n\nslopes <- slopes %>%\n  mutate(\n    t_statistic = (estimate - 11.77) / std.error\n)\n\n\nslopes %>%\n  ggplot(aes(x = t_statistic)) +\n  geom_point(\n    aes(y = 0),\n    position = \"jitter\",\n    size = 1 / 2,\n    alpha = 0.5\n  ) +\n  geom_boxplot(alpha = 0.5) +\n  labs(\n    x = \"Value of the t-statistic\",\n    title = \"Distribution of t-statistic under a true null hypothesis beta_1 = 11.77 (2000 samples)\",\n    y = \"\"\n  ) +\n  geom_density(color = \"steelblue\") +\n  geom_vline(xintercept = 0, color = \"red\") +\n  geom_vline(xintercept = c(-2), color = \"steelblue\", lty = 2) +\n  geom_vline(xintercept = c(-3), color = \"firebrick\", lty = 2) +\n  xlim(c(-4, 8)) +\n  scale_x_continuous(breaks = c(-3, -2, 0))\n\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale.\n\n\n\n\n\nFigure 6.2: Distribution of the t-statistic when the null hypothesis is correct.\n\n\n\n\nWe should see two things in Figure 6.2.\n\nFirst, the t-statistic is different in each sample, even though all samples come from one and the same population (model).\nThe center of the distribution is close to zero. We should have expected this, because we know that the null hypothesis in 6.2 is true.\nIn some samples the t-statistic has large negative values. In one sample it is even less than -3.\n\nFor the hypothesis pair in 6.2, large negative values of the t-statistic are consistent with the alternative. Small values close to zero are consistent with the null hypothesis. However, we can see in Figure 6.2 that large negative values of the t-statistic can happen by pure change chance even under a true null hypothesis. Because such extreme values can occur by chance, any decision rule that rejects the null hypothesis for t-statistics lower than some threshold will inevitably produce wrong decision (rejection of the null hypothesis even if it is true).\nLet us study the two decision rules depicted as vertical dashed lines in Figure 6.2. The first rule (blue) rejects the null hypothesis in samples with a t-statistic less than -2. The second decision rule rejects the null hypothesis for t-statistics less than -3.\nIn the simulation we can simply count the number of samples where we will make a wrong decision to reject the null hypothesis (remember, in this example H_0 is true).\n\nslopes <- slopes %>%\n  mutate(\n    wrong_decision_blue = t_statistic < -2,\n    wrong_decision_red = t_statistic < -3\n  )\n\n## Share of TRUE values (blue)\nsum(slopes$wrong_decision_blue)\n\n[1] 7\n\nmean(slopes$wrong_decision_blue)\n\n[1] 0.035\n\n## Share of TRUE values (red)\nsum(slopes$wrong_decision_red)\n\n[1] 2\n\nmean(slopes$wrong_decision_red)\n\n[1] 0.01\n\n\nThe “less than -2” rule makes a wrong decision in 7 out of 200 samples (3.5 percent). The “less than -3” rule makes a wrong decision in 2 out of 200 samples (one percent).\n\n\n6.5.2 Testing a wrong null hypothesis\nNow that we’ve seen the distribution of the t-statistic under a true null hypothesis, let’s now examine its distribution under a wrong null hypothesis.\nThe following hypothesis is wrong in our simulation (because the real \\beta_1 = 11.77).\n\nH_0: \\beta_1 \\leq 0\\\\\nH_1: \\beta_1 > 0\n\\tag{6.3}\nWe use the same statistic for testing this hypothesis. The only difference now is that the alternative is in the other direction. Therefore we would reject for large positive values of the t-statistic.\n\n\\text{t-statistic} = \\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)}\n\\tag{6.4}\nWe will compute the t-statistic in 6.4 for each sample and will visualize its distribution in Figure 6.3.\n\nslopes <- slopes %>%\n  mutate(\n    t_statistic0 = (estimate - 0) / std.error\n  )\n\n\nslopes %>%\n  ggplot(aes(x = t_statistic0)) +\n  geom_point(\n    aes(y = 0),\n    position = \"jitter\",\n    size = 1 / 2,\n    alpha = 0.5\n  ) +\n  geom_boxplot(alpha = 0.5) +\n  labs(\n    x = \"t-statistic\",\n    y = \"Density\"\n  ) +\n  geom_density(color = \"steelblue\") +\n  geom_vline(xintercept = 0, color = \"red\") +\n  geom_vline(xintercept = c(2), color = \"steelblue\", lty = 2) +\n  geom_vline(xintercept = c(3), color = \"firebrick\", lty = 2) +\n  xlim(c(-4, 8)) +\n  scale_x_continuous(breaks = c(0, 2, 3))\n\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale.\n\n\n\n\n\nFigure 6.3: Distribution of the t-statistic under the (wrong) null hypothesis \beta_1 = 0.\n\n\n\n\nNotice that the center of the distribution of the t-statistic is no longer close to zero. This happens because the null hypothesis is wrong in this example.\nLet us consider two decision rules for rejecting H_0. The first one rejects for t-statistics larger than 2, the other one rejects for t-statistics larger than 3.\nWe can count the number of samples where these two rules lead to wrong decisions. A wrong decision in this case is the non-rejection of the null hypothesis.\n\nslopes <- slopes %>%\n  mutate(\n    ## A wrong decision here is to not-reject the null\n    wrong_decision_blue_1 = t_statistic0 < 2,\n    wrong_decision_red_1 = t_statistic0 < 3\n  )\n\n## Number of TRUE values\nsum(slopes$wrong_decision_blue_1)\n\n[1] 0\n\nsum(slopes$wrong_decision_red_1)\n\n[1] 8\n\n## Share of TRUE values\nmean(slopes$wrong_decision_blue_1)\n\n[1] 0\n\nmean(slopes$wrong_decision_red_1)\n\n[1] 0.04\n\n\nThe rejection rule “greater than 2” rejected the null hypothesis in all samples. The rejection rul “greater than 3” failed to reject the null hypothesis in 8 out of 200 samples (four percent)."
  },
  {
    "objectID": "05-Hypothesis-Testing-and-CI.html#the-t-distribution",
    "href": "05-Hypothesis-Testing-and-CI.html#the-t-distribution",
    "title": "6  A Simulation Study",
    "section": "6.6 The t-distribution",
    "text": "6.6 The t-distribution\nThe mathematical statistics provides us with a theorem that states that under certain conditions (we will talk more about these assumptions) the t-statistic follows a (central) t-distribution with degrees of freedom equal to n - p where n is the number of observations in the sample and p is the number of model coefficients (betas) in the model.\nThe t-distributions relevant for us have zero mean and the variance determined by the the parameter of the distribution (the degrees of freedom). T-distributions with low degrees of freedom place less probability at the center (zero) and more in the tails of the distribution (extreme values far away from the center) Figure 6.4.\n\ndt <- expand_grid(\n  ## Creates a sequence of 100 numbers between -3 and 3\n  x = seq(-4, 4, length.out = 200),\n  df = c(1, 5, 50, 500)\n) %>%\n  mutate(\n    ## Computes the standard normal density at each of the 100 points in x\n    t_dens = dt(x, df = df),\n    df = factor(df)\n  )\n\nggplot() +\n  ## Draws the normal density line\n  geom_line(data = dt, aes(x = x, y = t_dens, colour = df)) + \n  labs(\n    y = \"Density\"\n  )\n\n\n\n\nFigure 6.4: Densities of t-distributions with different degrees of freedom (df).\n\n\n\n\nKnowing the distribution of the test statistic under the null hypothesis (if it is true) allows us to derive critical values (boundaries for the rejection rules) for the tests. These critical values depend on the type of the alternative in the test.\nFor a hypothesis pair of the type considered in Section 6.5.1 we reject for negative values ot the t-statistic.\n\nH_0: \\beta_1 \\geq \\beta_1^{H_0}\\\\\nH_1: \\beta_1 < \\beta_1^{H_0}\n\\tag{6.5}\nWe want to choose the critical value that meets a predetermined probability of wrong rejection of a true null hypothesis.\n\n\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\n\n\n\n\n\nFigure 6.5: Critical value for a one-sided alternative (left).\n\n\n\n\nIf we choose the error probability to be \\alpha, then the critical value for the one-sided alternative in 6.5 is the \\alpha quantile of the t-distribution. We write t_{\\alpha}(n - p) to denote this quantile.\nIn the simulation we had sample sizes n = 434 and two coefficients in the model (\\beta_0, \\beta_1), therefore p = 2. Let \\alpha = 0.05. We can compute the quantile using the qt function.\n\nqt(0.05, df = 434 - 2)\n\n[1] -1.648388\n\n\nThe 0.05 quantile of a t-distribution with 432 degrees of freedom is t_{0.05}(432) = -1.64. Therefore the probability to observe samples with a t-statistic less than -1.64 (under a true) null hypothesis is 0.05.\nLets consider another alternative.\n\nH_0: \\beta_1 \\leq \\beta_{H_0}\\\\\nH_1: \\beta_1 > \\beta_{H_0}\n\nThe t-statistic is the same as in the previous test. This time we reject for t-statistics that are large (and positive), because these are the values expected under the alternative.\n\n\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\n\n\n\n\n\nFigure 6.6: Critical value for a one-sided alternative (right).\n\n\n\n\nThis time the critical value of the 1 - \\alpha quantile of the t-distribution.\nFor df = 432 and \\alpha = 0.05 this quantile is t_{0.95}(432) = 1.64.\n\nqt(1 - 0.05, df = 434 - 2)\n\n[1] 1.648388\n\n\nDue to the symmetry of the t-distribution it is equal to the 0.05 quantile in absolute value.\nFinally, let us consider a two sided alternative.\n\nH_0: \\beta_1 = \\beta_1^{H_0} \\\\\nH_1: \\beta_1 \\neq \\beta_1^{H_0}\n\nIn this test we would reject H_0 both for very large positive values and for large (in absolute value) negative values of the t-statistic. Again, we can obtain the critical values from the t-distribution. Because we reject for both positive and negative values, we choose the critical value so that the total probability of t-statistics greater than the upper and less than the lower critical value is \\alpha.\n\n\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\n\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\n\n\n\n\n\nFigure 6.7: Critical values for a two-sided alternative.\n\n\n\n\n\nqt(0.05 / 2, df = 434 - 2)\n\n[1] -1.965471\n\nqt(1 - 0.05 / 2, df = 434 - 2)\n\n[1] 1.965471\n\n\nFor df = 432 and \\alpha = 0.05 the critical values are t_{0.05 / 2}(432) = -1.96 and t_{1 - 0.05 / 2}(432) = 1.96. Due to the symmetry of the t-distribution these critical values are equal in absolute value.\n\n6.6.1 The p-value\nAnother way to decide whether to reject a null hypothesis or not is to compute the p-value of the test. The p-value is conditional the probability (under the null hypothesis) to see samples with a t-statistic that is even more extreme than the one observed in a sample. How the p-value is calculated depends on the alternative in the test.\nLet us take the example with the children from Chapter 5. For convenience we print here again the output from summary.\n\nsummary(lm(kid_score ~ 1 + mom_hs, data = kids))\n\n\nCall:\nlm(formula = kid_score ~ 1 + mom_hs, data = kids)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-57.55 -13.32   2.68  14.68  58.45 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   77.548      2.059  37.670  < 2e-16 ***\nmom_hs        11.771      2.322   5.069 5.96e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.85 on 432 degrees of freedom\nMultiple R-squared:  0.05613,   Adjusted R-squared:  0.05394 \nF-statistic: 25.69 on 1 and 432 DF,  p-value: 5.957e-07\n\n\nNow we consider three null-hypotehsis and alternative pairs.\n\nH_0: \\beta_1 \\leq 0 \\\\\nH_0: \\beta_1 > 0\n The sample value of the t-statistic is\n\n\\frac{11.77 - 0}{2.3} \\approx 5.1\n\nFor this alternative extreme (consistent with the alternative) values of the t-statistic are large positive values. Therefore the p-value of this test is\n\nP(t > \\text{t-statistic}^{obs} | H_0)\n\nThe notation \\text{t-statistic}^{obs} refers to the value of the statistic in the sample (5.1). If H_0 is true, i.e. \\beta_1 = 0 we know that the t-statistic follows a t-distribution with df = 432. We can compute the probability of samples with t-statistics less than 5.1 using the pt function.\n\n1 - pt(5.1, df = 434 - 2)\n\n[1] 2.546875e-07\n\n\nThis probability is equal to about 2.5 out of 10 million. This means that samples with a t-statistic greater than the one observed (5.1) are extremely rare. Either we have been extremely lucky to select a very rare sample or the null hypothesis is wrong.\nLet us now consider the other one-sided alternative\n\nH_0: \\beta_1 \\geq 0 \\\\\nH_1: \\beta_1 < 0\n For this alternative extreme values of the t-statistic are large (in absolute value) negative values. Therefore the p-value of the test is\n\nP(t < \\text{t-statistic}^{obs} | H_0)\n We can compute this probability using pt.\n\npt(5.1, df = 434 -2)\n\n[1] 0.9999997\n\n\nThis p-value is close to one, implying that under the null hypothesis almost all samples would have a t-statistic less than 5.1.\nFinally, let us consider the two-sided alternative\n\nH_0: \\beta_1 = 0 \\\\\nH_1: \\beta_1 \\neq 0\n\nNow both large positive and large (in absolute value) negative values of the t-statistic are extreme (giving evidence against the null hypothesis). The p-value in this case is the sum of two probabilities. For t = 5.1 more extreme values against the null hypothesis are the ones greater than 5.1 and the ones less than -5.1.\n\n\\text{p-value} = P(t < -5.1 | H_0) + P(t > 5.1 | H_0)\n We can compute it with a the pt function. Notice that this is the same value (up to rounding errors) as the one shown in the Pr(>|t|) column in the output of summary.\n\npt(-5.1, df = 434 - 2) + (1 - pt(5.1, df = 434 - 2))\n\n[1] 5.09375e-07\n\n\nHow would the p-value look like if the t-statistic was negative, e.g. -5.1? Again, extreme values are the ones less that -5.1 and greater than the absolute value, i.e. 5.1. We can write this more succinctly (and this explains the notation for the column in the summary output):\n\n\\text{p-value} = P(|t| > |\\text{t-statistic}^{obs}| |H_0)\n\nFinally, let us see the distribution of p-values in the simulation study. Here we will compute it for the true null hypothesis H_0: \\beta_1 = 11.77, H_1: \\beta_1 \\neq 11.77. Figure Figure 6.8 shows the estimated density of the p-values (in each sample). You can see that the density is roughly constant over the interval [0, 1]. Indeed, it can be shown that the p-value if uniformly distributed over this interval if the null hypothesis is true.\n\nslopes <- slopes %>%\n  mutate(\n    p_value_h0_true = 2 * pt(-abs(t_statistic), df = 434 - 2)\n  )\n\n\nslopes %>%\n  ggplot(aes(x = p_value_h0_true)) + \n  geom_density() + \n  labs(\n    x = \"p-value\",\n    y = \"Density\"\n  )\n\n\n\n\nFigure 6.8: Distribution of p-values under the (true) null hypothesis\n\n\n\n\nWhen using the p-value to make a rejection decision we compare it to a predetermined probability of a wrong rejection of a true null hypothesis. This is the same approach that we followed when we derived the critical values of the tests. A widely used convention is to reject a null hypothesis if the p-value is less than 0.05.\nLet us apply this convention in the simulation and see in how many samples we make a wrong decision.\n\nslopes <- slopes %>%\n  mutate(\n    reject_h0_based_on_p_value = p_value_h0_true < 0.05\n  )\n\ntable(slopes$reject_h0_based_on_p_value)\n\n\nFALSE  TRUE \n  187    13"
  },
  {
    "objectID": "05-Hypothesis-Testing-and-CI.html#sec-sim-ci",
    "href": "05-Hypothesis-Testing-and-CI.html#sec-sim-ci",
    "title": "6  A Simulation Study",
    "section": "6.7 Confidence intervals",
    "text": "6.7 Confidence intervals\nFrom the distribution of the t-statistic we can see that\n\n\\text{t-statistic} = \\frac{\\hat{\\beta}_1 - \\beta_1}{se(\\hat{\\beta}_1)} \\sim t(n - p)\n\nTherefore the probability to observed a value of the t-statistic in the interval [t_{\\alpha / 2}(n - p), t_{1 - \\alpha / 2}] is 1 - \\alpha.\n\nP\\left(t_{\\alpha / 2}(n - p) < \\frac{\\hat{\\beta}_1 - \\beta_1}{se(\\hat{\\beta}_1)} < t_{1 - \\alpha / 2}(n - p)\\right) = \\\\\nP\\left(\\hat{\\beta_1} + se(\\hat{\\beta}_1) t_{\\alpha/2}(n - p) < \\beta_1 < \\hat{\\beta_1} + se(\\hat{\\beta}_1) t_{1 - \\alpha/2}(n - p) \\right) = 1 - \\alpha\n\nCompute the upper and the lower bounds of the confidence intervals for each sample in the simulation with a coverage probability of 1 - \\alpha = 0.9. In how many samples did the confidence interval contain the real coefficient \\beta_1?\n\nslopes <- slopes %>%\n  mutate(\n    CI_lower = estimate + std.error * qt(0.1 / 2, df = 434 - 2),\n    CI_upper = estimate + std.error * qt(1 - 0.1 / 2, df = 434 - 2),\n    ## Construct a new variables that is TRUE/FALSE if the\n    ## true value of beta_1 (11.77) is inside the confidence interval\n    beta1_in_CI = CI_lower < 11.77 & 11.77 < CI_upper\n  )\n\n\nsum(slopes$beta1_in_CI)\n\n[1] 178\n\nmean(slopes$beta1_in_CI)\n\n[1] 0.89\n\n\nThe CI contained the true value of \\beta_1 in 178 out of 200 samples (89 percent). This is quite close to the coverage probability 1 - \\alpha = 0.9 The CI that we constructed above are visualized in Figure 6.9. For the sake of clarity the plot is limited to the first 50 samples. Confidence intervals that do not contain the true value of \\beta_1 are show in red. Notice that the boundaries differ from sample to sample (as to the coefficient estimates).\n\nslopes %>%\n  ungroup() %>%\n  slice_head(n = 50) %>%\n  ggplot(\n    aes(\n      x = estimate, \n      y = factor(R),\n      xmin = CI_lower,\n      xmax = CI_upper,\n      color = beta1_in_CI\n    )\n  ) + \n  geom_point() + \n  geom_errorbarh() + \n  labs(\n    x = \"\",\n    y = \"Sample\",\n    color = \"Beta1 in CI\"\n  ) + \n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n  ) + \n  geom_vline(\n    xintercept = 11.77\n  )\n\n\n\n\nFigure 6.9: Confidence intervals for the first 50 samples. The vertical line is drawn at 11.77 (the real value of \\beta_1).\n\n\n\n\n\ndt <- tibble(\n  x = 1:10,\n  y = rnorm(10, mean = 2 + 0.3 * x, sd = 0.5)\n)\n\nfit <- lm(y ~ 0 + x, data = dt)\nsum(residuals(fit))\n\n[1] 4.705218"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bertsekas, Dimitri P., and John N. Tsitsiklis. 2008. Introduction to\nProbability. 2nd ed. Optimization and Computation Series.\nBelmont: Athena scientific.\n\n\nDalgaard, Peter. 2008. Introductory Statistics with\nR. 2nd edition. Statistics and Computing. New\nYork: Springer.\n\n\nFaraway, Julian James. 2015. Linear Models with R.\nSecond edition. Chapman & Hall/CRC Texts\nin Statistical Science Series. Boca Raton: CRC Press,\nTaylor & Francis Group.\n\n\nFreedman, David, Robert Pisani, and Roger Purves. 2007.\nStatistics. 4th ed. New York: W.W. Norton\n& Co.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and\nOther Stories. Analytical Methods for Social Research.\nCambridge New York, NY Port Melbourne, VIC New Delhi\nSingapore: Cambridge University Press. https://doi.org/10.1017/9781139161879.\n\n\nSheather, Simon. 2009. A Modern Approach to\nRegression with R. Edited by George\nCasella, Stephen Fienberg, and Ingram Olkin. Springer Texts\nin Statistics. New York, NY: Springer\nNew York. https://doi.org/10.1007/978-0-387-09608-7.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science:\nImport, Tidy, Transform, Visualize, and Model Data. First edition.\nSebastopol, CA: O’Reilly."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to linear regression analysis",
    "section": "",
    "text": "Setup\nThe exercise classes require a minimal software setup:\n\nOpen https://cran.r-project.org/ and you will find links to R builds for different operating systems. Click on the link matching your operating system and choose the latest version of R. When using the Windows operating system you will see a link “Install R for the first time”. Click on this link and then download R installer. Run the installer and accept the default settings.\nAfter installing R, open https://posit.co/download/rstudio-desktop/. If the web page recognizes your operating system you will see a download button (right side of the page) for R studio. If the button does not appear, scroll down the page and find the installer appropriate for your operating system.\nOpen R Studio on your computer and create a new R Markdown file. Find the file drop down menu on the top-left part of the interface, select “New file”, then find “R Markdown” and click on it. If this is the first time you create a R Markdown file, R Studio will ask you to install a number of packages. Confirm this and wait for the packages to download and install. Click “OK” on the dialogue button, and you will see a text file with a simple R Markdown template that demonstrates some basic functionality.\nIn this exercise class we use a lot of functions from the tidyverse system and several other packages. In order to access these you need the to install this package. Find the R console in R studio and paste the following line on the command line. Press enter to run it. This will install some of the necessary packages on your system.\n\n\ninstall.packages(c(\"tidyverse\", \"broom\", \"patchwork\", \"skimr\"))\n\nOptional: more on R Markdown: https://rmarkdown.rstudio.com/lesson-1.html\nOptional: a base R cheatsheet: https://www.datacamp.com/cheat-sheet/getting-started-r"
  }
]