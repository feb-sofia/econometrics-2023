[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to linear regression analysis",
    "section": "",
    "text": "Setup\nThe exercise classes require a minimal software setup:\n\nOpen https://cran.r-project.org/ and you will find links to R builds for different operating systems. Click on the link matching your operating system and choose the latest version of R. When using the Windows operating system you will see a link “Install R for the first time”. Click on this link and then download R installer. Run the installer and accept the default settings.\nAfter installing R, open https://posit.co/download/rstudio-desktop/. If the web page recognizes your operating system you will see a download button (right side of the page) for R studio. If the button does not appear, scroll down the page and find the installer appropriate for your operating system.\nOpen R Studio on your computer and create a new R Markdown file. Find the file dropdown menu on the top-left part of the interface, select “New file”, then find “R Markdown” and click on it. If this is the first time you create a R Markdown file, R Studio will ask you to install a number of packages. Confirm this and wait for the packages to download and install. Click “Ok” on the dialogue button, and you will see a text file with a simple R Markdown template that demonstrates some basic functionality.\nIn this exercise class we use a lot of functions from the tidyverse system and several other packages. In order to access these you need the to install this package. Find the R console in R studio and paste the following line on the command line. Press enter to run it. This will install some of the necessary packages on your system.\n\n\ninstall.packages(c(\"tidyverse\", \"broom\", \"patchwork\", \"skimr\"))\n\nOptional: more on R Markdown: https://rmarkdown.rstudio.com/lesson-1.html\nOptional: a base R cheatsheet: https://www.datacamp.com/cheat-sheet/getting-started-r"
  },
  {
    "objectID": "intro.html#prep",
    "href": "intro.html#prep",
    "title": "1  Data description",
    "section": "1.1 Prep",
    "text": "1.1 Prep\nAn exploratory analysis of the data reveals a large number of consecutive snapshots from one and the same camera of the same vehicle in a short period of time. In order to avoid counting these snapshots as multiple inbound or outbound moves, we take the first snapshot with the smallest time stamp (earliest snapshot), removing (XXX, w) subsequent snapshots from the same camera.\n\nplates_tbl <- xtabs( ~ plate + first_inbound, data = snaps_tmp)\n\n\npk_plates <- snaps_tmp %>% filter(str_starts(plate, \"PK\"), first_inbound == TRUE)\n\n\ninout_plates <- snaps_tmp %>% \n  filter(first_inbound == TRUE, day_span < 4)\n\n\nplates_tbl %>% \n  as_tibble() %>%\n  arrange(desc(n)) %>%\n  filter(n < 100) %>%\n  slice_head(n = 10)\n\n# A tibble: 10 × 3\n   plate    first_inbound     n\n   <chr>    <chr>         <int>\n 1 CO5983CK FALSE            98\n 2 M3732BH  FALSE            98\n 3 CB3382KH FALSE            97\n 4 CB3488AP FALSE            97\n 5 CB4551AK FALSE            97\n 6 CB7443PK FALSE            97\n 7 CB4372AK FALSE            96\n 8 CB1815AH FALSE            95\n 9 CB1985TH FALSE            95\n10 CB2141TK FALSE            95\n\n\n\ntmp <- snaps %>%\n  filter(str_ends(plate, \"A7170MB\"))\n\n\nsingle_vehicle <- snaps_tmp %>% \n  filter(plate == \"A7276HH\") %>%\n  arrange(dttime) %>%\n  select(dttime, cam_id, dir, loc, inner) %>%\n  filter(inner == 0)\n\nAdding missing grouping variables: `plate`, `day`\n\nsingle_vehicle\n\n# A tibble: 0 × 7\n# Groups:   plate, day [0]\n# ℹ 7 variables: plate <chr>, day <int>, dttime <dttm>, cam_id <chr>,\n#   dir <chr>, loc <chr>, inner <dbl>"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Conclusion",
    "section": "",
    "text": "The present analysis considered (XXX, w) snapshots from static traffic cameras around the small ring of Sofia, Bulgaria.\n\nThe analysis assumes that\n\nData from only one month so seasonal variation cannot be explored\nData is insufficient for weekly seasonality as there are only four weeks in the data\n\n\n3 Въпроси\n\nКакво всъщност снимат тези камери? Например CA8154HX е сниман над 10 000 пъти от една и съща камера на кръстовището на Славянска и В. Левски. Силно подозирам, че ако отидем там ще намерим колата паркирана някъде до това кръстовище.\nКакво са CAM и BUS?\nВ разпознатите номера на превозните средства има странни стойности, например 00, това валидни номера ли са?\nВъзможно ли е камерите да правят грешки при разпознаването на номерата?\nИмам впечатлението, че освен неразпознатите номера има и немалък брой превозни средства, които изобщо не са били засечени от камерите"
  },
  {
    "objectID": "03-Simple-Linear-Model.html",
    "href": "03-Simple-Linear-Model.html",
    "title": "3  Simple Linear Model",
    "section": "",
    "text": "4 Interpretation of the linear equation\n\\[\ni = 1,\\ldots,n = 30\\\\\nx_i: \\text{ number of invoices on day } i \\\\\ny_i: \\text{ processing time on day } i \\text{ (hours)} \\\\\n\\]\nUnits of measurement?\n\\[\ny_i [hours] = 0.1 [hours] + 0.015 [\\frac{hours}{invoice}] x_i[\\#invoices] + e_i[hours]\\\\\n\\]\n\\[\ny_i = 0.1 + 0.015 x_i + e_i\\quad \\text{black line}\\\\\ny_i =  0.1 + 0.001 x_i + e_i\\quad \\text{red line}\n\\]\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + e_i\n\\]\nwhere \\(\\beta_0\\) and \\(\\beta_1\\) are unknown coefficients. Because the coefficients are unknown, we must guess these from the data.\nWe can compare different guesses about the coefficients using the distance between the regression line (the model) and the dots (the observed data).\n\\[\n\\hat{y}_i = \\hat{\\mu}_i= \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\text{ regression line (model)}\n\\]\nFor example with \\(\\hat{\\beta}_0 = 0.1, \\hat{\\beta}_1 = 0.015\\) and \\(x = 120\\):\n\\[\ny_{26} = 2.5, x_{26} = 120\\\\\n\\hat{y}_{26} = 0.1 + 0.015 * 120  = 1.9\n\\]\nThe observed processing time on the 26-th day was 2.5 hours, leading to difference of 0.6 hours between observed and predicted processing time. We call this difference the residual (on day 26 in this example).\n\\[\nr_{26} = y_{26} - \\hat{y}_{26} = 2.5 - 1.9 = 0.6 [hours]\n\\]\nResidual for obs. \\(i\\).\n\\[\nr_i = y_i - \\hat{y}_i\n\\]\nLet us formalize the idea that small residuals lead to better prediction.\n\\[\n\\frac{1}{n}\\sum_{i = 1}^{n}r_i\n\\]\n\\[\n\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\n\\]\n\\[\n\\min_{\\hat{\\beta}_0, \\hat{\\beta}_1} RSS(\\hat{\\beta}_0, \\hat{\\beta}_1) = \\sum_{i = 1}^n r_i ^ 2 = \\sum_{i = 1}^n (y_i - \\hat{y}_i) ^ 2 =\\sum_{i = 1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2\n\\]\nRSS: Residual Sum of Squares. When we solve this minimization problem we get the Ordinary Least Squares (OLS) estimators for \\(\\beta_0\\) and \\(\\beta_1\\).\nCalculation of the OLS coefficient estimates given data and the model\n\\[\ny_i \\sim N(\\mu_i, \\sigma^2)\\\\\n\\mu_i = \\beta_0 + \\beta_1 x_i + e_i\n\\]\nUsing OLS we get the estimated (“guessed”)\n\\[\n\\hat{\\beta}_0 = 0.64171; \\hat{\\beta}_1 = 0.01129\\\\\n\\hat{y}_i = 0.64171 + 0.01129 x_i\n\\]\nCalculating the residual sum of squares (RSS) for different lines: a line with manually chosen (arbitrary) coefficients, and the OLS estimates of the coefficients.\n\\[\n\\hat{y}_i = 0.64171 + 0.01129 x_i\n\\]\nRSS for the manually chosen coefficients (0.1; 0.015)\nRSS for the OLS coefficients (intercept = 0.64171; slope = 0.01129)\nLet us get back to the original task to predict the time needed to process 50, 120, 201, 250, 400 invoices.\n\\[\n\\hat{y} = 0.64 + 0.011 x\n\\] \\[\n\\hat{y}_{x = 50} = 0.64 + 0.011 * 50 = 1.19 [hours]\n\\] \\[\n\\hat{y}_{x = 120} = 0.64 + 0.011 * 120 = 1.96 [hours]\n\\]\n\\[\n\\hat{y} = 0.64 + 0.011 x\n\\]\n\\[\n\\hat{y}_{x = 0} = 0.64 + 0.011 * 0 = 0.64 [hours]\n\\] The intercept estimates the expected fixed costs (in terms of time) of the firm.\nThe slope is 0.011 [hours/invoice]: marginal costs: these are the additional costs (in terms of time) for one additional unit of input (one invoice).\nThe predicted processing time for 50 invoices (given the linear model that we assumed and method for estimating its coefficients) is 1.2 hours.\nPrediction for 50 invoices:\n\\[\n\\hat{y}_{x = 50} = 0.64171 + 0.01129*50 = 1.20621 [hours]\n\\] Prediction for 120 invoices:\n\\[\n\\hat{y}_{x = 120} = 0.64171 + 0.01129*120 = 1.99651 [hours]\n\\]\nCreate a data object that holds the values for which we want to make a prediction.\nTo compute the predictions using R we can use the predict function.\n\\[\n\\hat{y}_{x = 50} = 0.6417 + 0.01129 * 50 = 1.206292\n\\]\nPrediction for 120 invoices:\n\\[\n\\hat{y}_{x = 120} = 0.6417 + 0.01129 * 120 = 1.996707\n\\]"
  },
  {
    "objectID": "00-Setup.html",
    "href": "00-Setup.html",
    "title": "1  Setup",
    "section": "",
    "text": "The exercise classes require a minimal software setup:\n\nOpen https://cran.r-project.org/ and you will find links to R builds for different operating systems. Click on the link matching your operating system and choose the latest version of R. When using the Windows operating system you will see a link “Install R for the first time”. Click on this link and then download R installer. Run the installer and accept the default settings.\nAfter installing R, open https://posit.co/download/rstudio-desktop/. If the web page recognizes your operating system you will see a download button (right side of the page) for R studio. If the button does not appear, scroll down the page and find the installer appropriate for your operating system.\nOpen R Studio on your computer and create a new R Markdown file. Find the file dropdown menu on the top-left part of the interface, select “New file”, then find “R Markdown” and click on it. If this is the first time you create a R Markdown file, R Studio will ask you to install a number of packages. Confirm this and wait for the packages to download and install. Click “Ok” on the dialogue button, and you will see a text file with a simple R Markdown template that demonstrates some basic functionality.\nIn this exercise class we use a lot of functions from the tidyverse system and several other packages. In order to access these you need the to install this package. Find the R console in R studio and paste the following line on the command line. Press enter to run it. This will install some of the necessary packages on your system.\n\n\ninstall.packages(c(\"tidyverse\", \"broom\", \"patchwork\", \"skimr\"))\n\nOptional: more on R Markdown: https://rmarkdown.rstudio.com/lesson-1.html\nOptional: a base R cheatsheet: https://www.datacamp.com/cheat-sheet/getting-started-r"
  },
  {
    "objectID": "02-Statistics-Review.html",
    "href": "02-Statistics-Review.html",
    "title": "2  Statistics Background",
    "section": "",
    "text": "3 Cumulative Distribution Function (CDF)\n\\[\\begin{align}\n    F(x) = \\sum_{x_i \\leq x} p(x_i).\n  \\end{align}\\]\n\\[\\begin{align}\n  P(X < 0) = 0 \\\\\n  P(X < 1) & = \\sum_{x_k < 1} p(x_k) = p(x_1) = 0.5. \\\\\n  P(X \\leq 1) & = \\sum_{x_k \\leq 1} p(x_k) = p(x_1) + p(x_2) = 0.5+ 0.3= 0.8\\\\\n  P(X < 2) & = \\sum_{x_k < 2} p(x_k) = p(x_1) + p(x_2) = 0.5+ 0.3= 0.8\\\\\n  P(X \\leq 2) & = \\sum_{x_k \\leq 2} p(x_k) = p(x_1) + p(x_2) + p(x_3) = 0.5+ 0.3+ 0.2= 1\\\\\n\\end{align}\\]\n\\[\nF(x) =\n\\begin{cases}\n  0 & x < 0 \\\\\n  0.5 & x \\leq 0 \\\\\n  0.8 & x \\leq 1 \\\\\n  1 & x \\leq 2 \\\\\n  1 & x > 2\n\\end{cases}\n\\]\n\\[\nX \\sim N(\\mu, \\sigma^2) \\\\\nf(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}\\right)^2}\\\\\nE(X) = \\mu \\\\\nVar(X) = \\sigma^2 \\\\\nSD(X) = \\sqrt{\\sigma^2} = \\sigma\n\\]\n\\[\nH_0: \\mu = a \\\\\nH_1: \\mu \\neq a\n\\]\n\\[\nt = \\frac{\\overline{x} - a}{S_x / \\sqrt{n}}\n\\]\nThe t-statistic follows a t-distribution with \\(n - 1\\) degrees of freedom."
  },
  {
    "objectID": "02-Statistics-Review.html#uniform-distributions",
    "href": "02-Statistics-Review.html#uniform-distributions",
    "title": "2  Statistics Background",
    "section": "4.3 Uniform distributions",
    "text": "4.3 Uniform distributions\n\\[\nX \\sim \\text{Uniform}(a, b), \\quad a,b \\in \\mathbb{R} \\\\\nf(x) = \\begin{cases}\n\\frac{1}{b - a} & a \\leq x \\leq b\\\\\n0 & \\text{otherwise}\n\\end{cases} \\\\\nE(X) = \\int_{a}^{b} f(x) dx = \\frac{1}{2}(a + b) \\\\\nVar(X) = \\int_{a}^{b} \\left(x - E(X)\\right)^2 = \\frac{1}{12}(b - a)^2 \\\\\nSD(X) = \\sqrt{Var(X)} = \\frac{1}{\\sqrt{12}}(b - a) \\\\\nF(x) = \\int_{-\\infty}^{x} = \\begin{cases}\n0 & x < a \\\\\n\\frac{x - a}{b - a} & a \\leq x \\leq b\\\\\n1 & x > b\n\\end{cases}\n\\]\nFor \\(a = -1\\) and \\(b = 1\\) the density of the uniform distribution is depicted in the following figure:\n\n## Illustration only\nunif_dens_plt <- ggplot() +\n  xlim(c(-2, 2)) +\n  stat_function(fun = dunif, args = list(min = -1, max = 1), n = 1000) +\n  labs(\n    x = \"x\",\n    y = \"Density\"\n  )\nunif_dens_plt\n\n\n\n\n\nmean(runif(n = 5, min = -1, max = 1))\n\n[1] 0.06903478\n\nmean(runif(n = 5, min = 3, max = 5))\n\n[1] 3.955678\n\n\n\nunif_sim <- tibble(\n  x = runif(n = 30, min = -1, max = 1)\n)\n\nunif_dens_plt +\n  geom_rug(aes(x = x), data = unif_sim)\n\n\n\n\n\\[\nP(X < -0.2) = F(-0.2)\n\\]\n\n## Compute the probability\npunif(-0.2, min = -1, max = 1)\n\n[1] 0.4\n\n## Compute the sample proportion in the simulated data\nmean(unif_sim$x < -0.2)\n\n[1] 0.4666667\n\n\n\\[\nP(X > -0.2) = 1 - F(-0.2)\n\\]\n\n## Compute the probability\n1 - punif(-0.2, min = -1, max = 1)\n\n[1] 0.6\n\npunif(-0.2, min = -1, max = 1, lower.tail = FALSE)\n\n[1] 0.6\n\n## Compute the sample proportion in the simulated data\nmean(unif_sim$x > -0.2)\n\n[1] 0.5333333\n\n\n\\[\nP(-0.2 < X < 0.3) = F(0.3) - F(-0.2)\n\\]\n\n## Probability\npunif(1.4, min = -1, max = 1) - punif(-0.1, min = -1, max = 1)\n\n[1] 0.55\n\n## Sample proportion in the simulation\nmean(unif_sim$x < 0.3 & unif_sim$x > -0.2)\n\n[1] 0.2333333\n\n\n\nsim_unif <- tibble(\n  x = runif(n = 50, min = -1, max = 1)\n)\n\nsim_unif %>%\n  ggplot(aes(x = x)) +\n  geom_histogram(bins = 10) +\n  xlim(c(-1, 1))\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`)."
  },
  {
    "objectID": "02-Statistics-Review.html#sec-probability",
    "href": "02-Statistics-Review.html#sec-probability",
    "title": "2  Statistics Review",
    "section": "2.1 Probability",
    "text": "2.1 Probability\nImagine a game where you flip a single coin once. The possible outcomes are then head (H) and tail (T). The set of all possible outcomes of the game is called the sample space of the experiment. We will denote this set with \\(\\Omega\\). For a single coin toss game the sample space is \\(\\Omega = \\{\\text{heads}, \\text{tails}\\}\\).\n\nDefinition 2.1 (Probability) Let \\(\\Omega\\) denote the sample space of a random experiment. Let \\(A \\subseteq \\Omega\\) and \\(B \\subset \\Omega\\) be two disjoint events (i.e. \\(A \\cap B = \\varnothing\\)). Disjoint events are events that cannot occur simultaneously.\n\\[\n\\begin{align}\n  & P(A) \\geq 0 \\qquad \\text{non-negativity}\\\\\n  & P(\\Omega) = 1 \\qquad  \\text{unit measure} \\\\\n  & P(A \\cup B) = P(A) + P(B) \\qquad \\text{additivity}\n\\end{align}\n\\]\n\n\nTheorem 2.1 (Probability of complements) Let \\(\\Omega\\) be a sample space, let \\(A \\subseteq \\Omega\\) be a subset of the sample space and let \\(\\bar{A} = \\Omega \\setminus A\\) be the complement of \\(A\\) in \\(\\Omega\\). Then the probability of the complement is given by:\n\\[\nP(A) = 1 - P(\\bar{A})\n\\]\n\n\nFor the proof note that \\(A\\) and \\(\\bar{A}\\) are disjoint by definition (\\(A \\cap \\bar{A} = \\varnothing\\)). Using the additivity property of probability together with the unit probability of the sample space \\(\\Omega\\) from Definition 2.1 it follows that:\n\\[\\begin{align}\n    P(A \\cup \\bar{A}) = P(A) + P(\\bar{A})\n  \\end{align}\\]\n\\[\\begin{align}\n    P(A \\cup \\bar{A}) & =  P(\\Omega) \\\\\n    P(A) + P(\\bar{A}) & = 1 \\implies \\\\\n    P(A) & = 1 - P(\\bar{A}).\n  \\end{align}\\]\n\n\nExample 2.1 (Weather forecast) Looking at the weather forecast for the next day you see that it will be raining (\\(A\\)) with probability \\(P(A) = 0.3\\). The sample space is \\(\\Omega = \\{A, \\bar{A}\\}\\) and the probability of not raining (\\(\\bar{A}\\)) is then \\(P(\\bar{A}) = 1 - 0.3 = 0.7\\).\n\n\nExample 2.2 (Dice) In a game where you roll a (6-sided) dice once the sample space is \\(\\Omega = \\{1, 2, 3, 4, 5, 6\\}\\). Denote the outcome of a roll with \\(X\\) and assume that the probability of each outcome is equal: \\(P(X = i) = 1 / 6, i = 1,2,\\ldots,6\\). The probability of the event \\(X = 1\\) is then \\(P(X = 1) = 1/6\\). The probability of the event “outcome is not one” is \\(P(X \\neq 1) = P(X > 1) = 5 / 6.\\)\n\nSee Bertsekas and Tsitsiklis (2008) (Chapter 1) for a more thorough treatment of the subject."
  },
  {
    "objectID": "01-Introduction-to-R.html#arithmetic-operations",
    "href": "01-Introduction-to-R.html#arithmetic-operations",
    "title": "1  Introduction to R",
    "section": "1.1 Arithmetic Operations",
    "text": "1.1 Arithmetic Operations\nShortcut to insert a code chunk: Ctrl-Alt-i Shortcut to run a single line of code: Ctrl-Enter Shortcut to run the whole chunk: Ctrl-Shift-Enter\n\n1 + 4\n\n[1] 5\n\n3 - 2\n\n[1] 1\n\n2 * 8\n\n[1] 16\n\n2 / 8\n\n[1] 0.25\n\n2^4\n\n[1] 16"
  },
  {
    "objectID": "01-Introduction-to-R.html#assignment",
    "href": "01-Introduction-to-R.html#assignment",
    "title": "1  Introduction to R",
    "section": "1.2 Assignment",
    "text": "1.2 Assignment\nShortcut for the assignment operator: Alt - (minus)\n\ny <- 34\ny - 40\n\n[1] -6"
  },
  {
    "objectID": "01-Introduction-to-R.html#vectors",
    "href": "01-Introduction-to-R.html#vectors",
    "title": "1  Introduction to R",
    "section": "1.3 Vectors",
    "text": "1.3 Vectors\n\nx <- c(1, 4)\n\n\n## Length, average, sum of a numeric vector\nmean(x)\n\n[1] 2.5\n\nsum(x)\n\n[1] 5\n\nlength(x)\n\n[1] 2\n\n\n\n## Documentation\n?mean"
  },
  {
    "objectID": "01-Introduction-to-R.html#character-values",
    "href": "01-Introduction-to-R.html#character-values",
    "title": "1  Introduction to R",
    "section": "1.4 Character values",
    "text": "1.4 Character values\n\nz <- \"Hello, world!\""
  },
  {
    "objectID": "01-Introduction-to-R.html#logical-values-and-logical-operators",
    "href": "01-Introduction-to-R.html#logical-values-and-logical-operators",
    "title": "1  Introduction to R",
    "section": "1.5 Logical values and logical operators",
    "text": "1.5 Logical values and logical operators\n\n2 < 5\n\n[1] TRUE\n\n2 > 5\n\n[1] FALSE\n\n\"Text 2\" == \"Text 2\"\n\n[1] TRUE\n\n\n\nz == \"Text 2\"\n\n[1] FALSE"
  },
  {
    "objectID": "01-Introduction-to-R.html#data",
    "href": "01-Introduction-to-R.html#data",
    "title": "1  Introduction to R",
    "section": "1.6 Data",
    "text": "1.6 Data\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n\n\ndt <- tibble(\n  id = 1:5,\n  y = c(2, 2.5, 3, 8, 12)\n)\ndt\n\n# A tibble: 5 × 2\n     id     y\n  <int> <dbl>\n1     1   2  \n2     2   2.5\n3     3   3  \n4     4   8  \n5     5  12  \n\n\n\nearnings <- read_csv(\"https://raw.githubusercontent.com/feb-uni-sofia/econometrics2021/main/data/earnings.csv\")\n\nRows: 1816 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): ethnicity\ndbl (14): height, weight, male, earn, earnk, education, mother_education, fa...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nheight (numeric): Height in inches (1 inch = 2.54 cm)\nweight (numeric): Weight in pounds (1 pound \\(\\approx\\) 0.45 kilograms)\nmale (numeric): 1: Male, 0: Female\nearn (numeric): Annual income in USD\nearnk (numeric): Annual income in 1,000 USD\nethnicity (character): Ethnicity\nage (numeric): Age\n\n\nearnings <- mutate(\n  earnings,\n  height_cm = 2.54 * height,\n  weight_kg = 0.45 * weight\n)\nearnings1 <- select(earnings, height_cm, weight_kg)\n\nThe same code can be rewritten in a more convenient way using pipes.\n\nearnings1 <- earnings %>%\n  mutate(\n    height_cm = 2.54 * height,\n    weight_kg = 0.45 * weight\n  ) %>%\n  select(height_cm, weight_kg)\n\nNote that the object holding the original data is unaffected by mutate and select."
  },
  {
    "objectID": "01-Introduction-to-R.html#basic-data-summaries",
    "href": "01-Introduction-to-R.html#basic-data-summaries",
    "title": "1  Introduction to R",
    "section": "1.7 Basic data summaries",
    "text": "1.7 Basic data summaries\n\n1.7.1 Location\n\\[\nx = (x_1, x_2,\\ldots, x_n)\\\\\n\\bar{x} = \\frac{1}{n}\\sum_{i = 1}^{n} x_i\n\\]\n\\(n = 1816\\).\n\nmean(earnings1$height_cm)\n\n[1] 169.0848\n\nmean(earnings1$weight_kg, na.rm = TRUE)\n\n[1] 70.33734\n\n\n\nmax(earnings1$height_cm) - min(earnings1$height_cm)\n\n[1] 63.5\n\n\n\nvar(c(1, 3))\n\n[1] 2\n\n\n\ntable(earnings$ethnicity)\n\n\n   Black Hispanic    Other    White \n     180      104       38     1494 \n\n\n\nearnings %>%\n  ggplot(aes(y = ethnicity)) +\n  geom_bar()\n\n\n\n\n\n\n1.7.2 Variability\n\nmax(earnings$height_cm)\n\n[1] 208.28\n\nmin(earnings$height_cm)\n\n[1] 144.78\n\n\n\nmax(earnings$height_cm) - min(earnings$height_cm)\n\n[1] 63.5\n\n\n\\[\nx = (x_1, \\ldots, x_n)\\\\\nS^2_x = \\frac{1}{n - 1} \\sum_{i = 1}^{n}(x_i - \\bar{x})^2: \\quad \\text{variance}\\\\\nS_x = \\sqrt{S^2_x} \\quad \\text{standard deviation}\n\\]\n\nx <- c(1, 3)\n((1 - 2)^2 + (3 - 2)^2) / 1\n\n[1] 2\n\nvar(x)\n\n[1] 2\n\nsd(x)\n\n[1] 1.414214\n\n\n\nvar(earnings$height_cm)\n\n[1] 94.72796\n\n\n\nsqrt(var(earnings$height_cm))\n\n[1] 9.732829\n\n\n\nsd(earnings$height_cm)\n\n[1] 9.732829\n\n\n\n## Basic summaries for the whole tibble\nearnings %>% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n1816\n\n\nNumber of columns\n17\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n16\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nethnicity\n0\n1\n5\n8\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nheight\n0\n1.00\n66.57\n3.83\n57.00\n64.00\n66.00\n69.25\n82.00\n▂▇▅▁▁\n\n\nweight\n27\n0.99\n156.31\n34.62\n80.00\n130.00\n150.00\n180.00\n342.00\n▅▇▃▁▁\n\n\nmale\n0\n1.00\n0.37\n0.48\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▅\n\n\nearn\n0\n1.00\n21147.30\n22531.77\n0.00\n6000.00\n16000.00\n27000.00\n400000.00\n▇▁▁▁▁\n\n\nearnk\n0\n1.00\n21.15\n22.53\n0.00\n6.00\n16.00\n27.00\n400.00\n▇▁▁▁▁\n\n\neducation\n2\n1.00\n13.24\n2.56\n2.00\n12.00\n12.00\n15.00\n18.00\n▁▁▁▇▃\n\n\nmother_education\n244\n0.87\n13.61\n3.22\n3.00\n12.00\n13.00\n16.00\n99.00\n▇▁▁▁▁\n\n\nfather_education\n295\n0.84\n13.65\n3.25\n3.00\n12.00\n13.00\n16.00\n99.00\n▇▁▁▁▁\n\n\nwalk\n0\n1.00\n5.30\n2.60\n1.00\n3.00\n6.00\n8.00\n8.00\n▃▁▃▁▇\n\n\nexercise\n0\n1.00\n3.05\n2.32\n1.00\n1.00\n2.00\n5.00\n7.00\n▇▁▁▁▃\n\n\nsmokenow\n1\n1.00\n1.75\n0.44\n1.00\n1.00\n2.00\n2.00\n2.00\n▃▁▁▁▇\n\n\ntense\n1\n1.00\n1.42\n2.16\n0.00\n0.00\n0.00\n2.00\n7.00\n▇▁▁▁▁\n\n\nangry\n1\n1.00\n1.42\n2.16\n0.00\n0.00\n0.00\n2.00\n7.00\n▇▁▁▁▁\n\n\nage\n0\n1.00\n42.93\n17.16\n18.00\n29.00\n39.00\n56.00\n91.00\n▇▇▃▃▁\n\n\nheight_cm\n0\n1.00\n169.08\n9.73\n144.78\n162.56\n167.64\n175.89\n208.28\n▂▇▅▁▁\n\n\nweight_kg\n27\n0.99\n70.34\n15.58\n36.00\n58.50\n67.50\n81.00\n153.90\n▅▇▃▁▁\n\n\n\n\n\n\ntable(earnings$ethnicity)\n\n\n   Black Hispanic    Other    White \n     180      104       38     1494"
  },
  {
    "objectID": "01-Introduction-to-R.html#visualizations",
    "href": "01-Introduction-to-R.html#visualizations",
    "title": "1  Introduction to R",
    "section": "1.8 Visualizations",
    "text": "1.8 Visualizations\nHistogram\n\nearnings %>%\n  ggplot(aes(x = height)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nA smooth density plot is an alternative way to visualize the distribution of a variable.\n\nearnings %>%\n  ggplot(aes(x = height)) +\n  geom_density()\n\n\n\n\nThe boxplot shows the median and the 25-th and 75-th percentiles (the box). The whiskers in the plot stretch to the minimum or the maximum observed value, unless there are extreme observations that are shown as single dots.\n\nearnings %>%\n  ggplot(aes(x = height)) +\n  geom_boxplot()\n\n\n\n\nGroup comparisons\n\nearnings %>%\n  ggplot(aes(x = height, y = ethnicity)) +\n  geom_boxplot()\n\n\n\n\nThe scatterplot will be our primary tool in studying associations between variables. It represents each observation as a point in a coordinate system defined by the variables that we would like to study.\n\nearnings1 %>%\n  ggplot(aes(x = weight_kg, y = height_cm)) +\n  geom_point(position = \"jitter\", alpha = 0.5) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"Weight (kg)\",\n    y = \"Height (cm)\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 27 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 27 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\nsummary(lm(height_cm ~ weight_kg, data = earnings1))\n\n\nCall:\nlm(formula = height_cm ~ weight_kg, data = earnings1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-27.639  -5.611  -0.084   5.645  36.248 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 145.00916    0.89138  162.68   <2e-16 ***\nweight_kg     0.34314    0.01237   27.73   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.15 on 1787 degrees of freedom\n  (27 observations deleted due to missingness)\nMultiple R-squared:  0.3009,    Adjusted R-squared:  0.3005 \nF-statistic: 769.1 on 1 and 1787 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bertsekas, Dimitri P., and John N. Tsitsiklis. 2008. Introduction to\nProbability. 2nd ed. Optimization and Computation Series.\nBelmont: Athena scientific."
  },
  {
    "objectID": "02-Statistics-Review.html#discrete-distributions",
    "href": "02-Statistics-Review.html#discrete-distributions",
    "title": "2  Statistics Background",
    "section": "2.2 Discrete distributions",
    "text": "2.2 Discrete distributions"
  },
  {
    "objectID": "02-Statistics-Review.html#discrete-random-variables",
    "href": "02-Statistics-Review.html#discrete-random-variables",
    "title": "2  Statistics Review",
    "section": "2.3 Discrete Random Variables",
    "text": "2.3 Discrete Random Variables\nImagine that you are about to buy a lottery ticket that costs 1 EUR. Until you buy and scratch the ticket you don’t actually know how much you will win, but before you commit to the purchase you may wonder what winnings you should expect. Without knowing the rules of the lottery you would be completely in the dark about your prospects, so let us assume that you actually know how the lottery works.\nFrom our point of view the rules of the game are completely determined by two things. The first one is the set of possible outcomes (the sample space) of the lottery and let’s assume that each ticket can win 0 EUR, 1 EUR or 2 EUR. Notice that the set of possible values is finite as there are only three possible outcomes. Let us write \\(X\\) for the (yet unknown) winning from our lottery ticket. In lottery games the value of \\(X\\) depends on some random mechanism (for example drawing numbered balls, spinning a wheel, etc.), therefore it is a function of the outcome of this random mechanism. We will call functions like \\(X\\) random variables. For the most part we will not refer the underlying random mechanism and will simply focus on the distribution of the possible values of \\(X\\). The second part of the rules is how often winnings of 0 EUR, 1 EUR and 2 EUR occur when you repeatedly play the game. Obviously, a game where half of the tickets win 2 EUR is quite different from a game where only one out of 100 tickets wins 2 EUR.\n\n\n\n\n\nTable 2.1:  Distribution of outcomes for two games of chance. Possible outcomes and probabilities for each outcome. \n \n  \n    Winnings(x) \n    P(x) \n    Winnings(y) \n    P(y) \n  \n \n\n  \n    0 EUR \n    0.5 \n    0 EUR \n    0.5 \n  \n  \n    1 EUR \n    0.3 \n    1 EUR \n    0.1 \n  \n  \n    2 EUR \n    0.2 \n    2 EUR \n    0.3 \n  \n\n\n\n\n\n\nLet us focus on the first game with probabilities (0.5, 0.3 and 0.2) and develop some intuitive understanding of these quantities. You can think about the probabilities in Table @ref(tab:discrete-game-probabilities) as theoretical proportions in a sense that if you play the lottery 100 times you would expect to win nothing (0 EUR) in about 50 games, 1 EUR in about 30 games and 2 EUR in about 20 games. Notice that to expect 20 2-EUR wins out of 100 games is absolutely not the same as the statement that you will win 2 EUR in exactly 20 out of 100 games! To convince yourself look at @ref(tab:discrete-games-simulation) which presents the results of five simulated games with 100 tickets each. You can play with this and similar games by changing the number of tickets and the number of games in this simulation. In the first game the player had 49 tickets that won nothing, but in the second game she had only 38 0-win tickets. When we say to expect 50 0-wins out of 100 tickets we mean that the number of observed (actually played) 0-wins will vary around 50. In neither of the five simulated games was the number of 0-wins exactly equal to 50 (this is also possible, though).\n::: {.cell =‘{}’ anchor=‘table’ tab.align=‘center’} ::: {.cell-output-display}\n\n\nSimulation of five games with 100 tickets each. Number of tickets by outcome (0, 1, or 2 EUR) and average ticket win.\n \n  \n      \n    x = 0 \n    x = 1 \n    x = 2 \n    average ticket win \n  \n \n\n  \n    Game 1 \n    49 \n    34 \n    17 \n    0.68 \n  \n  \n    Game 2 \n    38 \n    36 \n    26 \n    0.88 \n  \n  \n    Game 3 \n    47 \n    31 \n    22 \n    0.75 \n  \n  \n    Game 4 \n    49 \n    26 \n    25 \n    0.76 \n  \n  \n    Game 5 \n    62 \n    23 \n    15 \n    0.53 \n  \n\n\n\n::: :::\nThe probabilities in Table 2.1 completely describe the two games. The functions that assigns a probability to each possible outcome \\(p(x)\\) and \\(p(y)\\) are called probability mass functions. These functions incorporate everything there is to know about our hypothetical games. While this knowledge will not guarantee you a profit from gambling, it enables you to compute the expected value of each ticket, the probability that none of your tickets will win, etc. An important property of the probability mass function is that it is always non-negative (no negative probabilities) and that the sum of the probabilities over all possible values is exactly \\(1\\).\n\nDefinition 2.2 (Probability mass function) For a discrete random variable \\(X\\) with possible values \\(x_1,\\ldots,x_K\\) a function \\(p(x_i)\\) that assigns a probability to the possible values of \\(X\\) is called a probability mass function.\n\\[\n\\begin{align}\n  P(X = x_k) = p_k.\n\\end{align}\n\\] The probabilities are real numbers in the interval \\([0, 1]\\) and need to sum to 1 over all possible values.\n\\[\\begin{align}\n  p_k \\geq 0\\\\\n  \\sum_{k = 1} ^ {K} p_k = 1.\n\\end{align}\\]\n\nNote that the set of \\(K\\) possible values \\(x_1,\\ldots,x_K\\) is finite. The same definition can be used for infinite sets of possible values as long as these are countably infinite but we will skip this discussion.\n\nExample 2.3 (Probability mass function) The first game in Table 2.1 has three possible outcomes: \\(x_1 = 0, x_2 = 1, x_3 = 2\\). The probability mass function is then\n\\[\np(x) = \\begin{cases}\n0.5 & \\text{if} \\quad x = 0 \\\\\n0.3 & \\text{if} \\quad x = 1 \\\\\n0.2 & \\text{if} \\quad x = 2 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\n\nA common visualization of the probability mass function is a barchart with the heights of the bars corresponding to the probability of each outcome.\n\ntibble(\n  x = c(0:2),\n  p = c(0.5, 0.3, 0.2)\n) %>%\n  ggplot(aes(x = x, y = p)) + \n    geom_bar(stat = \"identity\")"
  },
  {
    "objectID": "02-Statistics-Review.html#expected-value",
    "href": "02-Statistics-Review.html#expected-value",
    "title": "2  Statistics Review",
    "section": "2.4 Expected value",
    "text": "2.4 Expected value\nJust as the descriptive statistics (average, empirical median, empirical quantiles, etc.) are useful for summarising a set of numbers we would like to be able to summarise distribution functions.\nImagine that you plan to buy 100 tickets from the first game in Table @ref(tab:discrete-game-probabilities). Based on the interpretation of probabilities as theoretical proportions you would expect that 50 of the tickets will win nothing, 30 of the tickets will bring you 1 EUR and 20 of the tickets will win 2 EUR. Thus you can write the expected winnings per ticket by summing the contributions of each ticket type:\n\\[\\begin{align}\n  \\text{expected winnings} & =\n    \\underbrace{\\frac{50}{100} \\times 0\\text{ EUR}}_{0\\text{ EUR tickets}} +\n    \\underbrace{\\frac{30}{100} \\times 1\\text{ EUR}}_{1\\text{ EUR tickets}} +\n    \\underbrace{\\frac{20}{100} \\times 2\\text{ EUR}}_{2\\text{ EUR tickets}} \\\\\n    & = 0.5 \\times 0 \\text{EUR} + 0.3 \\times 1 \\text{EUR} + 0.2 \\times 2 \\text{EUR} = 0.7\\text{EUR}.\n\\end{align}\\]{eq-expected-value-game-1}\nNote that the coefficients before the possible outcomes are simply their probabilities. Therefore in a game with 100 tickets you expect that each ticket will bring you \\(0.7\\) EUR (on average). Just as with the probabilities, the expected values does not tell you that your average win per ticket will be 0.7 EUR. If you take a look at the five simulated games in Table @ref(tab:discrete-games-simulation) you will notice that the realised average ticket wins are not equal to 0.7 but they vary around it. You can think about the expected value as the centre of the distribution (see Figure @ref(fig:game-probabilities-plot) and Freedman, Pisani, and Purves (2007), pp. 288). It is important to see that the expected value only depends on the probabilities and the possible values and it does not depend on the outcome of any particular game. Therefore it is a constant and not a random variable itself.\n\n\n\n\n\nProbabilities plot. The black vertical line depicts the expected value.\n\n\n\n\n\nLet us write the expected value in a more general way:\n\nDefinition 2.3 (Expected Value) For discrite random variable \\(X\\) with possible values \\(x_1, x_2,\\ldots,x_n\\) the weighted average of the possible outcomes: \\[\\begin{align}\n    E(X) = \\sum_{i = 1} ^ {n} x_i p(x_i) (\\#eq:expectation-discrete)\n  \\end{align}\\] is called the expected value of \\(X\\). Sometimes we will refer to the expected value as the mean of the distribution or the mean of the random variable following the distribution.\n\nWe introduced the expected value with an example of a game with 100 tickets in order to illustrate it. You should notice from Definition 2.3 that the expected value is independent of the number of games played as it is a property of the probability mass function of the game.\n\nExercise 2.1 (Expected Value) Let \\(Y\\) be a game with possible winnings of -1, 0, and 2 EUR. The probabilities of these outcomes are \\(P(x = -1) = 0.2, P(x = 0) = 0.7, P(x = 2) = 0.1\\).\n\nPlot the PMF using a barchart\nCalculate the expected winnings of this game using Definition 2.3\n\nPlay the game using the function sample (check its documentation either by typing ?sample on the command line in R or by searching for it online). It selects size of the values speficied in x according to the probabilities given in prob.\n\nsample(\n  x = c(-1, 0, 2), \n  size = 10, \n  prob = c(0.2, 0.7, 0.1),\n  replace = TRUE\n)\n\n [1]  0  0  0 -1  0  0  0  0 -1  0\n\n\nPlay the game a couple of times and look at the outcomes. Compute the average winnings using mean and compare the result with the expected value that you calculated in the previous step.\n\n\n2.4.1 Properties of the expected value\nIn the following we list a few important properties of the expected value that we will use throughout the course. In the following let \\(X\\) and \\(Y\\) be random variables with expected values \\(E(X)\\) and \\(E(Y)\\).\n\nTheorem 2.2 (Linearity of the Expected Value) Let \\(X\\) and \\(Y\\) be two random variables. Then the exected value of their sum equals the sum of their expected values.\n\\[\\begin{align}\n  E(X + Y) = E(X) + E(Y)\n\\end{align}\\]\n\n\nTheorem 2.3 (The Expected Value of a Constant) The expected value of a constant equals the constant itself. Let \\(a\\) be any fixed (not random) real number. Then its expected value is:\n\\[\\begin{align}\n  E(a) = a.\n\\end{align}\\]\n\n\nTheorem 2.4 (Expected value of a scaled random variable) Let \\(X\\) be a random variable and let \\(a\\) be any fixed (not random) real number. Then the expected value of \\(aX\\) is:\n\\[\\begin{align}\n  E(aX) = a E(X)\n\\end{align}\\]\n\n\nProof. \\[\\begin{align}\n  E(aX) = \\sum_{i = 1} ^ {n} a x_i p(x_i) = a \\sum_{i = 1} ^ {n} x_i p(x_i) = aE(X).\n\\end{align}\\]"
  },
  {
    "objectID": "02-Statistics-Review.html#variance",
    "href": "02-Statistics-Review.html#variance",
    "title": "2  Statistics Review",
    "section": "2.5 Variance",
    "text": "2.5 Variance\nLet us compare the two games in Table 2.1. Both have the same sample space (set of possible outcomes) and both have the same expected winnings, see ?eq-expected-value-game-1 and ?eq-expected-value-game-2. The games are not identical, though, because their probability distributions are different. If given the choice to play only one game, which one would you prefer?\nThe second game offers a higher probability to win the highest prize (2 EUR) at the cost of a lower probability for the middle prize (1 EUR). In other words it places a higher probability on extreme outcomes (outcomes that are far from the centre of the distribution, i.e. the expected value). A summary of a distribution that measures its spread (i.e. how likely are extreme values) is the variance:\n\nDefinition 2.4 (Variance) For a discrete random variable \\(X\\) with possible outcomes \\(x_1, x_2,\\ldots,x_n\\) and probability function p(x) the variance is the expected quadratic deviation from the expected value of the distribution \\(E(X)\\).\n\\[\\begin{align}\n  Var(X) & = E(X - E(X)) ^ 2 \\\\\n         & = \\sum_{i = 1}  ^ n \\left(x_i - E(X)\\right) ^ 2 p(x_i)\n\\end{align}\\]\n\n\nExample 2.4 (Variance) The variance of the first game (X) is:\n\\[\\begin{align}\n    Var(X) & = \\sum_{i = 1} ^ {3} (x_i - E(X)) ^ 2 p(x_i) \\\\\n           & = (x_1 - E(X)) ^ 2 p(x_i) + (x_2 - E(X)) ^ 2 p(x_2) + (x_3 - E(X)) ^ 2 p(x_3) \\\\\n           & = (\\devxa) ^ 2 \\times \\pa + (\\devxb) ^ 2 \\times \\pb + (\\devxc) ^ 2 \\times \\pc \\\\\n           & = \\varx.\n  \\end{align}\\]\n\n\nExercise 2.2 Compute the variance of \\(Y\\), the second game described in Table Table 2.1.\n\n\nSolution. \\(Var(Y) = \\vary\\)."
  },
  {
    "objectID": "02-Statistics-Review.html#sec-discrete-distributions",
    "href": "02-Statistics-Review.html#sec-discrete-distributions",
    "title": "2  Statistics Review",
    "section": "2.2 Discrete distributions",
    "text": "2.2 Discrete distributions\n\n\n\nWhen we presented the axioms of probability we mentioned the concept of a sample space and probability. For the sake of brevity we will skip almost all of the set theoretic foundation of probability measures and will directly introduce the concept of a random variable.\nLet us go back to the roots of probability theory that date back at least to the 17th century and the study of games of chance (gambling) (Freedman, Pisani, and Purves 2007, 248). Almost all introductory text on probability theory start with an example of some simple game of chance. We will follow this example, because it is easy to understand a simple game and the mathematical concepts involved. Later we will see how we can apply the concepts developed here to an extremely broad range of problems."
  },
  {
    "objectID": "02-Statistics-Review.html#independence-of-random-variables",
    "href": "02-Statistics-Review.html#independence-of-random-variables",
    "title": "2  Statistics Review",
    "section": "2.6 Independence of Random Variables",
    "text": "2.6 Independence of Random Variables\nTwo discrete random variables \\(X\\) and \\(Y\\) with possible values \\(x_1,\\ldots,x_K\\) and \\(y_1,\\ldots,y_L\\) are independent if for every \\(k\\) and \\(l\\):\n\\[\\begin{align}\n  P(X = x_k, Y = y_l) = P(X = x_k)P(Y = y_l)\n\\end{align}\\]"
  },
  {
    "objectID": "02-Statistics-Review.html#continuous-distributions",
    "href": "02-Statistics-Review.html#continuous-distributions",
    "title": "2  Statistics Review",
    "section": "2.7 Continuous Distributions",
    "text": "2.7 Continuous Distributions\n\n2.7.1 Uniform distributions\n\\[\\begin{align}\nX \\sim \\text{Uniform}(a, b), \\quad a,b \\in \\mathbb{R} \\\\\nf(x) = \\begin{cases}\n\\frac{1}{b - a} & a \\leq x \\leq b\\\\\n0 & \\text{otherwise}\n\\end{cases} \\\\\nE(X) = \\int_{a}^{b} f(x) dx = \\frac{1}{2}(a + b) \\\\\nVar(X) = \\int_{a}^{b} \\left(x - E(X)\\right)^2 = \\frac{1}{12}(b - a)^2 \\\\\nSD(X) = \\sqrt{Var(X)} = \\frac{1}{\\sqrt{12}}(b - a) \\\\\nF(x) = \\int_{-\\infty}^{x} = \\begin{cases}\n0 & x < a \\\\\n\\frac{x - a}{b - a} & a \\leq x \\leq b\\\\\n1 & x > b\n\\end{cases}\n\\end{align}\\]\nFor \\(a = -1\\) and \\(b = 1\\) the density of the uniform distribution is depicted in the following figure:\n\nunif_dens_plt <- ggplot() +\n  xlim(c(-2, 2)) +\n  stat_function(\n    fun = dunif, \n    args = list(\n      min = -1,\n      max = 1\n      ),\n    n = 1000\n  ) +\n  labs(\n    x = \"x\",\n    y = \"Density\"\n  )\nunif_dens_plt\n\n\n\n\nFigure 2.1: Density function of the uniform distribution over [-1, 1].\n\n\n\n\nIn order to select values at random from the uniform distribution with limits \\(a = -1\\) and \\(b = 1\\) we can use the runif function. The number of values selected is controlled by the n argument, while the parameters of the distribution are specified in min (\\(a\\)) and max (\\(b\\)).\n\n## Select 5 values at randorm from the uniform distribution over [-1, 1]\nx <- runif(n = 5, min = -1, max = 1)\n\n## Print x\nx\n\n[1]  0.7801805 -0.5024699  0.2865454  0.1473476  0.3013221\n\n\nThe following code plots the values in x as small lines over the x-axis (rug plot).\n\n## Code for illustration only\nunif_sim <- tibble(\n  x = x\n)\n\nunif_dens_plt +\n  geom_rug(aes(x = x), data = unif_sim)\n\n\n\n\nThe area under the density function for an interval is the probability of occurrence of outcomes in this interval. It is easy to calculate the area under Figure 2.1 for the interval [-1, -0.2]. The hight of the density function is constant at \\(1/2\\). The length of the interval is 0.8, therefore the area of the rectangle is \\(0.8 \\cdot 1/2 = 0.4\\).\n\\[\nP(X < -0.2) = 0.4\n\\] You can also compute this probability using the punif function. By default it computes probabilities of the type P(X < x), where x is its first argument. Note that you also need to speficy the limits of the distribution ([-1, 1] in this case).\n\n## Compute the probability\npunif(-0.2, min = -1, max = 1)\n\n[1] 0.4\n\n\nEarlier we used rnorm to select a couple of values at random from this distribution. Let us compare the observed proportion of values less than -0.2 to the theoretical probability.\n\n## Use < to see which values in x are less than -0.2\nx\n\n[1]  0.7801805 -0.5024699  0.2865454  0.1473476  0.3013221\n\nx < -0.2\n\n[1] FALSE  TRUE FALSE FALSE FALSE\n\n\n\n## Compute the sample proportion in the simulated data\n## This relies on the following conversion of the logical values\n## TRUE -> 1, FALSE -> 0. This conversion happens under the hood \n## in the mean function\n\nmean(x < -0.2)\n\n[1] 0.2\n\n\nAs an exercise, increase the number of games plated (size argument in rnorm) and compare the observed proportion and the probability.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBertsekas, Dimitri P., and John N. Tsitsiklis. 2008. Introduction to Probability. 2nd ed. Optimization and Computation Series. Belmont: Athena scientific.\n\n\nFreedman, David, Robert Pisani, and Roger Purves. 2007. Statistics. 4th ed. New York: W.W. Norton & Co."
  },
  {
    "objectID": "03-Simple-Linear-Model.html#interpretation-of-the-model-coefficients-1",
    "href": "03-Simple-Linear-Model.html#interpretation-of-the-model-coefficients-1",
    "title": "3  Simple Linear Model",
    "section": "6.1 Interpretation of the model coefficients",
    "text": "6.1 Interpretation of the model coefficients\n\\[\n\\hat{y} = 0.64 + 0.011 x\n\\]\n\\[\n\\hat{y}_{x=0} = 0.64 + 0.011*0 = 0.64 [hours]\n\\]\nFor \\(x = 0\\) (zero invoices) the expected processing time (\\(\\hat{y}\\)) equals 0.64 hours. These correspond to the fixed costs of the firm (in terms of time). 0.011 (hours/invoice) is the estimated marginal cost for an additional invoice (in terms of time).\nTo plot the OLS regression line quickly:\n\ninvoices %>%\n  ggplot(aes(x = Invoices, y = Time)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "03-Simple-Linear-Model.html#simulation",
    "href": "03-Simple-Linear-Model.html#simulation",
    "title": "3  Simple Linear Model",
    "section": "6.2 Simulation",
    "text": "6.2 Simulation\nNormal distribution\n\n# r: random, norm: normal\n# mean: expected value of the distribution,\n# sd: standard deviation of the distribution = square root of the variance\n\nu <- rnorm(1000, mean = 0, sd = 1)\n# u\n\n\nmean(u)\n\n[1] 0.005486152\n\n\n\ntibble(u) %>%\n  ggplot(aes(x = u)) +\n  geom_histogram(bins = 20)\n\n\n\n\nLets us assume, that we know a linear relationship between \\(y\\) and \\(x\\).\n\\[\ny =  0.1 + 0.015 x + u, \\quad u \\sim N(0, \\sigma^2 = 0.5^2)\n\\]\n\n## Number of onservations\nn <- 30\n\n## Generate a grid of 30 values for x between 10 and 250\nx <- round(seq(10, 250, length.out = n), 0)\nx\n\n [1]  10  18  27  35  43  51  60  68  76  84  93 101 109 118 126 134 142 151 159\n[20] 167 176 184 192 200 209 217 225 233 242 250\n\n\n\n## Select values at random from a standard\n## normal distribution, i.e. mean (expected value) = 0, standard dev. = 0.5\nu <- rnorm(n, mean = 0, sd = 0.5)\ny <- 0.1 + 0.015 * x + u\nsim_data <- tibble(x = x, y = y)\n# sim_data %>%\n#   ggplot(aes(x = x, y = y)) +\n#     geom_point() +\n#     geom_abline(intercept = 0.1, slope = 0.015) +\n#     ## Controls the range of the y-axis\n#     ylim(c(0, 5)) +\n#     ## Controls the range of the x-axis\n#     xlim(c(0, 260))\nlm(y ~ 1 + x)\n\n\nCall:\nlm(formula = y ~ 1 + x)\n\nCoefficients:\n(Intercept)            x  \n    -0.0216       0.0163"
  },
  {
    "objectID": "04-Simple-ANOVA.html",
    "href": "04-Simple-ANOVA.html",
    "title": "4  Simple ANOVA",
    "section": "",
    "text": "5 Linear regression model with a single categorical predictor\nVariables description:\nTwo groups of kids: the ones whose mother had a high school degree (mom_hs = 1) and the rest (mom_hs = 0).\nQuestion: are this two groups different with respect to IQ (as measured by kid_score).\nResearch hypothesis: children of mothers without a high school degree have lower IQ on average compared to children of mothers with a high school degree.\nFormulated as a linear regression model:\n\\[\ni = 1,\\ldots, n = 434\\\\\ny_i: \\text{Kid IQ score} \\\\\nx_i \\in \\{0, 1\\}\n\\]\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + u_i, u_i \\sim N(0, \\sigma^2)\n\\]\nFind the OLS estimates for \\(\\beta_0\\) and \\(\\beta_1\\) using the lm function.\nWrite the estimated regression equation\n\\[\\begin{align}\n\\hat{\\mu}_i = 77.55 + 11.77 \\hat{x}_i\n\\end{align}\\]\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + u_i\n\\]\nUnder some assumptions it can be shown that under the null hypothesis (this simply means that we assume the null hypothesis is true)\n\\[\nH_0: \\beta_1 = \\beta_{H_0}\\\\\nH_1: \\beta_1 \\neq \\beta_{H_0}\\\\\n\\]\n\\[\n\\text{t-statistic} = \\frac{\\hat{\\beta_1} -  \\beta_{H_0}}{SE(\\hat{\\beta}_1)}\n\\]\n\\[\n\\text{t-statistic} \\underbrace{\\sim}_{H_0} t(\\text{df} = n - p)\n\\]\nThe t-statistic follow a t-distribution with \\(n - p\\) degrees of freedom (parameter of the distribution), where \\(n\\) is the number of observations in the linear model (in our example \\(n = 434\\) kids) and \\(p\\) is the number of coefficients in the linear equation. In our linear regression model the number of coefficients in \\(p = 2\\): the intercept \\(\\beta_0\\), and the slope coefficient \\(\\beta_1\\)."
  },
  {
    "objectID": "04-Simple-ANOVA.html#simulation",
    "href": "04-Simple-ANOVA.html#simulation",
    "title": "4  Simple ANOVA",
    "section": "5.1 Simulation",
    "text": "5.1 Simulation\nWe want to create a simulation where we select 2000 samples of children from a given model. Fix the following code so that there are 2000 samples (currently there are 2) and so that there are exactly as many children with \\(mom_hs = 1\\) and \\(mom_hs = 0\\) as in the original data kids.\n\\[\ny_i \\sim N(\\mu_i, 19.85^2) \\\\\n\\mu_i = 77.548 + 11.771 x_i \\\\\n\\]\n\n## Fix the random numbers generator so that you can reproduce your results\nset.seed(123)\n\nsim_df <- expand_grid(\n  R = 1:200,\n  mom_hs = rep(c(0, 1), c(93, 341))\n)\n\nsim_df <- sim_df %>%\n  mutate(\n    mu = 77.548 + 11.771 * mom_hs,\n    kid_score = rnorm(n = n(), mean = mu, sd = 19.85)\n  )\n\nSelect the first simulated sample and find the OLS coefficient estimates using lm.\n\n## Estimate the OLS coefficients with the data from the first sample of children\nsim_sample_1 <- sim_df %>% filter(R == 1)\nlm(kid_score ~ 1 + mom_hs, data = sim_sample_1)\n\n\nCall:\nlm(formula = kid_score ~ 1 + mom_hs, data = sim_sample_1)\n\nCoefficients:\n(Intercept)       mom_hs  \n      78.92        10.42  \n\n\nThe following code computes the OLS estimates for each of the samples.\n\nsim_coeff <- sim_df %>%\n  group_by(R) %>%\n  ## The tidy function reformats the output of lm so that it can fit in a data frame\n  do(tidy(lm(kid_score ~ mom_hs, data = .))) %>%\n  select(R, term, estimate, std.error, statistic)\n\nThe data set sim_coeff now contains estimated coefficients (\\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\)) for every sample. Check the first two rows and compare it to your results for the first estimated sample. To plot the distribution of \\(\\hat{\\beta}_1\\) (the slope coefficients) we filter the data set so that we keep only the raw where term == \"mom_hs\" (i.e our estimates for \\(\\beta\\)).\n\nslopes <- sim_coeff %>% filter(term == \"mom_hs\")\n\nPlot the distribution of the slope estimates\n\nslopes %>%\n  ggplot(aes(x = estimate)) +\n  geom_point(\n    aes(y = 0),\n    position = \"jitter\",\n    size = 1 / 2,\n    alpha = 0.5\n  ) +\n  geom_boxplot(alpha = 0.5) +\n  labs(\n    x = \"Slope estimate\",\n    title = \"Distribution of slope estimates (over 2000 samples)\",\n    y = \"\"\n  ) +\n  geom_density(color = \"steelblue\") +\n  geom_vline(xintercept = 11.77, color = \"red\")\n\n\n\n#  xlim(c(0, 21))\n\nCopy the code above and adapt it to show the distribution of the intercept coefficients.\n\n# ???"
  },
  {
    "objectID": "04-Simple-ANOVA.html#summary-of-the-lm-output",
    "href": "04-Simple-ANOVA.html#summary-of-the-lm-output",
    "title": "4  Simple ANOVA",
    "section": "5.2 Summary of the lm output",
    "text": "5.2 Summary of the lm output\n\nsummary(fit)\n\n\nCall:\nlm(formula = kid_score ~ 1 + mom_hs, data = kids)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-57.55 -13.32   2.68  14.68  58.45 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   77.548      2.059  37.670  < 2e-16 ***\nmom_hs        11.771      2.322   5.069 5.96e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.85 on 432 degrees of freedom\nMultiple R-squared:  0.05613,   Adjusted R-squared:  0.05394 \nF-statistic: 25.69 on 1 and 432 DF,  p-value: 5.957e-07\n\n\nStandard error: this is an estimate of the standard deviation of the estimator for the coefficient based on assumptions about the error term (\\(u_i\\)) in the model.\nThe standard deviation of \\(\\hat{\\beta}_1\\), computed using the simulated samples:\n\n## sd: standard deviation\nsd(slopes$estimate)\n\n[1] 2.504912"
  },
  {
    "objectID": "04-Simple-ANOVA.html#testing-a-true-null-hypothesis",
    "href": "04-Simple-ANOVA.html#testing-a-true-null-hypothesis",
    "title": "4  Simple ANOVA",
    "section": "5.3 Testing a true null hypothesis",
    "text": "5.3 Testing a true null hypothesis\n\\[\nH_0: \\beta_1 = 11.77\\\\\nH_1: \\beta_1 \\neq 11.77\n\\]\nt-test\n\\[\n\\text{t-statistic} = \\frac{\\hat{\\beta}_1 - 11.77}{SE(\\hat{\\beta}_1)}\n\\]\nWhen is the value of the t-statistic close to zero? 1. The estimate is close to the value under the null hypothesis. 2. For high values of the standard error\n\\[\nt = \\frac{11.77 - 11.77}{2.322} = 0\n\\]\n\nslopes <- slopes %>%\n  mutate(\n    t_statistic = (estimate - 11.77) / std.error\n  )\n\n\nslopes %>%\n  ggplot(aes(x = t_statistic)) +\n  geom_point(\n    aes(y = 0),\n    position = \"jitter\",\n    size = 1 / 2,\n    alpha = 0.5\n  ) +\n  geom_boxplot(alpha = 0.5) +\n  labs(\n    x = \"Value of the t-statistic\",\n    title = \"Distribution of t-statistic under a true null hypothesis beta_1 = 11.77 (2000 samples)\",\n    y = \"\"\n  ) +\n  geom_density(color = \"steelblue\") +\n  geom_vline(xintercept = 0, color = \"red\") +\n  geom_vline(xintercept = c(-2, 2), color = \"steelblue\", lty = 2) +\n  geom_vline(xintercept = c(-3, 3), color = \"firebrick\", lty = 2) +\n  scale_x_continuous(breaks = c(-3, -2, 0, 2, 3))\n\n\n\n#  xlim(c(0, 21))\n\nThe real coefficient equals 11.77 (it is known, because we choose it for the simulation).\nLets assume a rule that we reject the null hypothesis \\(H_0: \\beta_1 = 0\\) vs. \\(H_1: \\beta_1 \\neq 0\\) if the value of the t-statistic is less than -2 or greater than +2.\nIn how many samples will we wrongly reject the null hypothesis using this rule?\n\ntesting_1 <- slopes %>%\n  mutate(\n    ## Logical OR: |\n    wrong_decision_blue = t_statistic < -2 | t_statistic > 2,\n    wrong_decision_red = t_statistic < -3 | t_statistic > 3\n  )\n## Share of TRUE values\nmean(testing_1$wrong_decision_blue)\n\n[1] 0.065\n\nmean(testing_1$wrong_decision_red)\n\n[1] 0.01"
  },
  {
    "objectID": "04-Simple-ANOVA.html#testing-a-wrong-null-hypothesis",
    "href": "04-Simple-ANOVA.html#testing-a-wrong-null-hypothesis",
    "title": "4  Simple ANOVA",
    "section": "5.4 Testing a wrong null hypothesis",
    "text": "5.4 Testing a wrong null hypothesis\n\\[\nH_0: \\beta_1 = 0\\\\\nH_1: \\beta_1 \\neq 0\n\\]\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + u_i\n\\]\nIf \\(H_0\\) is true, the the model is simply\n\\[\ny_i = \\beta_0 + u_i\n\\]\nt-test\n\\[\n\\text{t-statistic} = \\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)}\n\\] The value of the t-statistic is small when the estimate for the coefficient is close to the value under the null hypothesis. The value of the t-statistic will be small, if the standard error of the estimator is high.\n\\[\nt = \\frac{11.77 - 0}{2.322} = 5.069\n\\]\nCompute the value of the t-statistic for all samples in the simulation (and compare it to the value of the statistic column in the sim_coef dataset)\n\nslopes <- slopes %>%\n  mutate(\n    t_statistic0 = (estimate - 0) / std.error\n  )\n\n\nslopes %>%\n  ggplot(aes(x = t_statistic0)) +\n  geom_point(\n    aes(y = 0),\n    position = \"jitter\",\n    size = 1 / 2,\n    alpha = 0.5\n  ) +\n  geom_boxplot(alpha = 0.5) +\n  labs(\n    x = \"Value of the t-statistic\",\n    title = \"Distribution of t-statistic, beta_1 = 0 (false)\",\n    y = \"\"\n  ) +\n  geom_density(color = \"steelblue\") +\n  geom_vline(xintercept = 0, color = \"red\") +\n  geom_vline(xintercept = c(-2, 2), color = \"steelblue\", lty = 2) +\n  geom_vline(xintercept = c(-3, 3), color = \"firebrick\", lty = 2) +\n  xlim(c(-4, 8)) +\n  scale_x_continuous(breaks = c(-3, -2, 0, 2, 3))\n\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale.\n\n\n\n\n\n\ntesting_2 <- slopes %>%\n  mutate(\n    ## Logical AND: &\n    wrong_decision_blue = t_statistic0 < 2 & t_statistic0 > -2,\n    wrong_decision_red = t_statistic0 < 3 & t_statistic0 > -3\n  )\n## Share of TRUE values\nmean(testing_2$wrong_decision_blue)\n\n[1] 0\n\nmean(testing_2$wrong_decision_red)\n\n[1] 0.04"
  },
  {
    "objectID": "04-Simple-ANOVA.html#how-to-choose-critical-values",
    "href": "04-Simple-ANOVA.html#how-to-choose-critical-values",
    "title": "4  Simple ANOVA",
    "section": "5.5 How to choose critical values?",
    "text": "5.5 How to choose critical values?\nConvention: choose the critical values so that the probability of rejecting a true null hypothesis is 5%."
  },
  {
    "objectID": "04-Simple-ANOVA.html#probabilities-and-quantiles-of-the-t-distribution",
    "href": "04-Simple-ANOVA.html#probabilities-and-quantiles-of-the-t-distribution",
    "title": "4  Simple ANOVA",
    "section": "6.1 Probabilities and quantiles of the t-distribution",
    "text": "6.1 Probabilities and quantiles of the t-distribution\n\n\n\n\n\n\n6.1.1 Probability\n\n# p: probability, t: t-distribution\npt(-2, df = 434 - 2)\n\n[1] 0.02306292\n\n\n\nrt(1, df = 434 - 2)\n\n[1] 0.6343223\n\n\n\n# r: random, t: t-distribution\nmean(rt(1000000, df = 434 - 2) < -2)\n\n[1] 0.022944\n\n# mean(rt(1000000, df = 2) < -2)\n\n\n\n6.1.2 Quantiles\n\n## q: quantile, t: t-distribution\nqt(p = 0.02306292, df = 434 - 2)\n\n[1] -2"
  },
  {
    "objectID": "04-Simple-ANOVA.html#critical-values-in-t-tests",
    "href": "04-Simple-ANOVA.html#critical-values-in-t-tests",
    "title": "4  Simple ANOVA",
    "section": "6.2 Critical values in t-tests",
    "text": "6.2 Critical values in t-tests\n\n# 0.025 quantile of the t-distribution with 2 degrees of freedom\nqt(0.025, df = 434 - 2)\n\n[1] -1.965471\n\n\n\n# r: random, t: t-distribution\nmean(rt(1000000, df = 434 - 2) < -1.965471)\n\n[1] 0.024963\n\n\nA convention is to use a 5% error probability of rejecting a true null hypothesis, so we use the quantiles of the t-distribution to derive critical values as follows:\n\n## Lower critical value: the 0.025 quantile of the t-distribution\nqt(0.025, df = 434 - 2)\n\n[1] -1.965471\n\n## Upper critical value: the 0.975 quantile of the t-distribution\n## lower.tail = FALSE instructs qt to calculate\nqt(0.025, df = 434 - 2, lower.tail = FALSE)\n\n[1] 1.965471\n\n\nBoth critical values are equal in absolute value."
  }
]